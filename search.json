[
  {
    "objectID": "fundamentos_probabilidade.html",
    "href": "fundamentos_probabilidade.html",
    "title": "Fundamentos de Probabilidades",
    "section": "",
    "text": "Espaço de possibilidades de um experimento\n\n\n\nProbabilidade\n\nEspaço amostral\n\nEventos\n\nExperimento aleatório\n\n\n\nEspaço de possibilidades de um experimento aleatório, abordando a definição de evento e probabilidade.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombinando as probabilidades de eventos\n\n\n\nProbabilidade\n\nEventos complexos\n\nDiagrama de Venn\n\nDiagrama de árvore\n\n\n\nCombinação de probabilidades de eventos, utilizando diagramas de Venn e árvores para representar uniões e interseções.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilidade condicional e independência\n\n\n\nProbabilidade condicional\n\nEventos dependentes\n\nTeorema de Bayes\n\nDiagrama de árvore\n\n\n\nExploração da probabilidade condicional e independência, com aplicações do Teorema de Bayes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeorema de Bayes\n\n\n\nTeorema de Bayes\n\nProbabilidade condicional\n\nEventos dependentes\n\n\n\nApresentação do Teorema de Bayes e sua aplicação no cálculo de probabilidades condicionais.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "introducao_r.html",
    "href": "introducao_r.html",
    "title": "Introdução ao R",
    "section": "",
    "text": "Estrutura da linguagem\n\n\n\nR\n\nProgramação\n\n\n\nEstrutura da linguagem R, incluindo operações básicas, tipos de objetos (vetores, data frames, matrizes, listas) e sintaxe principal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Básico da) Manipulação de data frames\n\n\n\nR\n\nProgramação\n\nManipulação de dados\n\nData frames\n\n\n\nPrincípios de manipulação de data frames no R: importação, seleção de linhas e colunas e criação de variáveis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Básico da) Visualização gráfica\n\n\n\nR\n\nProgramação\n\nGráficos em R\n\nVisualização de dados\n\n\n\nIntrodução à criação de gráficos em R: gráficos de barras, histogramas, boxplots, dispersão e exportação de figuras.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "Análise de variância",
    "section": "",
    "text": "Análise de variância de um fator\n\n\nAvaliação de uma única fonte de variação, partição da soma de quadrados e aplicação do teste F.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "modelos_regressao_bayes.html",
    "href": "modelos_regressao_bayes.html",
    "title": "Modelos de Regressão Bayesianos",
    "section": "",
    "text": "Regressão Linear Bayesiana\n\n\nRegressão linear bayesiana — altura em adultos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFluxo de Trabalho na Modelagem Bayesiana\n\n\n\nInferência bayesiana\n\nModelagem estatística\n\nBambi\n\nPyMC\n\nFluxo de trabalho\n\nDistribuições a priori\n\nInferência a posteriori\n\n\n\nExplorando o fluxo de trabalho bayesiano em modelos de regressão linear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplorando Modelos de Regressão Bayesiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelos Estatísticos e Modelos Científicos\n\n\nTrês estratégias para modelar a relação entre tamanho populacional e número de ferramentas\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "estrutura_dados.html",
    "href": "estrutura_dados.html",
    "title": "Estrutura de Dados",
    "section": "",
    "text": "Estrutura e tipos de dados\n\n\n\nR\n\nEstrutura de dados\n\nTipos de dados\n\nAnálise de dados\n\n\n\nDescrição de diferentes estruturas de dados e tipos de variáveis, com foco em níveis de mensuração e tratamento de valores ausentes em tabelas.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html",
    "title": "Modelo Normal Bayesiano",
    "section": "",
    "text": "Exploraremos a inferência bayesiana, com foco na modelagem de dados contínuos por meio da distribuição normal. Nosso objetivo será desenvolver a intuição sobre como escolher distribuições a priori e como o PyMC nos auxilia a visualizar as consequências dessas escolhas sobre a distribuição preditiva a priori da variável de interesse. Para isso, utilizaremos um exemplo baseado na distribuição de altura em adultos.\n# Configuração inicial e importação de bibliotecas\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Configurações para plots\nplt.style.use('seaborn-v0_8-darkgrid')\nplt.rcParams['figure.figsize'] = (9, 6)"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html#explorando-a-distribuição-normal",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html#explorando-a-distribuição-normal",
    "title": "Modelo Normal Bayesiano",
    "section": "1 Explorando a Distribuição Normal",
    "text": "1 Explorando a Distribuição Normal\nA Distribuição Normal, frequentemente chamada de curva de sino ou curva Gaussiana, é central em estatística. Ela é caracterizada por dois parâmetros: a média (\\(\\mu\\)) e o desvio padrão (\\(\\sigma\\)). A média determina o centro da distribuição, enquanto o desvio padrão determina sua dispersão ou largura. Muitos fenômenos naturais podem ser adequadamente descritos por essa distribuição.\nA ideia intuitiva: Pense na altura de adultos. Há um valor central (a média) em torno do qual a maioria das alturas se agrupa. Há também uma variação: algumas pessoas são mais altas, outras mais baixas. O desvio padrão nos diz o quão espalhadas essas alturas tendem a ser em relação à média.\n\n1.1 Curva de densidade de probabilidade\nGerando a Distribuição de Densidade Normal para diferentes valores de \\(\\mu\\) e \\(\\sigma\\).\n\n# Parâmetros da distribuição\nmu_1 = 20\nsigma_1 = 3\n\nmu_2 = 20\nsigma_2 = 6\n\nmu_3 = 30\nsigma_3 = 5\n\n# Limites gráficos\nx_min = np.min(np.array([mu_1, mu_2, mu_3]) - 4*np.array([sigma_1, sigma_2, sigma_3]))\nx_max = np.max(np.array([mu_1, mu_2, mu_3]) + 4*np.array([sigma_1, sigma_2, sigma_3]))\nx = np.linspace(x_min, x_max, 1000) # Faixa de alturas para plotar\n\npdf_1 = stats.norm.pdf(x, loc=mu_1, scale=sigma_1)\npdf_2 = stats.norm.pdf(x, loc=mu_2, scale=sigma_2)\npdf_3 = stats.norm.pdf(x, loc=mu_3, scale=sigma_3)\n\nVisualizando as distribuições de densidade de probabilidade\n\nplt.plot(x, pdf_1, label=f'$\\mu={mu_1}, \\sigma={sigma_1}$', color='b', lw=2)\nplt.plot(x, pdf_2, label=f'$\\mu={mu_2}, \\sigma={sigma_2}$', color='r', lw=2)\nplt.plot(x, pdf_3, label=f'$\\mu={mu_3}, \\sigma={sigma_3}$', color='g', lw=2)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Densidade de Probabilidade', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='-', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigura 1: Desidades da distribuição normal para diferentes valores de μ e σ.\n\n\n\n\n\n\n\n1.2 Amostrando valores de distribuição normal\nNa Figura 1 vemos as curvas teóricas de densidade da distribuiçao normal. Podemos também gerar amostras valores ao acaso destas distribuições para verificar como estas amostras se parecem. Isso simula o processo de sortear alturas de uma população que segue essa distribuição.\n\nmu = 20\nsigma = 4\nnum_amostras = 60\n\nVerificando o histograma dos valores sorteados.\n\namostras_y = stats.norm.rvs(loc=mu, scale=sigma, size=num_amostras)\nx_dens = np.linspace(mu-4*sigma, mu+4*sigma, 500)\n\nplt.hist(amostras_y, bins=30, density=True, alpha=0.8, color='lightblue', label='Amostras Geradas')\nsns.kdeplot(amostras_y, color='blue', linewidth=2, label='Densidade Empírica')\nplt.plot(x_dens, stats.norm.pdf(x_dens, loc=mu, scale=sigma), color='red', linewidth=2.5, label='Densidade Teórica')\nplt.xlabel('X', fontsize=12)\nplt.ylabel('Densidade / Frequência Normalizada', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigura 2: Histograma de amostras geradas a partir de uma distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\sigma\\), acompanhado da densidade empírica estimada por kernel e da densidade teórica correspondente.\n\n\n\n\n\n\n\n\n\n\n\nAtividade em laboratório\n\n\n\n\nRode este o trecho de código acima algumas vezes e observe como se dá a variação amostral.\nAumente e diminua o tamanho da amostras e verifique a variação entre as curvas enpíricas e a curva teórica."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html#intuição-bayesiana",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-normal-bayesiano-priori.html#intuição-bayesiana",
    "title": "Modelo Normal Bayesiano",
    "section": "2 Intuição Bayesiana",
    "text": "2 Intuição Bayesiana\nEm inferência Bayesiana, começamos com crenças sobre os parâmetros (a priori) e as atualizamos com dados (a posteriori). Para a altura humana (\\(y\\)), podemos assumir que a distribuição normal é um bom modelo preditivo.\nDeste modo, escrevemos que:\n\\[y \\sim \\mathcal{N}(\\mu, \\sigma)\n\\tag{1}\\]\nEm seguida, precisamos sugerir uma dristribuição a priori para a média \\(\\mu\\) e o desvio padrão \\(\\sigma\\) que traduzam de forma adequada o que esperamos sobre a distribuição de altura em adultos. Sabemos por exemplo que a média da altura de adultos não é 50 cm nem 300 cm. Qual sua intuição sobre o desvio padrão?\n\n\n\n\n\n\nAtividade em laboratório\n\n\n\n\nAssumindo que a distribuição de alturas em adultos segue uma dsitribuição normal proponha valores razoáveis para a média (\\(\\mu\\)) e o desvio padrão (\\(\\sigma\\)).\nPara ajudar a decidir sobre o que seriam valores valores razoáveis, plote as curvas de densidade de probabilidade resultante de sua escolha e faça algumas simulações para verificar quais valores estremos sua escolha é capaz de gerar, utilizando os códigos da Seção 1.2.\n\n\n\n\n2.1 Checagem Priori Preditiva\nAssumindo que a altura de adultos segue uma distribuição (Equação 1), vamos assumir que o parâmetro \\(\\mu\\) segue também uma distribuição normal e que \\(\\sigma\\) segue uma distribuição log-normal\nComo utilizamos estes pressupostos para escolher distribuição razoiáveis para \\(\\mu\\) e \\(\\sigma\\)?\nPriori para \\(\\mu\\)\n\nmean_prior_mu =  # INSIRA SUA ESCOLHA PARA A MÉDIA DA PRIORI DE mu\nsd_prior_mu =  # INSIRA SUA ESCOLHA PARA O DESVIO PADRÃO DA PRIORI DE mu\n\n# Gere sequancia de x e calcule a PDF\nxmean_prior = np.linspace(mean_prior_mu - 4*sd_prior_mu, mean_prior_mu + 4*sd_prior_mu, 1000)\npdf_mean_prior = stats.norm.pdf(x = xmean_prior, loc = mean_prior_mu, scale = sd_prior_mu)\n\n# Plote os resultados\nplt.plot(xmean_prior, pdf_mean_prior)\nplt.title(f'Priori para $\\mu$')\nplt.show()\n\nPriori para \\(\\sigma\\)\n\nlmean_prior_sigma =  # INSIRA SUA ESCOLHA PARA A MÉDIA DA PRIORI DE sigma\nlsd_prior_sigma =  # INSIRA SUA ESCOLHA PARA O DESVIO PADRÃO DA PRIORI DE sigma\n\nxsd_prior = np.linspace(0.01, 20, 1000)\npdf_sd_prior = stats.lognorm.pdf(xsd_prior, s=lsd_prior_sigma, scale=lmean_prior_sigma)\n\nplt.close()\nplt.plot(xsd_prior, pdf_sd_prior)\nplt.title(f'Priori para $\\sigma$')\nplt.show()\n\nExtraindo distribuição a priori preditiva de \\(y\\) no PyMC\n\n# Definindo o modelo APENAS com as priores compartilhadas\nwith pm.Model() as prior_predictive_model:\n    \n    # Priori para a média\n    mu = pm.Normal(\"mu\", mu = mean_prior_mu, sigma = sd_prior_mu)\n\n    # Priori lognormal para o desvio padrão\n    sigma = pm.Lognormal(\"sigma\", mu = np.log(lmean_prior_sigma), sigma = lsd_prior_sigma)\n\n    # Distribuição preditiva de y\n    y_pred = pm.Normal(\"y_pred\", mu = mu, sigma = sigma)\n\n    # Amostras da priori preditiva\n    prior_predictive_samples = pm.sample_prior_predictive(samples=1000)\n\nAgora, vamos visualizar a distribuição dessas amostras preditivas a priori:\n\n\n\ny_pred_prior = prior_predictive_samples.prior[\"y_pred\"].values.flatten()\n\nplt.figure(figsize=(10, 6))\nplt.hist(y_pred_prior, color='skyblue', edgecolor='black')\nplt.xlabel('Alturas priori simulada (cm)', fontsize=12)\nplt.ylabel('Densidade', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\nFigura 3\n\n\n\n\n\n\n\n\n\nDiscussão\n\n\n\nOlhe para o histograma. As alturas simuladas parecem razoáveis para alturas de adultos? A distribuição faz sentido dada a sua intuição? Se sim, suas priores iniciais eram sensatas. Se não, é importante considerar um ajuste de suas priores (ex: tornar a priori de \\(\\sigma\\) mais restrita se a dispersão for muito grande, ou ajustar a localização/escala da priori de \\(\\mu\\)).\n\n\nChecagem priori preditiva com PyMC\nPodemos chegar não somente a distribuição preditiva de \\(y\\), mas também dos parâmetros \\(\\mu\\) e \\(\\sigma\\) usando o PyMC. Além disso, poderíamos testar outras distribuições a priori para algum dos parâmetros, por exemplo sigma. Teste cada uma destas e verifique os efeitos sobre as distribuições preditivas.\n\n# Definindo o modelo APENAS com as priores compartilhadas\nwith pm.Model() as prior_predictive_model:\n    \n    # Priori para a média\n    mu = pm.Normal(\"mu\", mu=175, sigma=10)\n\n    # Escolha uma das prioris para sigma:\n    sigma = pm.Lognormal(\"sigma\", mu=np.log(0.08), sigma=0.5)\n    # sigma = pm.InverseGamma(\"sigma\", alpha=8, beta=0.9)\n    # sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n    # sigma = pm.HalfCauchy(\"sigma\", beta=0.1)\n    # sigma = pm.Exponential(\"sigma\", lam=20)\n    # sigma = pm.TruncatedNormal(\"sigma\", mu=0.08, sigma=0.02, lower=0)\n    # sigma = pm.Uniform(\"sigma\", lower=0, upper=1)\n\n    # Distribuição preditiva de y\n    y_pred = pm.Normal(\"y_pred\", mu=mu, sigma=sigma)\n\n    # Amostras da priori preditiva\n    prior_predictive_samples = pm.sample_prior_predictive(samples=1000)\n\n\n\n\nmu_pred_prior = prior_predictive_samples.prior[\"mu\"].values.flatten()\nsigma_pred_prior = prior_predictive_samples.prior[\"sigma\"].values.flatten()\ny_pred_prior = prior_predictive_samples.prior[\"y_pred\"].values.flatten()\n\nfig, axes = plt.subplots(1, 3, figsize=(9, 3))\n\naxes[0].hist(mu_pred_prior, bins=30, color='skyblue', edgecolor='black')\naxes[0].set_xlabel(\"μ\")\naxes[0].set_ylabel(\"Frequência\")\n\naxes[1].hist(sigma_pred_prior, bins=30, color='lightgreen', edgecolor='black')\naxes[1].set_xlabel(\"σ\")\naxes[1].set_ylabel(\"Frequência\")\n\naxes[2].hist(y_pred_prior, bins=30, color='salmon', edgecolor='black')\naxes[2].set_xlabel(\"alturas (y)\")\naxes[2].set_ylabel(\"Frequência\")\n\nplt.tight_layout()\nplt.show()\n\n\nFigura 4"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-distr-prob.html",
    "href": "conteudo/intro_bayes/intro-bayes-distr-prob.html",
    "title": "De contagens a probabilidades",
    "section": "",
    "text": "Voltemos ao problema das bolinhas de gude. Temos uma caixa contendo quatro bolinhas, que podem ser azuis ou brancas. Sabemos que há exatamente quatro bolinhas, mas não conhecemos a distribuição entre as cores, pois podemos ver apenas uma bolinha por vez através de um orifício. Para estimar quantas bolas de cada cor há na caixa, fazemos uma observação, misturamos as bolinhas, fazemos outra observação e assim por diante. Antes de realizarmos qualquer observação, podemos listar cinco configurações possíveis para o conteúdo da caixa:\nNosso objetivo é estimar o número \\(N\\) de bolas azuis, o qual pode variar, neste exemplo, de 0 a 4. Como não dispomos de conhecimento prévio sobre a composição da caixa antes da primeira observação, adotamos uma distribuição a priori uniforme entre as hipóteses. Assim, cada hipótese recebe uma probabilidade de \\(p = \\frac{1}{5}\\)."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-distr-prob.html#distribuições-a-priori-a-posteriori-e-verossimilhança",
    "href": "conteudo/intro_bayes/intro-bayes-distr-prob.html#distribuições-a-priori-a-posteriori-e-verossimilhança",
    "title": "De contagens a probabilidades",
    "section": "1 Distribuições a priori, a posteriori e verossimilhança",
    "text": "1 Distribuições a priori, a posteriori e verossimilhança\nSuponha que realizamos três observações da caixa e a sequência registrada seja [🔵, ⚪, 🔵] – ou seja, duas bolas azuis. Para atualizar nosso conhecimento sobre a composição da caixa, combinamos nossa distribuição a priori com a verossimilhança de cada hipótese. A verossimilhança é calculada a partir da contagem do número de maneiras em que cada hipótese pode gerar a sequência observada. Em seguida, aplicamos a regra de Bayes, que nos fornece a distribuição a posteriori por meio da fórmula:\n\\[\\text{Posterior}_i = \\frac{\\text{Priori}_i \\times P_i}{\\sum_{j} \\left(\\text{Priori}_j \\times P_j\\right)}, \\tag{1}\\]\nonde \\(P_i\\) representa, de forma proporcional, o número de caminhos possíveis para que a hipótese \\(i\\) gere a sequência [🔵, ⚪, 🔵].\nA equação Equação 1 indica que, para cada valor que \\(N\\) pode assumir, julgamos sua plausibilidade como proporcional ao número de maneiras pelas quais esse valor pode ter gerado os dados, multiplicado pela sua evidência anterior (a distribuição a priori). Esse produto – representado na Tabela 2 por \\(\\text{Priori}_i \\times P_i\\) – é então normalizado, dividindo cada um deles pelo somatório \\(\\sum (1/5 \\times \\text{Nº de caminhos})\\). Essa normalização gera a distribuição a posteriori, que reflete nosso conhecimento atualizado após a incorporação das evidências observadas.\nNa Tabela 2, a coluna “Maneiras de produzir N = 2 [🔵⚪🔵]” indica o número de caminhos possíveis para que cada hipótese gere a sequência observada. Note que as hipóteses [⚪⚪⚪⚪] e [🔵🔵🔵🔵] não conseguem gerar a sequência (ou seja, possuem verossimilhança zero).\n\n\n\nTabela 2: Atualização da distribuição de probabilidade combinando a priori e verossimilhança\n\n\n\n\n\n\n\n\n\n\n\n\nHipótese\nN\nPriori\nManeiras de produzir N = 2 [🔵⚪🔵]\nPosterior\n\n\n\n\n[⚪⚪⚪⚪]\n0\n\\(1/5\\)\n\\(0 \\times 4 \\times 0 = 0\\)\n\\(\\dfrac{(1/5 \\times 0)}{\\sum (1/5 \\times \\text{Nº de caminhos})} = 0\\)\n\n\n[🔵⚪⚪⚪]\n1\n\\(1/5\\)\n\\(1 \\times 3 \\times 1 = 3\\)\n\\(\\dfrac{(1/5 \\times 3)}{\\sum (1/5 \\times \\text{Nº de caminhos})} = 0.15\\)\n\n\n[🔵🔵⚪⚪]\n2\n\\(1/5\\)\n\\(2 \\times 2 \\times 2 = 8\\)\n\\(\\dfrac{(1/5 \\times 8)}{\\sum (1/5 \\times \\text{Nº de caminhos})} = 0.40\\)\n\n\n[🔵🔵🔵⚪]\n3\n\\(1/5\\)\n\\(3 \\times 1 \\times 3 = 9\\)\n\\(\\dfrac{(1/5 \\times 9)}{\\sum (1/5 \\times \\text{Nº de caminhos})} = 0.45\\)\n\n\n[🔵🔵🔵🔵]\n4\n\\(1/5\\)\n\\(4 \\times 0 \\times 4 = 0\\)\n\\(\\dfrac{(1/5 \\times 0)}{\\sum (1/5 \\times \\text{Nº de caminhos})} = 0\\)\n\n\n\n\n\n\nDessa forma, a plausibilidade de cada hipótese é convertida em probabilidades – valores não negativos cuja soma é igual a 1 – para cada uma das hipóteses sobre o conteúdo da caixa. O resultado final da inferência bayesiana é fornecer uma base probabilística para a tomada de decisão sobre um fenômeno parcialmente desconhecido, expressando o quão plausível é cada hipótese à luz dos dados disponíveis (Figura 1).\n\n\n\n\n\n\n\n\nFigura 1: Distribuição a priori (esquerda) e a posteriori (direita) para o número de bolas azuis. A priori, as hipóteses têm probabilidade uniforme; a posteriori, a observação [🔵, ⚪, 🔵] atualiza a plausibilidade, favorecendo hipóteses intermediárias\n\n\n\n\n\nEsses conceitos possuem nomenclaturas específicas, e vale a pena aprendê-los, pois você os encontrará repetidamente:\n\nA conjectura sobre o número de bolinhas azuis \\(N\\) é chamada de valor do parâmetro – uma maneira de indexar as possíveis explicações para os dados.\nO número relativo de maneiras pelo qual esse parâmetro pode produzir os dados é chamado de verossimilhança (likelihood). Essa medida é obtida ao enumerar todas as sequências de dados possíveis e, em seguida, descartar aquelas que não são compatíveis com os dados observados.\nA plausibilidade anterior de um valor específico é denominada distribuição de probabilidade a priori.\nA plausibilidade atualizada de um valor específico, após a incorporação dos dados, é denominada distribuição de probabilidade a posteriori, utilizada para inferir a probabilidade de cada hipótese ou conjunto de hipóteses sobre o parâmetro."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html",
    "title": "Inferência Bayesiana Binomial com PyMC",
    "section": "",
    "text": "O PyMC é uma biblioteca de programação probabilística em Python que facilita a construção de modelos bayesianos, utilizando métodos como a amostragem MCMC (Markov Chain Monte Carlo) para estimar distribuições a posteriori de forma eficiente.\nPara modelar uma variável aleatória com distribuição Binomial, podemos especificar uma distribuição a priori do tipo Beta para o parâmetro \\(p\\), observar os dados (número de sucessos em \\(N\\) ensaios) e, então, obter a distribuição a posteriori utilizando técnicas de amostragem fornecidas pelo PyMC."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#estimativa-bayesiana-com-pymc",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#estimativa-bayesiana-com-pymc",
    "title": "Inferência Bayesiana Binomial com PyMC",
    "section": "1 Estimativa Bayesiana com PyMC",
    "text": "1 Estimativa Bayesiana com PyMC\n\nDefinir os dados\n\n\\(N\\): número total de observações (ensaios Bernoulli).\n\n\\(k\\): número de sucessos observados.\n\nDefinir o modelo probabilístico\n\nEspecifique uma distribuição a priori para o parâmetro \\(p\\), como uma Beta(\\(\\alpha\\), \\(\\beta\\)).\n\nModele os dados observados usando uma distribuição Binomial(\\(N\\), \\(p\\)):\nwith pm.Model() as modelo:\n    p = pm.Beta(\"p\", alpha=α_prior, beta=β_prior)\n    y = pm.Binomial(\"y\", n=N, p=p, observed=k)\n\nExecutar a amostragem MCMC\n\nUse o método pm.sample() para gerar amostras da distribuição a posteriori.\n\nO PyMC utiliza, por padrão, o algoritmo NUTS (No-U-Turn Sampler), baseado em Hamiltonian Monte Carlo.\ntrace = pm.sample()\n\nInspecionar os resultados da amostragem\n\nUse az.summary(trace) para obter estatísticas descritivas: média, desvio padrão, intervalos de credibilidade, \\(\\hat{R}\\) (diagnóstico de convergência), entre outros.\n\nVisualize as cadeias com az.plot_trace(trace), que mostra as séries temporais e histogramas das amostras.\n\nVisualize a distribuição a posteriori com az.plot_posterior(trace, var_names=[\"p\"]).\n\nCalcular probabilidades e intervalos de credibilidade\n\nUse as amostras da posteriori para calcular quantidades como \\(P(x_1 \\leq p \\leq x_2)\\), filtrando os valores de p entre esses limites e estimando a proporção.\n\nExemplo:\namostras_p = trace.posterior[\"p\"].values.flatten()\nprob = ((amostras_p &gt;= x1) & (amostras_p &lt;= x2)).mean()\n\nVisualizar os resultados\n\nFaça gráficos para comparar a priori e a posteriori, ou destacar regiões de interesse com os intervalos de credibilidade.\n\nCombine visualizações com matplotlib, arviz e outras bibliotecas para tornar as conclusões mais claras."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#exemplo-em-python",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#exemplo-em-python",
    "title": "Inferência Bayesiana Binomial com PyMC",
    "section": "2 Exemplo em Python",
    "text": "2 Exemplo em Python\n\nimport pymc as pm\nimport arviz as az  # Pacote auxiliar para análise e visualização dos resultados\n\n# 1. Definir os dados observados\nN = 10    # Número total de ensaios (Bernoulli)\nk = 6     # Número de sucessos observados\n\n# 2. Parâmetros da distribuição a priori (Beta)\nalpha_param = 2\nbeta_param = 2\n\n# 3. Definir o modelo probabilístico no PyMC\nwith pm.Model() as model:\n    # 3.1. Definição da distribuição a priori para p\n    p = pm.Beta(\"p\", alpha=alpha_param, beta=beta_param)\n    \n    # 3.2. Observações via distribuição Binomial\n    obs = pm.Binomial(\"obs\", n=N, p=p, observed=k)\n    \n    # 4. Amostragem MCMC (posteriori)\n    trace = pm.sample()\n\n# 5. Inspeção dos resultados\nprint(az.summary(trace, var_names=[\"p\"], kind=\"stats\"))\n\n# 6. Visualizações da amostragem e da posteriori\naz.plot_trace(trace, var_names=[\"p\"])  # Trajetória e histograma das amostras\naz.plot_posterior(trace, var_names=[\"p\"], rope=[0.3, 0.7]);  # Posteriori com intervalo de relevância\n\n\n\n\n\n\n\n    mean     sd  hdi_3%  hdi_97%\np  0.573  0.124   0.347    0.806"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#interpretação",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#interpretação",
    "title": "Inferência Bayesiana Binomial com PyMC",
    "section": "3 Interpretação",
    "text": "3 Interpretação\n\nA forma e a localização da posteriori dependerão tanto dos dados observados (\\(k\\), \\(N\\)) quanto dos parâmetros da distribuição a priori (\\(\\alpha\\), \\(\\beta\\)).\n\nConforme \\(N\\) aumenta, a verossimilhança passa a dominar o resultado, reduzindo o impacto de uma a priori moderada.\n\nSe você alterar \\(\\alpha_{\\mathrm{prior}}\\) e \\(\\beta_{\\mathrm{prior}}\\), verá como suposições prévias mudam a forma inicial da posteriori, sobretudo em situações com poucos dados."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#exercício",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-pymc.html#exercício",
    "title": "Inferência Bayesiana Binomial com PyMC",
    "section": "4 Exercício",
    "text": "4 Exercício\n\nVarie \\(N\\) e \\(k\\) para simular cenários distintos (poucos sucessos, muitos sucessos) e veja como a posteriori se adapta.\n\nAltere \\(\\alpha_{\\mathrm{prior}}\\) e \\(\\beta_{\\mathrm{prior}}\\) (por exemplo, prior fortemente concentrada em 0.8) e observe se, com poucos dados, a posteriori permanece próxima da distribuição a priori."
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html",
    "title": "O modelo da distribuição normal",
    "section": "",
    "text": "Bibliotecas utilizadas nesta seção\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nA partir da Figura 1, percebemos que a maioria dos alunos tem alturas intermediárias, enquanto poucos são muito altos ou muito baixos, o que está de acordo com nossa intuição sobre a distribuição das alturas em adultos. Vamos construir passo-a-passo uma função matemática que seja capaz de capturar este comportamento."
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#um-modelo-para-a-distribuição-de-alturas",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#um-modelo-para-a-distribuição-de-alturas",
    "title": "O modelo da distribuição normal",
    "section": "1 Um modelo para a distribuição de alturas",
    "text": "1 Um modelo para a distribuição de alturas\nComeçaremos com a função de crescimento exponencial:\n\\[f(x) = e^x\\]\ne de decaimento exponencial:\n\\[f(x) = e^{-x}\\]\nCombinando as duas, temos:\n\\[f(x) = e^{-\\mid x \\mid}\\]\nPara ter uma transição mais suave, fazemos uma pequena modificação na função:\n\\[f(x) = e^{-x^2}\\]\nO código a seguir cria vetores a partir destas funções que podemos visualizar graficamente:\nx = np.linspace(-4, 4, 1000)\n\n# Crescimento exponencial\nfx = np.exp(x)\nplt.plot(x, fx)\nplt.title(r'$f(x) = e^x$')\nplt.show()\n\n# Decaimento exponencial\nfx = np.exp(-x)\nplt.plot(x, fx)\nplt.title(r'$f(x) = e^{-x}$')\nplt.show()\n\n# Combinação dos dois\nfx = np.exp(-np.abs(x))\nplt.plot(x, fx)\nplt.title(r'$f(x) = e^{-|x|}$')\nplt.show()\n\n# Transição suave\nfx = np.exp(-x**2)\nplt.plot(x, fx)\nplt.title(r'$f(x) = e^{-x^2}$')\nplt.show()\n\n\n\n\n\n\nCrescimento exponencial\n\n\n\n\n\n\n\nDecaimento exponencial\n\n\n\n\n\n\n\n\n\nCombinando o crescimento e decaimento\n\n\n\n\n\n\n\nFazendo uma transição suave"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#inserindo-o-parâmetro-de-dispersão-sigma",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#inserindo-o-parâmetro-de-dispersão-sigma",
    "title": "O modelo da distribuição normal",
    "section": "2 Inserindo o parâmetro de dispersão \\(\\sigma\\)",
    "text": "2 Inserindo o parâmetro de dispersão \\(\\sigma\\)\nEm \\(f(x) = e^{-x^2}\\), não há nada de especial com a escolha da base de Euler (\\(e = 2.718282...\\)). Poderíamos ter escolhido qualquer outro número, por exemplo, \\(30^{-x^2}\\), o que nos daria uma função com formato similar:\n\n# Comparação entre e^{-x^2} e 30^{-x^2}\nfx1 = np.exp(-x**2)\nfx2 = 30**(-x**2)\n\nplt.plot(x, fx1, label=r'$f(x) = e^{-x^2}$')\nplt.plot(x, fx2, label=r'$f(x) = 30^{-x^2}$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNote, entretanto, que a função \\(f(x) = e^{-x^2}\\) tem um decaimento mais suave se comparado à \\(f(x) = 30^{-x^2}\\), um comportamento que pode ser controlado inserindo uma constante \\(c = \\frac{1}{2\\sigma^2}\\):\n\\[f(x) = e^{-\\frac{1}{2\\sigma^2}x^2}\\]\nFazendo desta forma, o parâmetro \\(\\sigma\\) passa a controlar a largura ou dispersão da curva: valores maiores de \\(\\sigma\\) tornam o decaimento mais lento e a curva mais “espalhada”, enquanto valores menores de \\(\\sigma\\) a tornam mais estreita e concentrada ao redor de zero.\n\n\n\n\n\n\nCuriosidade\n\n\n\nA escolha da constante \\(c = \\frac{1}{2\\sigma^2}\\) tem o efeito prático de fazer com que a concavidade da curva mude exatamente nos pontos \\(x = +\\sigma\\) e \\(x = -\\sigma\\). Na função da distribuição normal, \\(\\sigma\\) será chamado de desvio padrão.\n\n\n\n# Variando o valor de sigma\nsigmas = [0.5, 1, 2]\n\nfor sigma in sigmas:\n    fx = np.exp(-(1/(2*(sigma**2)))*x**2)\n    plt.plot(x, fx, label=fr'$\\sigma = {sigma}$')\n\nplt.legend()\nplt.title(r'$f(x) = e^{-\\frac{1}{2\\sigma^2}x^2}$')\nplt.show()"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#inserindo-o-parâmetro-de-posição-mu",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#inserindo-o-parâmetro-de-posição-mu",
    "title": "O modelo da distribuição normal",
    "section": "3 Inserindo o parâmetro de posição \\(\\mu\\)",
    "text": "3 Inserindo o parâmetro de posição \\(\\mu\\)\nPor enquanto temos a função:\n\\[f(x) = e^{-\\frac{1}{2\\sigma^2}x^2}\\]\nque nos permite agora alterar a abertura da curva, mas está centralizada em zero. Se quisermos que elaesta função possa representar fenômenos que não estejam centrados em zero, precisamos ser capazes de deslocar a função para a direita ou para a esquerda. Fazemos isso inserindo um novo parâmetro que será denominado de a média \\(\\mu\\) da dsitribuição:\n\\[f(x) = e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\]\n\n# Variando o valor de mi (média)\nmis = [-2, 0, 2]\nsigma = 1\n\nfor mi in mis:\n    fx = np.exp(-(1/(2*(sigma**2)))*(x-mi)**2)\n    plt.plot(x, fx, label=fr'$\\mu = {mi}$')\n\nplt.legend()\nplt.title(r'$f(x) = e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}$')\nplt.show()"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#usando-a-função-como-uma-distribuição-de-probabilidades",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#usando-a-função-como-uma-distribuição-de-probabilidades",
    "title": "O modelo da distribuição normal",
    "section": "4 Usando a função como uma Distribuição de Probabilidades",
    "text": "4 Usando a função como uma Distribuição de Probabilidades\nSe queremos utilizar a função acima para prever a frequência relativa de alturas, precisamos que a área abaixo da curva seja igual a 1, o que a transforma em uma Função de Densidade de Probabilidade (PDF).\nVemos entretanto que a área da função é igual a \\(\\sigma \\sqrt{2\\pi}\\).\nO que pode ser conferido obtendo a integral da função: \\(\\int_{-\\infty}^{+\\infty} f(x) d(x)\\)\n\nfrom scipy.integrate import quad\n\n# Definindo a função f\ndef f(x, mi, sigma):\n    if sigma &lt;= 0:\n        sigma = 1\n    fx = np.exp(-(1/(2*(sigma**2)))*(x-mi)**2)\n    return (fx)\n\n# Area sob a curva\nmi = 0\nsigma = 1\n\narea, erro = quad(f, -np.inf, np.inf, args = (mi, sigma))\n\nprint(f\"Área sob a curva = {area:.5f}\")\n\n# 2 x raiz(2 x pi)\nprint(sigma * np.sqrt(2*np.pi))\n\nÁrea sob a curva = 2.50663\n2.5066282746310002\n\n\nPara corrigir a área sob a curva, inserimos \\(\\sigma \\sqrt{2\\pi}\\) no denominador da função, ficando com:\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\]\nA função acima é conhecida como Distribuição Normal ou Curva de Gauss. Nesta função \\(\\mu\\) é a média, que representa o ponto central da curva, e \\(\\sigma\\) é o desvio padrão que controla a abertura da curva.\nPodemos verificar agora que a área desta função é sempre igua a 1.\n\nfrom scipy.integrate import quad\n\n# Definindo a função f\ndef fnormal(x, mi, sigma):\n    if sigma &lt;= 0:\n        sigma = 1\n    fx = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-(1/(2*(sigma**2)))*(x-mi)**2)\n    return (fx)\n\n# Area sob a curva\nmi = 0\nsigma = 30\narea, erro = quad(fnormal, -np.inf, np.inf, args = (mi, sigma))\n\nprint(f\"Área sob a curva = {area:.5f}\")\n\nÁrea sob a curva = 1.00000"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#biblioteca-scipy",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-modelo.html#biblioteca-scipy",
    "title": "O modelo da distribuição normal",
    "section": "5 Biblioteca SciPy",
    "text": "5 Biblioteca SciPy\nExiste uma função pronta em python que nos dá a função da distribuição normal disponível em scipy.stats. Como já importamos esta bibloteca no início do código, podemos acessá-la para comparar com nossa função fnormal:\n\nimport scipy.stats as st\n\nx = np.linspace(2, 18, 1000)\ny1 = fnormal(x, mi = 10, sigma = 2)\ny2 = st.norm.pdf(x = x, loc = 10, scale = 2)\n\nE colocar as figuras lado-a-lado:\n\nfig, axes = plt.subplots(1, 2)\n\naxes[0].plot(x, y1)\naxes[0].set_title('função `fnormal`')\naxes[1].plot(x, y2)\naxes[1].set_title('função scipy.stats.norm.pdf()')\n\nplt.tight_layout()"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html",
    "href": "conteudo/distribuicao_normal/distr_norm.html",
    "title": "O modelo de distribuição normal",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nsource('scripts/normal_empirica_gg.r')\nTécnicas de estatística descritiva nos permitem entender os padrões resultantes de fenômenos que já aconteceram, enquanto a inferência estatística nos fornece elementos para fazer predições sobre o que poderá acontecer. A predição se torna possível pelo uso de modelos probabilísticos, entre os quais está a distribuição normal de probabilidades.\nModelos probabilísticos são definidos por funções de probabilidade e as variáveis descritas por estes modelos são denominadas de variáveis aleatórias. Uma variável aleatória resulta de um experimento aleatório como i) medir a altura de uma pessoa; ii) tomar a temperatura em uma cidade; ii) medir a taxa de crescimento de uma bactéria; etc. A questão relevante nestes experimentos é que antes de serem realizados, não temos certeza sobre qual serão seus resultados.\nEmbora não saibamos quais serão os resultados de um experimento aleatório com exatidão, podemos nos basear em algum modelo probabilidades para prever a chance de um resultado observado estar dentro de determinados limites. Neste sentido, o papel de um modelo probabilístico é, delimitar a incerteza ao redor dos resultados possíveis de um experimento aleatório.\nAo medir a altura de uma pessoa podemos supor que, possivelmente, o resultado ficará abaixo de \\(1,9\\) m. Supomos isto pois temos conhecimento de que a altura de maior parte das pessoas está abaixo deste limite. Se quisermos atribuir um valor de probabilidade a esta suposição devemos:\nNeste capítulo iremos discutir pela primeira vez o modelo de distribuição normal e aprenderemos como encontrar estas probabilidades.\nImporte a base de dados altura2022.csv\nie = read_delim(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/altura2022.csv\")\nA distribuição normal de probabilidades descreve uma curva em forma de sino também chamada de distribuição gaussiana. Um dos motivos que a tornaram central em estatística foi a percepção de que o comportamento de muitos fenômenos naturais podem ser descritos adequadamente por este modelo teórico. Veja por exemplo, o histograma de alturas de \\(110\\) estudantes de uma turma de Introdução a Estatística do curso de Bacharelado Interdisciplinar em Ciências do Mar (UNIFESP). A linha vermelha sobre este histograma representa a distribuição normal teórica. À direita desta figura está um histograma da temperatura média anual em uma cidade americana, onde também foi sobreposta uma curva normal teórica. Embora estes dados descrevam fenômenos completamente distintos, a distribuição normal se adequa razoavelmente bem aos dois histogramas.\nFigura 1: Altura (m) de alunos de um curso de estatística e temperatura média anual de uma cidade americana.\nO segundo motivo que torna a distribuição normal uma das mais importantes em estatística será nosso tema de estudo neste e nos próximos capítulos, pois a distribuição normal surge como o modelo esperado para a distribuição das médias amostrais sob determinadas condições, o que nos permite utilizar uma variedade de procedimentos analíticos no campo da inferência e testes de hipótese."
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html#o-modelo-normal-de-probabilidades",
    "href": "conteudo/distribuicao_normal/distr_norm.html#o-modelo-normal-de-probabilidades",
    "title": "O modelo de distribuição normal",
    "section": "1 O modelo normal de probabilidades",
    "text": "1 O modelo normal de probabilidades\nO modelo normal de probabilidades é uma função matemática dada por:\n\\[f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}, x \\in \\mathbb{R} | -\\infty \\le y \\le +\\infty\\]\nA expressão envolve as quantias \\(\\mu\\) e \\(\\sigma\\), definidas como os parâmetros da distribuição que representam respectivamente, sua média e o desvio padrão. Para dizer que uma variável aleatória \\(X\\) tem distribuição normal por meio da expressão:\n\\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma)\\)\nEsta expressão diz de \\(X\\) é normalmente distribuída (\\(\\mathcal{N}\\)) e que esta distribuição tem parâmetros \\(\\mu\\) e \\(\\sigma\\).\nA média de uma distribuição normal é o ponto central da curva e o desvio padrão mede o espalhamento das observações ao redor de \\(\\mu\\). Em um fenômeno descrito por valores baixos de \\(\\sigma\\), a maioria das observações estará próxima a \\(\\mu\\), enquanto para valores altos de \\(\\sigma\\) as observações estarão mais distantes de \\(\\mu\\). Deste modo, podemos alterar o formato da distribuição normal alterando seu parâmetro de posição (i.e. a média \\(\\mu\\)) e de dispersão (i.e. o desvio padrão \\(\\sigma\\)).\n\n\n\n\n\n\n\n\nFigura 2: Distribuições normais de probabilidade para diferentes combinações de média e desvio padrão.\n\n\n\n\n\nSe as observações sobre um determinado fenômeno sugerem um padrão em forma de sino, podemos buscar a melhor combinação de \\(\\mu\\) e \\(\\sigma\\) e descrever o fenômeno por meio de um modelo normal. Ao fazer isto, a distribuição normal nos ajuda a calcular as probabilidade da ocorrência de eventos futuros estarem em diferentes faixas de valores. No caso das alturas dos alunos por exemplo, vemos que a probabilidade de um aluno ter mais de \\(2\\) metros ou menos de \\(1,5\\) metros é extremamente baixa. Assumindo um modelo de distribuição normal para a distribuição de alturas, podemos utilizar o conjunto de dados para estimar os parâmetros da população e calcular quais seriam estas probabilidades.\n\n\n\n\n\n\nUm pouco de história\n\n\n\nAlguns atribuem a proposição deste modelo normal a Abraham de Moivre, um matemático Francês que chegou a a distribuição normal como uma aproximação a distribuição binomial em seu livro The Doctrine of Chances em \\(1718\\). A distribuição normal de probabilidades é simétrica, ou seja, os valores extremos são igualmente representados acima e abaixo da região central (média). Você poderá encontrar o termo bell curve em inglês, devido à sua forma de sino, ou ainda distribuição gaussiana em homenagem a Carl Friedrich Gauss um dos mais importantes matemáticos do século XXI. Gauss lidou com a distribuição normal quando desenvolveu a Teoria da distribuição dos erros observacionais no contexto do Método dos Mínimos Quadrados em \\(1823\\)."
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html#entendendo-a-função-normal",
    "href": "conteudo/distribuicao_normal/distr_norm.html#entendendo-a-função-normal",
    "title": "O modelo de distribuição normal",
    "section": "2 Entendendo a função normal",
    "text": "2 Entendendo a função normal\nA função \\(f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\) é uma função de densidade de probabilidade. Antes de aplicar esta distribuição para encontrar valores de probabilidade, vamos aprender simplesmente para descrever a funções de densidade assumindo valores particulares de \\(\\mu\\) e \\(\\sigma\\). Para isto, vamos tentar simular o histograma de alturas similar ao da Figura 1. Vamos assumir que a distribuição de alturas tenha a seguinte media e desvio padrão:\n\\(\\mu = 1.7\\) metros\n\\(\\sigma = 0.09\\) metros\nPara uma determinada altura \\(x = 1.6\\) metros, a \\(f(x)\\) assume o valor:\n\\(f(1.6) = \\frac{1}{\\sqrt(2\\pi \\times0.09^2)}e^{-\\frac{1}{2}(\\frac{1.6 - 1.7}{0.09})^2} = 2.391\\)\nEste resultado corresponde ao ponto \\(y\\) no gráfico da distribuição normal (Figura 3) em que \\(x = 1.6\\). Podemos encontar \\(f(x)\\) para quaisquer valores dentro dos reais \\(\\mathbb{R}\\) entre \\(-\\infty\\) e \\(+\\infty\\).\nAssim, se calcularmos \\(f(x)\\) para diferentes pontos em \\(x\\) teremos um esboço da função de densidade normal. Na Figura 3, por exemplo, apresentamos \\(f(x)\\) para os valores:\n\\(X = 1.4, 1.45, 1.5, 1.55, 1.6, 1.65, 1.7, 1.75, 1.8, 1.85, 1.9, 1.95, 2\\)\nassumindo \\(\\mu = 1.7\\) e \\(\\sigma = 0.09\\)\n\n\n\n\n\n\n\n\nFigura 3: Pontos na distribuição normal de densidade de probrabilidade.\n\n\n\n\n\n\n2.1 Calculando de \\(f(x)\\) no R: a função dnorm()\nNo R, os resultados acima podem ser obtidos com a função dnorm(), que fornece um modo simples para calcularmos \\(f(x)\\) na distribuição normal. Nesta função a letra ‘d’ vem de densidade da distribuição normal.\nPara encontrar \\(f(x)\\) para um dado valor fazemos simplesmente:\n\nmu &lt;- 1.7\ndp &lt;- 0.11\ndnorm(1.5, mean = mu, sd = dp)\n\n[1] 0.6945048\n\n\nSe quisermos obter \\(f(x)\\) para múltiplos valores de \\(x\\) podemos fazer:\n\nx &lt;- c(1.4, 1.5, 1.6, 1.7)\ndnorm(x, mean = mu, sd = dp)\n\n[1] 0.0879777 0.6945048 2.3991470 3.6267480"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html#cálculo-de-probabilidade-com-a-função-normal-de-densidade",
    "href": "conteudo/distribuicao_normal/distr_norm.html#cálculo-de-probabilidade-com-a-função-normal-de-densidade",
    "title": "O modelo de distribuição normal",
    "section": "3 Cálculo de probabilidade com a função normal de densidade",
    "text": "3 Cálculo de probabilidade com a função normal de densidade\nEncontrar a probabilidade de uma variável aleatória \\(X\\) estar dentro de uma deteminada faixa de valores significa fazer predições a respeito da probabilidade de ocorrência de uma observação futura. Por ser uma função de probabilidade, a área abaixo de \\(f(x)\\) na distribuição normal é igual a \\(1\\).\n\\[P(-\\infty \\le X \\le +\\infty) = \\int_{-\\infty}^{+\\infty}f(x) dx = 1\\]\nAssim, se desejamos obter probabilidade de uma variável estar dentro de um determinado limite, devemos calcular a área abaixo da curva para este limite. Por exemplo, a probabilidade de uma observação em \\(X\\) estar entre \\(x_1\\) e \\(x_2\\) será:\n\n\n\n\n\n\n\n\nFigura 4: Representação das probabilidades de um intervalo da distribuição normal de densidade.\n\n\n\n\n\n\n3.1 Calculando probabilidades no R: a função pnorm()\nUsando o R, a probabilidade de amostrarmos um aluno que tenha entre menos de \\(1.5\\) metros pode ser obtida por meio da função pnorm:\n\nmu &lt;- 1.7\ndp &lt;- 0.11\npnorm(q = 1.5, mean = mu, sd = dp, lower.tail = TRUE)\n\n[1] 0.03451817\n\n\n\n\n\n\n\n\nArgumentos da função:\n\n\n\nq: o valor de \\(x\\)\nmean: média \\(\\mu\\) da função normal\nsd: desvio padrão \\(\\sigma\\) da função normal\nlower.tail: se a função irá retornar a probabilidade abaixo (TRUE) ou acima (FALSE) de q\nveja o menu de ajuda digitando ?pnorm no Console do R\n\n\nSe quisermos encontrar a probabilidade \\(P(X \\ge 1.5)\\) alteramos o parâmetro lower.tail\n\npnorm(q = 1.5, mean = mu, sd = dp, lower.tail = FALSE)\n\n[1] 0.9654818\n\n\nSe desejamos obter a probabilidade de \\(x\\) estar entre \\(1.5\\)m e \\(1.7\\)m podemos fazer: \\[P(1.5 \\le X \\le 1.7) = P(X \\le 1.7) - P(X \\le 1.5)\\]\nNo R temos:\n\np1 &lt;- pnorm(q = 1.7, mean = mu, sd = dp, lower.tail = TRUE)\np2 &lt;- pnorm(q = 1.5, mean = mu, sd = dp, lower.tail = TRUE)\npfinal &lt;- p1 - p2\n\npfinal\n\n[1] 0.4654818\n\n\nou simplesmente:\n\ndiff(pnorm(q = c(1.7, 1.5),\n           mean = mu,\n           sd = dp,\n           lower.tail = TRUE)\n     )\n\n[1] -0.4654818\n\n\nAqui estão representados cada um dos intervalos calculados.\n\n\nCódigo\ndfc &lt;- data.frame(X = seq(0,sup, length = 10000)) |&gt; \n  mutate(dx = dnorm(X, mean = mu, sd = dp))\n\n\ngc1 &lt;- ggplot(dfc, mapping = aes(y = dx, x = X)) +\n   stat_function(fun = dnorm, args = list(mean = mu, sd = dp)) +\n  geom_area(data = subset(dfc, X &lt;= 1.7), aes(y = dx), \n            fill = \"#eb4034\", color = NA, alpha = 0.5) + \n  scale_x_continuous(\n      name = 'X',\n      limits = c(1.4,2),\n      breaks = seq(1.4, 2, by = 0.05)) +\n   ylab('f(x)') +\n   annotate(geom = 'text', x = 1.5, y = 3, \n            label = bquote(\"P(X\" &lt;= ~ 1.7 ~\")\" == .(round(p1,3))),\n            color = '#eb4034') +\n   theme_classic()\n\ngc2 &lt;- ggplot(dfc, mapping = aes(y = dx, x = X)) +\n   stat_function(fun = dnorm, args = list(mean = mu, sd = dp)) +\n   geom_area(data = subset(dfc, X &lt;= 1.5), \n             aes(y = dx), fill = \"#eb4034\", color = NA, alpha = 0.5) + \n  scale_x_continuous(\n      name = 'X',\n      limits = c(1.4,2),\n      breaks = seq(1.4, 2, by = 0.05)) +\n   ylab('f(x)') +\n   annotate(geom = 'text', x = 1.5, y = 3, \n            label = bquote(\"P(X\" &lt;= ~ 1.5 ~\")\" == .(round(p2,3))),\n            color = '#eb4034') +\n   theme_classic()\n\n\ngc3 &lt;- ggplot(dfc, mapping = aes(y = dx, x = X)) +\n   stat_function(fun = dnorm, args = list(mean = mu, sd = dp)) +\n   geom_area(data = subset(dfc, X &gt;= 1.5 & X &lt;= 1.7 ), \n             aes(y = dx), fill = \"#eb4034\", color = NA, alpha = 0.5) + \n  scale_x_continuous(\n      name = 'X',\n      limits = c(1.4,2),\n      breaks = seq(1.4, 2, by = 0.05)) +\n   ylab('f(x)') +\n   annotate(geom = 'text', x = 1.5, y = 3, \n            label = bquote(\"P(\" ~ 1.5 &lt;= ~ \"X\" &lt;= ~ 1.7 ~\")\" == .(round(pfinal,3))),\n            color = '#eb4034') +\n   theme_classic()\n  \ngc1 / gc2 / gc3"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html#a-distribuição-normal-padronizada",
    "href": "conteudo/distribuicao_normal/distr_norm.html#a-distribuição-normal-padronizada",
    "title": "O modelo de distribuição normal",
    "section": "4 A distribuição normal padronizada",
    "text": "4 A distribuição normal padronizada\nA integral para a função normal é difícil de ser calculada pois não tem solução analítica. Isto era um problema para os cientistas até meados do século \\(XX\\) que precisavam calcular valores de probabilidades para diferentes combinações de \\(\\mu\\) e \\(\\sigma\\). Naquele momento, a solução para facilitar a vida dos pesquisadores foi criar uma tabela descrevendo estas probabilidades em uma distribuição normal padronizada, ou seja para valores particulares de \\(\\mu\\) e \\(\\sigma\\). Padronizar aqui, significa transfomar cada valor \\(x_i\\) de modo que as observações resultantes tenham média igual a \\(0\\) e desvio padrão igual a \\(1\\).\nEsta transformação é apicada a cada observação \\(x_i\\), obtendo-sem um valor de \\(z_i\\) correspondente por meio da expressão.\n\\[z_i = \\frac{x_i - \\mu}{\\sigma}\\]\nA transformação \\(Z\\) é útil, pois ainda que seja difícil calcular as probabilidades para uma variável aleatória \\(X\\), após a transformação teremos uma variável \\(Z\\) para a qual os valores de probabilidade estão tabelados. Deste modo, \\(Z\\) é uma variável aleatória com \\(\\overline{z} = 0\\) e \\(s = 1\\) tal que:\n\\[Z \\sim \\mathcal{N}(0,\\,1)\\]\nApós a transformação \\(Z\\) nos exemplos sobre altura dos alunos e da temperatura mensal temos:\n\n\nCódigo\nie &lt;- ie |&gt; \n   mutate(ALTURA_z = (ALTURA - mean(ALTURA, na.rm = T))/sd(ALTURA, na.rm = T))\ntemp &lt;- temp |&gt; \n   mutate(tm_z = (tm - mean(tm, na.rm = T))/sd(tm, na.rm = T))\n\naltz_plt &lt;- ggplot(ie, aes(x = ALTURA_z)) +\n   geom_histogram(aes(y = after_stat(density)), \n                  fill = 'dodgerblue4', \n                  color = 'black', bins = 10) +\n   stat_function(fun = dnorm, \n                 args = list(mean = mean(ie$ALTURA_z, na.rm = T),\n                                          sd = sd(ie$ALTURA_z, na.rm = T))) +\n   labs(x = \"Distribuição Z\",\n        y = \"Frequencia relativa\") +\n   theme_classic()\n\ntempz_plt &lt;- ggplot(temp, aes(x = tm_z)) +\n   geom_histogram(aes(y = after_stat(density)),\n                  fill = 'dodgerblue4', \n                  color = 'black', bins = 10) +\n   stat_function(fun = dnorm, \n                 args = list(mean = mean(temp$tm_z, na.rm = T),\n                                          sd = sd(temp$tm_z, na.rm = T))) +\n   labs(x = \"Distribuição Z\",\n        y = \"Frequencia relativa\") +\n   theme_classic()\n\n(alt_plt | temp_plt) / \n  (altz_plt | tempz_plt)\n\n\n\n\n\n\n\n\nFigura 5: Distribuição das variáveis originais e após a transformação Z.\n\n\n\n\n\n\n\n\n\n\n\nEscore Z\n\n\n\nO escore Z pode ser apresentado como uma medida de posição de uma observação na amostra (\\(z_i\\)) que representava uma medida relativa desta observação com relaçao à média e ao desvio padrão do conjunto de dados. Por exemplo, um valor de \\(z_i = 2\\) significa que a observação original \\(x_i\\) está \\(2\\) desvios padrões acima de sua respectiva média \\(\\mu\\).\n\n\n\n4.1 Probabilidades em uma distribuição normal padronizada\nNos dois exemplos anteriores, verifica-se que todas as observações estão situadas, aproximadamente, entre \\(z = -3\\) e \\(z = +3\\). De fato, a distribuição normal padronizada ou distribuição Z tem propriedades bem conhecidas. Como sua média é \\(\\mu = 0\\) e seu desvio padrão é \\(\\sigma = 1\\), a maior parte das observações fica limitada entre \\(z = -3\\) e \\(z = +3\\). Para ser exato, podemos descrever as probabilidades de uma observação estar dentro de alguns limites conhecidos. Por exemplo, \\(95\\%\\) das observações estará entre \\(z = -1.96\\) e \\(z = +1.96\\), isto é,\n\\[P(-1.96 \\le Z \\le +1.96) = 0.95\\]\nDe forma similar, \\(90\\%\\) da área central da curva se encontra entre \\(z = -1.64\\) e \\(z = +1.64\\). Estes e outros limites na distribuição normal padronizada podem ser verificados na figura abaixo.\n\n\nCódigo\n# Ver função completa no arquivo 'scripts/normal_empirica_gg.r'\nnormal_empirica_gg(xlabels = c(-4:4))\n\n\n\n\n\n\n\n\nFigura 6: Áreas de probabilidade em uma distribuição Normal Padronizada (Distribuição Z).\n\n\n\n\n\nVamos exemplificar o uso da distribuição \\(Z\\) no cálculo de probabilidades utilizando os dados de altura dos alunos. Para estes dados, iremos encontrar \\(P(X \\le 1.5)\\). Este procedimento consiste de:\n\n\nCódigo\nmu &lt;- 1.7\ndp &lt;- 0.11\nx &lt;- 1.5\nz_1.5 &lt;- (x - mu)/dp\n\n\n\nTransformar \\(x = 1.5\\) em \\(z_{1.5}\\) por meio de \\(z_{1.5} = \\frac{1.5 - 1.7}{0.11} = -1.818\\);\n\n\n\nCódigo\nmu &lt;- 1.7\ndp &lt;- 0.11\nx &lt;- 1.5\nz_1.5 &lt;- (x - mu)/dp\n\nz_1.5\n\n\n[1] -1.818182\n\n\n\nEncontrar encontrar \\(P(Z \\le z_{1.5}) = P(Z \\le -1.818) = 0.0345182\\).\n\n\n\nCódigo\npnorm(q = z_1.5, mean = 0, sd = 1, lower.tail = TRUE)\n\n\n[1] 0.03451817\n\n\nCompare este resultado com o obtido anteriormente para verificar que é equivalente a \\(P(X \\le 1.5)\\).\n\n\n\n\n\n\nA transformação \\(Z\\)\n\n\n\nSuponha uma variável aleatória \\(X\\) nomalmente distribuída conforme \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^2)\\). Desejamos encontrar \\(m\\) tal que:\n\\(P(X \\le m) = \\alpha\\)\n\n\\(\\alpha\\) aqui representa um valor de probabilidade qualquer determinada pela área na distribuição normal abaixo de \\(m\\).\n\nAo aplicar a transformação \\(Z\\) teremos:\n\\(P(\\frac{X - \\mu}{\\sigma} \\le \\frac{m - \\mu}{\\sigma}) = \\alpha\\)\ncomo \\(\\frac{X - \\mu}{\\sigma} = Z\\) temos que:\n\\(P(Z \\le \\frac{m - \\mu}{\\sigma}) = \\alpha\\)\nPor meio desta expressão, você pode encontar \\(m\\) uma vez fornecido \\(\\alpha\\) ou encontrar \\(\\alpha\\), desde que seja fornecido \\(m\\).\nO mesmo vale se quisermos encontrar a probabilidade determinada por um intervalo definido de \\(m\\) até \\(n\\) (\\(m &lt; n\\)). Para isto fazemos:\n\\(P(m \\le X \\le n) = \\alpha\\)\n\\(P(\\frac{m - \\mu}{\\sigma} \\le \\frac{X - \\mu}{\\sigma} \\le \\frac{n - \\mu}{\\sigma}) = \\alpha\\)\n\\(P(\\frac{m - \\mu}{\\sigma} \\le Z \\le \\frac{n - \\mu}{\\sigma}) = \\alpha\\)\n\n\n\n\n4.2 Tabela \\(Z\\)\nAo utilizarmos um software estatístico não é necessário fazer esta transformação. A transformação \\(Z\\) era necessária na ausência de ferramentas computacionais, ou seja, quando a única opção era utilizarmos a Tabela \\(Z\\) para evitar cálculos tediosos considerando cada combinação de \\(\\mu\\) e \\(\\sigma\\).\nA Tabela Z disponibiliza os valores de probabilidade para um grande número de valores e é apresentada na grande maioria dos livros de estatística.\nVocê pode utilizar a Tabela \\(Z\\) para encontrar \\(P(X \\le 1.5)\\). Note que o valor transformado é \\(z_{1.5} = -1.818\\). Este será o valor que iremos buscar na tabela. Para isto:\n\nEncontre a página que oferece valores negativos, uma vez que \\(z_{1.5} &lt; 0\\);\nNa coluna 1 desta página (coluna z) encontre a linha -1.8 que refere-se à unidade, e à primeira casa decimal de \\(z_{1.5}\\);\nEncontre a coluna 0.02 (quarta coluna da tabela \\(Z\\)) que apresenta a segunda casa decimal de \\(z_{1.5}\\). Isto nos leva ao valor mais próximo do calculado (\\(z_{1.5} = -1.818\\)).\nCruze a linha escolhida no item 3 com a coluna escolhida no item 4. Você irá encontrar o valor \\(0,0344\\). Este valor e a probabilidade de obtermos um valor de \\(z \\le 1.5\\) na distribuição normal padronizada, ou seja, \\(P(Z \\le z_{1.5})\\). A diferença entre este valor e o encontrado com o R se deve unicamente ao limite de precisão na Tabela \\(Z\\)."
  },
  {
    "objectID": "conteudo/distribuicao_normal/distr_norm.html#exercícios-resolvidos",
    "href": "conteudo/distribuicao_normal/distr_norm.html#exercícios-resolvidos",
    "title": "O modelo de distribuição normal",
    "section": "5 Exercícios resolvidos",
    "text": "5 Exercícios resolvidos\n\n5.1 Distribuição de comprimento\nAs comunidades de peixes em riachos de cabeceira são compostas por espécies de pequeno porte. Rhamdioglanis transfasciatus é uma destas espécies, desconhecida do público em geral, porém muito abundante em pequenos riachos bem preservados. Dados de captura sugerem que o tamanho dos indivíduos pode ser razoavelmente bem descrito por um modelo de distribuição normal.\nImporte a base de dados rhamdioglanis.csv\n\nrh &lt;- read_delim('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/rhamdioglanis.csv', delim = ';',\n                 locale = locale(decimal_mark = ','))\n\n\n\nCódigo\nggplot(rh, aes(x = Comprimento)) +\n   geom_histogram(aes(y = after_stat(density)),\n                  fill = 'dodgerblue4', color = 'black', bins = 15) +\n   stat_function(fun = dnorm, args = list(mean = mean(rh$Comprimento),\n                                          sd = sd(rh$Comprimento))) +\n   labs(x = 'Comprimento de Rhamdioglanis transfasciatus (cm)',\n        y = 'Densidade') +\n   theme_classic()\n\n\n\n\n\n\n\n\n\nSuponha o comprimento desta espécie tenha uma distribuição normal com \\(\\mu = 10\\) cm e \\(\\sigma = 3\\) cm. Encontre:\n\nA probabilidade de capturar um indivíduo maior de 14 cm de comprimento, \\(P(X \\ge 14)\\).\nA probabilidade de capturar um indivíduo menor de 5 cm de comprimento, \\(P(X \\le 5)\\).\nA probabilidade de encontrar um indivíduo entre 5 e 14 cm, \\(P(5 \\le X \\le 14)\\).\nSe um trecho de riacho contém 800 indivíduos, quantos são maiores que 14 cm de comprimento.\n\nRESOLUÇÃO\n\n\n\n\n\n\n\\(P(X \\ge 14)\\)\n\n\n\n\n\nVamos encontrar o respectivo valor de \\(Z\\) pela transformação\n\\(z_{14} = \\frac{14 - 10}{3} = 1.33\\)\nNa tabela \\(Z\\) procuramos a linha que mostra a unidade e \\(1^a\\) casa decimal de \\(1.33\\) e em seguida encontramos a coluna que representa a \\(2^a\\) casa decimal de \\(1.33\\). Cruzando linha e coluna encontramos o valor \\(0,9082\\). Note que este valor representa a área abaixo de 1.33, isto é, \\(P(Z \\le z_{14})\\). No entanto, queremos \\(P(Z \\ge z_{14})\\) que representa a área da curva acima de \\(1.33\\). Para isto basta fazermos \\(1 - 0,9082\\).\nDeste modo, \\(P(Z \\ge z_{14}) = 1 - P(Z \\le z_{14}) = 1 - 0,9082 = 0.0918\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nii. \\(P(X \\le 5)\\)\n\n\n\n\n\n\\(z_{5} = \\frac{5 - 10}{3} = -1.67\\)\nNa tabela \\(Z\\) procuramos a linha que mostra a unidade e \\(1^a\\) casa decimal de \\(-1.67\\) e em seguida encontramos a coluna que representa a \\(2^a\\) casa decimal de \\(-1.67\\). Cruzando linha e coluna encontramos o valor \\(0,0475\\) que representa a área desejada.\nDeste modo, \\(P(X \\le 5) = P(Z \\le z_{5}) = 0,0475\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niii. \\(P(5 \\le X \\le 14)\\)\n\n\n\n\n\nVamos subtrair as quantias \\(P(Z \\le 14) - P(Z \\le 5)\\)\nEstes valores já foram encontrados nos itens anteriores, de modo que basta fazermos:\n\\(P(5 \\le X \\le 14) = 0,9082 - 0,0475 = 0.8607\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niv. Indivíduos maiores que 14 cm de comprimento\n\n\n\n\n\nSe a proporção de indivíduos acima de 14 é \\(P(X &gt; 14) = 0.0918\\) e a população tem \\(N = 800\\) indivíduos, teremos:\n\\(0.0918 \\times 800 = 73\\) indivíduos maiores que 14 cm.\n\n\n\n\n\n\n\n\n\nRESOLUÇÃO no R\n\n\n\n\n\nO exercício pode ser resolvido pelo R por meio da função pnorm.\n\nmu &lt;- 10\nsigma &lt;- 3\nN &lt;- 800\nla &lt;- 14\nlb &lt;- 5\n\n\ni. \\(P(Z \\ge 14)\\)\n\npnorm(q = la, mean = mu, sd = sigma, lower.tail = FALSE)\n\n[1] 0.09121122\n\n\n\nii. \\(P(Z \\le 5)\\)\n\npnorm(q = lb, mean = mu, sd = sigma, lower.tail = TRUE)\n\n[1] 0.04779035\n\n\n\niii. \\(P(5 \\le X \\le 14)\\)\n\ndiff(\n   pnorm(q = c(lb, la),\n         mean = mu,\n         sd = sigma,\n         lower.tail = TRUE)\n   )\n\n[1] 0.8609984\n\n\n\niv. Número de indivíduos maiores que \\(14\\) cm de comprimento\n\npg_la &lt;- pnorm(q = la, mean = mu, sd = sigma, lower.tail = FALSE)\n\nN * pg_la\n\n[1] 72.96898\n\n\n\n\n\n\n\n5.2 Intervalos em uma distribuição normal\nSuponha variável aleatória \\(X\\) normalmente distribuída conforme com \\(\\mu = 50\\) e \\(\\sigma = 10\\). Encontre:\n\nO valor de \\(a\\) tal que \\(P(X \\le a) = 0,10\\).\nO valor de \\(b\\) tal que \\(P(X \\ge b) = 0,85\\).\nO intervalo simétrico ao redor da média delimitado por \\(c\\) e \\(d\\) (\\(c &lt; d\\)), que contém \\(95\\%\\) da área sob a curva.\nO valor de \\(e\\) tal que \\(P(50-e \\le X \\le 50+e) = 0.99\\)\n\n\nRESOLUÇÃO\nVeja que neste exercício, foram oferecidos valores de probabilidades e solicitado que você obtivesse os limites em uma distribuição normal específica. Este processo é oposto ao do excercício anterior.\n\n\n\n\n\n\ni. O valor de \\(a\\)\n\n\n\n\n\nSe \\(P(X \\le a) = 0,10\\), a área da curva abaixo de \\(a\\) é \\(0,10\\). Procurando por este valor na tabela \\(Z\\) vemos que o valor mais próximo é \\(0,1003\\) que corresponde a um escore \\(z = -1,28\\). Vamos utilizar este valor para encontrar sua correspondência para a variável aleatória \\(X\\) que tem média \\(\\mu = 50\\) e desvio padrão \\(\\sigma = 10\\).\n\\(z = \\frac{a - \\mu}{\\sigma} :: -1,28 = \\frac{a - 50}{10}\\)\n\\(a = (-1,28 \\times 10) + 50 = 37.2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nii. O valor de \\(b\\)\n\n\n\n\n\nSe \\(P(X \\ge b) = 0,85\\), a área abaixo de \\(b\\) que devemos encontrar na tabela \\(Z\\) é \\(1 - 0,85 = 0.15\\). Vemos que o valor mais próximo é \\(0,1492\\) que corresponde a \\(z = -1,04\\). Ao utilizar este resultado na expressão abaixo temos:\n\\(z = \\frac{b - \\mu}{\\sigma} :: -1,04 = \\frac{b - 50}{10}\\)\n\\(b = (-1,04 \\times 10) + 50 = 39.6\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO intervalo simétrico ao redor da média delimitado por \\(c\\) e \\(d\\) (\\(c &lt; d\\)), que contém \\(95\\%\\) da área sob a curva.\n\n\n\n\n\nSe entre \\(c\\) e \\(d\\) está \\(95\\%\\) da área da curva, temos uma área de \\(1 - 0,95 = 0,05\\) fora da curva. Como o intervalo é simétrico, teremos \\(0,025\\) abaixo de \\(c\\) e \\(0,025\\) acima de \\(d\\).\nAo procurar na tabela \\(Z\\) por \\(0,025\\) encontraremos \\(z = -1,96\\) que equivale ena distribuição de X a:\n\\(z = \\frac{c - \\mu}{\\sigma} :: -1,96 = \\frac{c - 50}{10}\\)\n\\(c = (-1,96 \\times 10) + 50 = 30.4\\)\nNovamente, como o intervalo é simétrico e a dsitribuição de \\(Z\\) é centrada em zero, o ponto \\(d\\) será de +\\(1,96\\) que resulta em:\n\\(z = \\frac{d - \\mu}{\\sigma} :: +1,96 = \\frac{d - 50}{10}\\)\n\\(d = (+1,96 \\times 10) + 50 = 69.6\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niv. O valor de \\(e\\) tal que \\(P(50-e \\le X \\le 50+e) = 0.99\\)\n\n\n\n\n\nPodemos fazer aqui:\n\\(P(50-e \\le X \\le 50+e) = P(\\frac{50-e - \\mu}{\\sigma} \\le \\frac{X-\\mu}{\\sigma} \\le \\frac{50+e-\\mu}{\\sigma}) = 0.99\\)\ncomo \\(\\mu = 50\\) e \\(\\sigma = 10\\) temos:\n\\(P(\\frac{-e}{10} \\le Z \\le \\frac{e}{10}) = 0.90\\)\nComo a área central ocupa \\(0,99\\) da distribuição, restam \\(0,005\\) na cauda superior e \\(0,005\\) na cauda inferior:\n\n\n\n\n\n\n\n\n\nPara encontrar \\(-e\\) buscamos por \\(0,005\\) na tabela \\(Z\\) e encontramos \\(0,0051\\) como valor mais próximo, referente a \\(z_{-e} = -2,57\\). Substituindo na equação temos:\n\\(\\frac{-e}{10} \\le -2,57 :: -e = -2,57 \\times 10 :: e = 25,7\\)\n*Note na figura acima que os limite das áreas em azul são:\n\\(\\mu - e = 50 - 25.7 = 24.3\\) e\n\\(\\mu - e = 50 + 25.7 = 75.7\\)\n\n\n\n\n\n\n\n\n\nRESOLUÇÃO no R\n\n\n\n\n\nO exercício pode ser resolvido pelo R por meio da função qnorm.\n\nEm qnorm, o ‘q’ vem de quantis da distribuição normal.\n\n\nmu = 50\nsigma = 10\n\n(a &lt;- qnorm(p = 0.10, mean = mu, sd = sigma, lower.tail = TRUE))\n\n[1] 37.18448\n\n(b &lt;- qnorm(p = 1-0.85, mean = mu, sd = sigma, lower.tail = TRUE))\n\n[1] 39.63567\n\n(c &lt;- qnorm(p = (1-0.95)/2, mean = mu, sd = sigma, lower.tail = TRUE))\n\n[1] 30.40036\n\n(d &lt;- qnorm(p = (1-0.95)/2, mean = mu, sd = sigma, lower.tail = FALSE))\n\n[1] 69.59964\n\n(e &lt;- -qnorm(p = (1-0.99)/2, mean = mu, sd = sigma, lower.tail = TRUE) + 50)\n\n[1] 25.75829\n\n\n\n\n\n\n\n5.3 Quantos desvios padrões?\nSuponha uma variável aleatória normalmente distribuída representada por \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^2)\\), determine:\n\nO valor de \\(a\\) tal que \\(P(X &lt; a) = 0,20\\).\n\\(P(X \\le \\mu + 2\\sigma)\\).\nO valor de \\(c\\) tal que \\(P(\\mu -c\\sigma \\le X \\le \\mu +c\\sigma) = 0.99\\)\n\nRESOLUÇÃO\n\n\n\n\n\n\ni. O valor de \\(a\\) tal que \\(P(X &lt; a) = 0,20\\).\n\n\n\n\n\n\\(P(X &lt; a) = P(\\frac{X - \\mu}{\\sigma} &lt; \\frac{a - \\mu}{\\sigma}) = P(Z &lt; \\frac{a - \\mu}{\\sigma}) = 0,20\\)\nProcurando pelo valor de \\(z\\) que delimita \\(0,20\\) da área abaixo de \\(a\\) encontramos por \\(z = -0,84\\), de modo que:\n\\(-0,84 = \\frac{a - \\mu}{\\sigma}\\)\n\\(a = \\mu -0,84\\sigma\\)\n\n\n\n\n\n\n\n\n\nii. \\(P(X \\le \\mu + 2\\sigma)\\)\n\n\n\n\n\nA expressão \\(\\mu + 2\\sigma\\) nos diz que o limite de interesse está \\(2\\) desvios padrões acima de \\(\\mu\\). Ao procurar pelo valor de \\(z = 2,0\\) na tabela \\(Z\\), veremos que a probabilidade de interesse é \\(P(X \\le \\mu + 2\\sigma) = 0,9772\\)\n\n\n\n\n\n\n\n\n\niii. O valor de \\(c\\) tal que \\(P(\\mu -c\\sigma \\le X \\le \\mu +c\\sigma) = 0.99\\)\n\n\n\n\n\nDesenvolvendo esta expressão teremos\n\\(P(-c \\le \\frac{X - \\mu}{\\sigma} \\le +c) = P(-c \\le Z \\le +c) = 0.99\\)\nFora deste intervalo simétrico, teremos uma área de \\(0,005\\) na cauda inferior e \\(0,005\\) na cauda superior da distribuição \\(Z\\).\nAo procurar por \\(0,005\\) na tabela \\(Z\\) encontramos \\(z = -2,57\\), de modo que \\(c = 2,57\\).\n\n\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/quartis.html",
    "href": "conteudo/estatistica_descritiva/quartis.html",
    "title": "Medidas de posição: quartis",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nsource('scripts/assimetria_ggplot.r')\nA média, mediana, moda e ponto médio são um tipo de medidas de posição que indicam uma posição particular, isto é, a posição central ao redor da qual os dados estão dispersos. Existem, no entanto, outras medidas de posição como quartis, comumente utilizadas na descrição, análise e interpretação de dados.\nOs quartis de uma distribuição de valores são obtidos após ordenarmos os dados em ordem crescente e, em seguida, agrupá-los em partes iguais, contendo cada uma 25% do número total de observações. Se temos 20 observações, cada parte conterá, portanto, cinco observações, \\(20 \\times 0.25 = 5\\). Os quartis são as posições numéricas que dividem estas partes.\nOs quartis podem ser indicados por \\(Q_1\\), \\(Q_2\\) e \\(Q_3\\), conforme a figura abaixo.\nOs quartis que veremos aqui são medidas empíricas dos limites indicados na figura acima. Calculamos estes limites a partir de uma amostra de tamanho \\(n\\)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/quartis.html#cálculo-dos-quartis-na-posição-j",
    "href": "conteudo/estatistica_descritiva/quartis.html#cálculo-dos-quartis-na-posição-j",
    "title": "Medidas de posição: quartis",
    "section": "1 Cálculo dos quartis na posição \\(j\\)",
    "text": "1 Cálculo dos quartis na posição \\(j\\)\nExistem diferentes algoritmos possíveis para o cálculo dos quartis. Veremos um deles. Para isso, siga os passos abaixo:\n\nReorganize \\(X\\) em ordem crescente de \\(k = 1\\) a \\(k = n\\). Seja \\(n\\) o número de observações em \\(X\\), teremos, portanto, \\(X_k\\) como o valor observado na posição \\(k\\) em ordem crescente. Deste modo, para \\(k = 1\\), teremos \\(X_1\\) como o menor valor, e para \\(k = n\\), teremos \\(X_n\\) como o maior valor.\nCalcule\n\n\\[L = \\frac{j \\times (n+1)}{4};\\]\n\nDefina \\(k\\) como o maior número inteiro abaixo de \\(L\\);\nCalcule\n\n\\[Q_j = X_k + (L - k) \\times (X_{k+1}-X_k);\\]\n\n\\(Q_j\\) será um elemento entre \\(X_k\\) e \\(X_{k+1}\\). Se \\(X_k\\) for um número inteiro, \\(Q_j = X_k\\).\n\nExemplo para o cálculo de \\(Q_1\\)\n\nset.seed(1)\nX &lt;- round(rnorm(20, 10, 2), 1)\nnX &lt;- length(X)\n\nConsidere a variável \\(X\\) com \\(n =\\) 20 observações.\n\\(X\\) = 8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2\n\nsX &lt;- sort(X)\n\nj1 &lt;- 1\nL1 &lt;- j1 * (nX + 1) / 4\nk1 &lt;- floor(L1)\nQ1 &lt;- sX[k1] + (L1 - k1) * (sX[k1+1] - sX[k1])\n\nj2 &lt;- 2\nL2 &lt;- j2 * (nX + 1) / 4\nk2 &lt;- floor(L2)\nQ2 &lt;- sX[k2] + (L2 - k2) * (sX[k2+1] - sX[k2])\n\nj3 &lt;- 3\nL3 &lt;- j3 * (nX + 1) / 4\nk3 &lt;- floor(L3)\nQ3 &lt;- sX[k3] + (L3 - k3) * (sX[k3+1] - sX[k3])\n\n\nOrganize em ordem crescente para determinar os valores de \\(X\\) nas posições \\(k\\).\n\n\nPosicao_k &lt;- paste(1:length(X), \"a Posição\", sep = \"\")\ndf &lt;- tibble(`Posição k` = Posicao_k, `X ordenado` = sX)\n\ndf |&gt; \n  gt()\n\n\n\n\n\n\n\nPosição k\nX ordenado\n\n\n\n\n1a Posição\n5.6\n\n\n2a Posição\n8.3\n\n\n3a Posição\n8.4\n\n\n4a Posição\n8.7\n\n\n5a Posição\n8.8\n\n\n6a Posição\n9.4\n\n\n7a Posição\n9.9\n\n\n8a Posição\n10.0\n\n\n9a Posição\n10.4\n\n\n10a Posição\n10.7\n\n\n11a Posição\n10.8\n\n\n12a Posição\n11.0\n\n\n13a Posição\n11.2\n\n\n14a Posição\n11.2\n\n\n15a Posição\n11.5\n\n\n16a Posição\n11.6\n\n\n17a Posição\n11.9\n\n\n18a Posição\n12.2\n\n\n19a Posição\n13.0\n\n\n20a Posição\n13.2\n\n\n\n\n\n\n\n\nPara \\(j = 1\\) (\\(Q_1\\)), calcule:\n\n\\(L = \\frac{1 \\times (20+1)}{4} = 5.25\\);\n\nDefina \\(k\\) como o maior número inteiro abaixo de \\(L\\). Portanto, se \\(L = 5.25\\), \\(k = 5\\).\nNote que a observação correspondente à \\(k = 5\\) (5\\(^a\\) posição) é 8.8, enquanto a observação correspondente à \\(k = 6\\) (6\\(^a\\) posição) é 9.4. Deste modo, calcule\n\n\\(Q_1 = 8.8 + (5.25 - 5) \\times (9.4 - 8.8) = 8.95\\).\nVemos então que, para a variável \\(X\\) em questão, o primeiro quartil é:\n\\(Q_1 = 8.95\\)\nExercício: Calcule agora os valores correspondentes a \\(Q_2\\) e \\(Q_3\\) e verifique que os resultados são: \\(Q_2 = 10.75\\) e \\(Q_3 = 11.575\\)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/quartis.html#cálculo-dos-quartis-no-r",
    "href": "conteudo/estatistica_descritiva/quartis.html#cálculo-dos-quartis-no-r",
    "title": "Medidas de posição: quartis",
    "section": "2 Cálculo dos quartis no R",
    "text": "2 Cálculo dos quartis no R\nPodemos programar a sequência de funções acima utilizando o R:\n\nX &lt;- c(8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, \n       13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2)\n\n# Ordenando X em ordem crescente\nsX &lt;- sort(X, decreasing = FALSE)\n# Encontrando o número de observações em X\nn &lt;- length(X) \n# Encontrando os quartis (Q1, Q2 e Q3)\nj &lt;- c(1, 2, 3)\nL &lt;- j * (n + 1) / 4\nk &lt;- floor(L)\nQ &lt;- sX[k] + (L - k) * (sX[k+1] - sX[k])\nnames(Q) &lt;- c('Q1', 'Q2', 'Q3')\n\n# Visualizando os quartis\nQ\n\n    Q1     Q2     Q3 \n 8.950 10.750 11.575 \n\n\nEntretanto, existe uma função no R denominada quantile, que pode ser utilizada da seguinte forma:\n\nquantile(X, probs = c(0.25, 0.50, 0.75))\n\n   25%    50%    75% \n 9.250 10.750 11.525 \n\n\n\n\n\n\n\n\nObservações\n\n\n\n\nLembre que o quartil \\(Q_1\\) delimita a posição \\(25\\%\\), \\(Q_2\\) delimita a posição \\(50\\%\\) (\\(=\\) mediana) e \\(Q_3\\) delimita a posição \\(75\\%\\). Por este motivo, utilizamos o argumento probs = c(0.25, 0.50, 0.75). Assim, a função quantile é mais geral que a rotina passada anteriormente, pois permite o cálculo para qualquer posição entre os quantis \\(0\\%\\) e \\(100\\%\\).\nNote também que os resultados foram ligeiramente diferentes, uma vez que existem diferentes algoritmos para o cálculo dos quartis. A função quantile permite a escolha entre \\(9\\) algoritmos diferentes e, por padrão, utiliza o type = 7. O passo-a-passo que mostramos corresponde ao type = 6. Você pode verificar que, ao digitar o comando abaixo, os resultados serão os mesmos que calculamos manualmente.\n\n\nquantile(X, probs = c(0.25, 0.50, 0.75), type = 6)\n\n   25%    50%    75% \n 8.950 10.750 11.575 \n\n\n\nEssas diferenças diminuem à medida que o tamanho amostral aumenta.\nFinalmente, os quartis discutidos aqui são casos particulares de limites mais gerais denominados quantis, que indicam uma posição específica na distribuição. Por exemplo, o limite \\(Q_1\\) poderia ser chamado de Quantil \\(0,25\\). Assim, podemos encontrar qualquer posição, como o quantil \\(0,10\\) que delimita os \\(10\\%\\) menores valores.\nNo cálculo dos quantis para um limite \\(p\\) qualquer (\\(0 \\le p \\le 1\\)), a única mudança no algoritmo apresentado está na obtenção de \\(L\\) (passo 2), que é feita como:\n\n\\[L = p \\times (n+1)\\]"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/quartis.html#obtendo-os-quartis-a-partir-de-uma-tabela-de-dados",
    "href": "conteudo/estatistica_descritiva/quartis.html#obtendo-os-quartis-a-partir-de-uma-tabela-de-dados",
    "title": "Medidas de posição: quartis",
    "section": "3 Obtendo os quartis a partir de uma tabela de dados",
    "text": "3 Obtendo os quartis a partir de uma tabela de dados\nImporte a base de dados Reservatorios_Parana_parcial.csv.\n\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\n\nUsaremos a função summarise para obter os quartis para a variável CPUE.\n\nres |&gt; \n  reframe(Quartis = quantile(CPUE, probs = c(0.25, 0.50, 0.75)),\n          Limites = c('25%', '50%', '75%')) |&gt; \n  gt()\n\n\n\n\n\n\n\nQuartis\nLimites\n\n\n\n\n7.43\n25%\n\n\n11.74\n50%\n\n\n16.30\n75%"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/quartis.html#boxplots-uma-representação-gráfica-dos-quartis",
    "href": "conteudo/estatistica_descritiva/quartis.html#boxplots-uma-representação-gráfica-dos-quartis",
    "title": "Medidas de posição: quartis",
    "section": "4 Boxplots: uma representação gráfica dos quartis",
    "text": "4 Boxplots: uma representação gráfica dos quartis\nOs quartis de uma distribuição nos ajudam a entender o formato de uma distribuição. Uma das formas amplamente estabelecidas de representá-los graficamente é por meio de um boxplot. Para a variável acima, o boxplot será:\n\n\nCódigo\ndf &lt;- data.frame(X)\nPX &lt;- quantile(X, probs = c(0, 0.25, 0.50, 0.75, 1))\nLegX &lt;- c(\"Mínimo\", \"Q1\", \"Q2 = Mediana\", \"Q3\", \"Máximo\")\nggplot(df, aes(y = X)) +\n  geom_boxplot(fill = 'lightblue', coef = 10) +\n  annotate(geom = 'text', x = .8, y = PX - 0.3,\n           label = LegX) +\n  annotate(geom = 'segment', \n           x = 0.5, xend = 1.23,\n           y = PX, yend = PX,\n           arrow = arrow(ends = 'first')) +\n  scale_y_continuous(breaks = 5:14) +\n  theme_classic() +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nFigura 2: Divisão em quartis de um boxplot\n\n\n\n\n\nEm um boxplot, a linha central representa a Mediana ou \\(2^o\\) quartil (\\(Q_2\\)), e os limites da caixa são o \\(1^o\\) e \\(3^o\\) quartis, respectivamente \\(Q_1\\) e \\(Q_3\\). As extremidades geralmente são os pontos máximo e mínimo da distribuição.\nExiste uma relação entre os histogramas e os boxplots. Ambos podem ser utilizados para avaliar o grau de assimetria de uma distribuição, como apresentado abaixo. Em uma distribuição simétrica, a caixa do boxplot tende a se concentrar no meio da distribuição. Já em distribuições assimétricas, a caixa tende a ficar deslocada à esquerda ou à direita (Figura 3).\n\n\nCódigo\n# Ver função completa no arquivo 'scripts/assimetria_ggplot.r'\nassimetria_ggplot(fig = 'bh')\n\n\n\n\n\n\n\n\nFigura 3: Padrão de assimetria representado por meio de histogramas e boxplots."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html",
    "href": "conteudo/estatistica_descritiva/tendcentral.html",
    "title": "Medidas de tendência central",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nsource('scripts/getmode.r')\nsource('scripts/assimetria_ggplot.r')\nTabelas de frequência e histogramas permitem a visualização dos padrões de distribuição de uma variável quantitativa, evidenciando limites inferiores e superiores, faixas de valores mais ou menos frequentes etc. Neste capítulo, veremos medidas-resumo que possibilitam descrever a tendência central de um conjunto de dados. Algumas dessas medidas são a média aritmética, a mediana, a moda e o ponto médio."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#média-aritmética",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#média-aritmética",
    "title": "Medidas de tendência central",
    "section": "1 Média aritmética",
    "text": "1 Média aritmética\nConsidere a variável \\(X\\) com \\(n\\) elementos \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(\\cdots, X_n\\). A média aritmética (\\(\\overline{X}\\)) é dada por:\n\\[\\overline{X}=\\frac{X_1+X_2+X_3+\\cdots+X_n}{n}=\\frac{\\sum_{i=1}^n{X_i}}{n}\\]\nExemplo\nSeja a variável \\(X\\) abaixo:\n\n\nCódigo\nn &lt;- 5\nset.seed(1)\nX &lt;- sample(x = 1:10, size = n, rep = TRUE)\n\n\n\\(X =\\) {9, 4, 7, 1, 2}\n\\(X\\) possui 5 observações e tem média:\n\\(\\overline{X}=\\frac{9 + 4 + 7 + 1 + 2}{5}\\)\n\\(\\overline{X}=\\frac{23}{5} = 4.6\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#mediana",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#mediana",
    "title": "Medidas de tendência central",
    "section": "2 Mediana",
    "text": "2 Mediana\nÉ definida como o valor do meio de uma distribuição, de modo que metade dos valores está abaixo e metade acima da mediana. Se organizarmos a variável \\(X\\) em ordem crescente teremos:\n\n\nCódigo\nX &lt;- sort(X)\nXmed &lt;- median(X)\n\n\n\\(X =\\) {1,2 , 4 , 7,9}\nsendo a mediana igual a \\(4\\).\nNeste exemplo, temos \\(n = 5\\) observações. Se tivermos um número par de observações, teremos duas na posição central. Por exemplo, se:\n\n\nCódigo\nset.seed(1)\nX &lt;- sample(x = 1:10, size = 6, rep = TRUE)\n\n\n\\(X =\\) {9, 4, 7, 1, 2, 7}\nvemos que após ordenarmos \\(X\\):\n\\(X =\\) {1, 2, 4, 7, 7, 9}\nteremos o \\(4\\) e o \\(7\\) como valores do meio.\nNeste caso, a mediana fica como sendo:\n\\(\\frac{4 + 7}{2} = 5.5\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#moda",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#moda",
    "title": "Medidas de tendência central",
    "section": "3 Moda",
    "text": "3 Moda\nÉ definida como o valor mais frequente de uma distribuição.\n\n\nCódigo\nset.seed(1)\nX &lt;- sample(x = 1:10, size = 6, rep = TRUE)\n\n\nPara \\(X =\\) {9, 4, 7, 1, 2, 7}, a moda é 7, o valor que mais se repete na distribuição.\n\n\n\n\n\n\nNota\n\n\n\nA moda nem sempre é única. Se vários valores repetem-se igualmente, teremos mais de uma moda na distribuição."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#ponto-médio",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#ponto-médio",
    "title": "Medidas de tendência central",
    "section": "4 Ponto médio",
    "text": "4 Ponto médio\nÉ calculado com base nos dois valores extremos da distribuição (mínimo e máximo), sendo obtido por:\n\\[P_{medio}=\\frac{X_{minimo} + X_{maximo}}{2}\\]\nPara \\(X =\\) {9, 4, 7, 1, 2, 7}, o ponto médio é:\n\\(PM = \\frac{1 + 9}{2} = \\frac{10}{2} = 5\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#efeito-da-assimetria-sobre-os-descritores-de-tendência-central",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#efeito-da-assimetria-sobre-os-descritores-de-tendência-central",
    "title": "Medidas de tendência central",
    "section": "5 Efeito da assimetria sobre os descritores de tendência central",
    "text": "5 Efeito da assimetria sobre os descritores de tendência central\nCada um dos descritores de tendência central descritos acima é mais ou menos sensível ao grau de assimetria de uma distribuição. Em uma distribuição perfeitamente simétrica, onde as observações estão igualmente dispersas acima e abaixo do ponto central, os valores da média, mediana, moda e ponto médio coincidem. Por outro lado, pode ocorrer da distribuição ser assimétrica. Neste caso, a posição relativa dos descritores irá depender se a assimetria é à direita ou à esquerda. Esta discrepância ocorre devido à sensibilidade destes descritores a valores extremos na distribuição. O ponto médio é o mais sensível à presença de pontos extremos, seguido da média, mediana e moda.\n\n\nCódigo\n# Ver função completa no arquivo 'scripts/assimetria_ggplot.r'\nassimetria_ggplot()\n\n\n\n\n\n\n\n\nFigura 1: Efeito da assimetria de uma distribuição sobre o ponto médio, a média aritmética, a mediana e a moda.\n\n\n\n\n\n\n\n\n\n\n\nMedidas de tendência central\n\n\n\nMédia aritmética: utiliza todo o conjunto de dados. Relativamente sensível a valores extremos;\nMediana: o valor do meio. Metade dos pontos está acima e metade abaixo da mediana. A mediana é uma medida resistente a valores extremos;\nModa: valor mais frequente. Se mais de um valor aparece com a mesma frequência, os dados têm uma distribuição multimodal;\nPonto médio: considera somente o valor mínimo e máximo. O ponto médio é fácil de calcular, porém não utiliza a maioria do conjunto de dados e é muito sensível a valores extremos."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/tendcentral.html#obtendo-medidas-de-uma-tabela-de-dados",
    "href": "conteudo/estatistica_descritiva/tendcentral.html#obtendo-medidas-de-uma-tabela-de-dados",
    "title": "Medidas de tendência central",
    "section": "6 Obtendo medidas de uma tabela de dados",
    "text": "6 Obtendo medidas de uma tabela de dados\nImporte a base de dados Reservatorios_Parana_parcial.csv.\n\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\n\nMedidas-resumo podem ser obtidas com a função summarise.\nVamos encontrar a média aritmética e a mediana da variável CPUE.\n\nres |&gt; \n  summarise(CPUE_medio = mean(CPUE),\n            CPUE_mediana = median(CPUE)) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_medio\nCPUE_mediana\n\n\n\n\n12.70097\n11.74\n\n\n\n\n\n\n\nOs valores são parecidos, porém a média é um pouco superior. Provavelmente a distribuição deve ser ligeiramente assimétrica à direita. Podemos verificar isto por meio de um histograma.\n\ncl_cpue &lt;- seq(0, 35, by = 5)\n\nggplot(res, aes(x = CPUE)) +\n  geom_histogram(breaks = cl_cpue, \n                 fill = 'darkblue',\n                 color = 'white') +\n  theme_classic()\n\n\n\n\n\n\n\n\nAlguns valores de captura próximos a \\(30\\) kg estão fazendo com que a média esteja um pouco acima da mediana.\nVamos verificar agora a média da variável Area dos reservatórios:\n\nres |&gt; \n  summarise(CPUE_medio = mean(Area, na.rm = TRUE),\n            CPUE_mediana = median(Area, na.rm = TRUE)) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_medio\nCPUE_mediana\n\n\n\n\n64.7369\n12\n\n\n\n\n\n\n\nPara esta variável, a discrepância é muito maior.\n\n\nobs: tivemos que utilizar o argumento na.rm = TRUE para excluir do cálculo reservatórios com dados faltantes para Area.\n\n\n\ncl_area &lt;- seq(0, 500, by = 50)\n\nggplot(res, aes(x = Area)) +\n  geom_histogram(breaks = cl_area, \n                 fill = 'darkblue',\n                 color = 'white') +\n  theme_classic()\n\n\n\n\n\n\n\n\nAo verificar o histograma, vemos que existe uma grande concentração de reservatórios com áreas até \\(50\\) \\(km^2\\), porém poucos reservatórios muito grandes com mais de \\(200\\) \\(km^2\\). Estes grandes reservatórios deslocam a média aritmética à direita.\nPodemos ver quais são estes reservatórios utilizando a função filter.\n\nr_grandes &lt;- res |&gt; \n  filter(Area &gt;= 200) |&gt; \n  select(Reservatorio, Area)\n\nr_grandes |&gt; \n  gt()\n\n\n\n\n\n\n\nReservatorio\nArea\n\n\n\n\nSalto Santiago\n208.0\n\n\nCapivara\n419.3\n\n\nChavantes\n400.0\n\n\nRosana\n220.0\n\n\n\n\n\n\n\nEntre os 31 reservatórios, temos 4 com áreas acima de \\(200\\) \\(km^2\\) (Salto Santiago, Capivara, Chavantes, Rosana). A influência destes reservatórios é maior para a média aritmética, pois esta é mais sensível a valores extremos do que a mediana. Se calcularmos o ponto médio, veremos que esta influência é ainda maior.\n\nres |&gt; \n  summarise(CPUE_medio = mean(Area, na.rm = TRUE),\n            CPUE_mediana = median(Area, na.rm = TRUE),\n            P_medio = sum(range(Area, na.rm = TRUE)) / 2) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_medio\nCPUE_mediana\nP_medio\n\n\n\n\n64.7369\n12\n209.685\n\n\n\n\n\n\n\nSe calcularmos os descritores de tendência central sem estes reservatórios, vemos que a diferença entre os descritores diminui, mas não desaparece, o que ocorre devido à elevada assimetria nesta variável.\n\nres |&gt; \n  filter(Area &lt; 200) |&gt; \n  summarise(CPUE_medio = mean(Area, na.rm = TRUE),\n            CPUE_mediana = median(Area, na.rm = TRUE),\n            P_medio = sum(range(Area, na.rm = TRUE)) / 2) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_medio\nCPUE_mediana\nP_medio\n\n\n\n\n25.2028\n7.2\n69.535"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/escorez.html",
    "href": "conteudo/estatistica_descritiva/escorez.html",
    "title": "Medidas de posição: transformação Z",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nsource('scripts/normal_empirica_gg.r')\nO índice (ou escore) \\(Z\\) indica a posição de uma observação particular (\\(X_i\\)) dentro de uma distribuição, relacionando a posição de \\(X_i\\) com a média e o desvio padrão da distribuição de \\(X\\). Suponha uma variável com média \\(\\overline{X}\\) e desvio padrão \\(s\\). O índice de \\(Z_i\\) para uma observação \\(i\\) particular é calculado por:\n\\[Z_i = \\frac{X_i - \\overline{X}}{s}\\]\nSeja, por exemplo, a variável \\(X\\):\nCódigo\nset.seed(1)\nX &lt;- round(rnorm(20, 10, 2), 1)\nnX &lt;- length(X)\nsX &lt;- sort(X)\n\\(X\\) = 8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2\nCom média e desvio padrão \\(\\overline{X} = 10.39\\) e \\(s = 1.82\\), respectivamente.\nO índice \\(Z_i\\) para a \\(3a\\) observação \\(X_{3} = 8.3\\) pode ser obtido por:\nCódigo\ni &lt;- 3\nXm &lt;- mean(X)\nXsd &lt;- sd(X)\nZi &lt;- (X[i] - Xm) / Xsd\n\\(Z_8.3 = \\frac{8.3 - 10.39}{1.82} = -1.15\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/escorez.html#interpretando-o-valor-de-z",
    "href": "conteudo/estatistica_descritiva/escorez.html#interpretando-o-valor-de-z",
    "title": "Medidas de posição: transformação Z",
    "section": "1 Interpretando o valor de \\(Z\\)",
    "text": "1 Interpretando o valor de \\(Z\\)\nO cálculo do índice \\(Z\\) passa pela centralização e padronização da variável \\(X\\):\n\nCentralização: a porção \\(X_i - \\overline{X}\\) mede o desvio de cada observação, isto é, a distância (positiva ou negativa) entre \\(X_i\\) e \\(\\overline{X}\\). O termo centralização refere-se ao comportamento dos desvios estarem distribuídos ao redor de zero, isto é, a média dos desvios é zero.\n\n\\[\\sum_{i=1}^{n}\\frac{(X_i - \\overline{X})}{n} = 0\\]\n\nPadronização: ao dividirmos a quantia \\(X_i - \\overline{X}\\) pelo desvio padrão de \\(X\\), obtemos a nova variável denominada \\(Z\\). O termo padronização refere-se ao fato de o desvio padrão de \\(Z\\) ser igual a \\(1\\).\n\nA transformação \\(Z\\) consiste, portanto, em gerar uma nova variável com média \\(\\overline{Z} = 0\\) e desvio padrão \\(s_{Z} = 1\\).\nDeste modo, o valor de \\(Z_i\\) associado a uma observação \\(X_i\\) particular nos indica quantos desvios padrões \\(X_i\\) está acima ou abaixo da média de seu grupo.\n\n\n\n\n\n\nRelação \\(Z\\) e \\(X\\)\n\n\n\n\nSe \\(Z_i = 0\\), então \\(X_i = \\overline{X}\\);\nSe \\(Z_i &gt; 0\\), então \\(X_i &gt; \\overline{X}\\);\nSe \\(Z_i &lt; 0\\), então \\(X_i &lt; \\overline{X}\\);\n\n\n\nPara uma distribuição com média igual a \\(10\\) e desvio padrão igual a \\(3\\), por exemplo, uma observação \\(X_i = 16\\) terá um valor de \\(Z = \\frac{16-10}{3} = 2\\), indicando que está dois desvios padrões acima da média de \\(X\\)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/escorez.html#cálculo-de-z-no-ambiente-r",
    "href": "conteudo/estatistica_descritiva/escorez.html#cálculo-de-z-no-ambiente-r",
    "title": "Medidas de posição: transformação Z",
    "section": "2 Cálculo de \\(Z\\) no ambiente R",
    "text": "2 Cálculo de \\(Z\\) no ambiente R\nSeja:\n\n\nCódigo\nset.seed(1)\nX &lt;- round(rnorm(20, 10, 2), 1)\n\n\n\\(X\\) = 8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2\n\\(Z\\) pode ser obtido pelos comandos:\n\nXm &lt;- mean(X)\nXsd &lt;- sd(X)\nZ &lt;- (sort(X) - Xm) / Xsd\n\nPodemos ver na Tabela 1 os valores de cada observação \\(X_i\\) e dos respectivos valores de \\(Z_i\\) em ordem crescente.\n\n\nCódigo\nPosicao_k &lt;- paste(1:length(X), \"a Posição\", sep = \"\")\ndf &lt;- tibble(`Posicao k` = Posicao_k, `X ordenado` = sX, Z = round(Z, 2)) |&gt;\n  add_row(\n    `Posicao k` = c(\"Média\", \"Desvio padrão\"),\n    `X ordenado` = c(round(mean(sX), 2), round(sd(sX), 2)),\n    Z = c(round(mean(Z), 2), round(sd(Z), 2))\n  )\n\ndf |&gt;\n  gt()\n\n\n\n\nTabela 1: Associação entre uma distribuição X e a transformação Z.\n\n\n\n\n\n\n\n\n\nPosicao k\nX ordenado\nZ\n\n\n\n\n1a Posição\n5.60\n-2.63\n\n\n2a Posição\n8.30\n-1.15\n\n\n3a Posição\n8.40\n-1.09\n\n\n4a Posição\n8.70\n-0.93\n\n\n5a Posição\n8.80\n-0.87\n\n\n6a Posição\n9.40\n-0.54\n\n\n7a Posição\n9.90\n-0.27\n\n\n8a Posição\n10.00\n-0.21\n\n\n9a Posição\n10.40\n0.01\n\n\n10a Posição\n10.70\n0.17\n\n\n11a Posição\n10.80\n0.23\n\n\n12a Posição\n11.00\n0.34\n\n\n13a Posição\n11.20\n0.45\n\n\n14a Posição\n11.20\n0.45\n\n\n15a Posição\n11.50\n0.61\n\n\n16a Posição\n11.60\n0.66\n\n\n17a Posição\n11.90\n0.83\n\n\n18a Posição\n12.20\n0.99\n\n\n19a Posição\n13.00\n1.43\n\n\n20a Posição\n13.20\n1.54\n\n\nMédia\n10.39\n0.00\n\n\nDesvio padrão\n1.82\n1.00\n\n\n\n\n\n\n\n\n\n\nPodemos comparar graficamente as distribuições das variáveis \\(X\\) e \\(Z\\).\n\n\nCódigo\nhX &lt;- ggplot(df, aes(x = `X ordenado`)) +\n  geom_histogram(fill = 'darkblue', color = 'white', bins = 9) +\n  ylab('Frequência') +\n  scale_x_continuous(n.breaks = 7) +\n  theme_classic(base_size = 20)\n\nhZ &lt;- ggplot(df, aes(x = Z)) +\n  geom_histogram(fill = 'darkblue', color = 'white', bins = 9) +\n  ylab('Frequência') +\n  scale_x_continuous(n.breaks = 7) +\n  theme_classic(base_size = 20)\n\nhX / hZ\n\n\n\n\n\n\n\n\nFigura 1: Histogramas representando a distribuição de X e Z.\n\n\n\n\n\nVeja na Tabela 1 que conforme o valor de \\(X_i\\) se distancia da média de \\(X = 10.39\\), mais distante de zero será o valor de \\(Z_i\\). Neste exemplo, as observações mais extremas de \\(X\\) estão, respectivamente, a -2.63 desvios padrões abaixo e 1.54 desvios padrões acima da média. Como discutido acima, a nova variável \\(Z\\) tem média \\(\\overline{Z} = 0\\) (está centralizada) e desvio padrão \\(s_Z = 1\\) (está padronizada)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/escorez.html#obtendo-a-transformação-z-a-partir-de-uma-tabela-de-dados",
    "href": "conteudo/estatistica_descritiva/escorez.html#obtendo-a-transformação-z-a-partir-de-uma-tabela-de-dados",
    "title": "Medidas de posição: transformação Z",
    "section": "3 Obtendo a transformação \\(Z\\) a partir de uma tabela de dados",
    "text": "3 Obtendo a transformação \\(Z\\) a partir de uma tabela de dados\nImporte a base de dados Reservatorios_Parana_parcial.csv.\n\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\n\nUtilizando a função mutate, vamos manter somente a variável CPUE e criar outra coluna denominada CPUE_z.\n\ndf_z &lt;- res |&gt; \n  select(CPUE) |&gt; \n  mutate(CPUE_z = (CPUE - mean(CPUE)) / sd(CPUE)) |&gt; \n  round(2)\n\ndf_z |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE\nCPUE_z\n\n\n\n\n9.22\n-0.47\n\n\n28.73\n2.17\n\n\n11.59\n-0.15\n\n\n30.76\n2.45\n\n\n5.95\n-0.92\n\n\n7.75\n-0.67\n\n\n7.51\n-0.70\n\n\n4.01\n-1.18\n\n\n20.83\n1.10\n\n\n2.43\n-1.39\n\n\n12.55\n-0.02\n\n\n11.73\n-0.13\n\n\n13.72\n0.14\n\n\n16.50\n0.52\n\n\n4.71\n-1.08\n\n\n7.95\n-0.64\n\n\n13.12\n0.06\n\n\n16.10\n0.46\n\n\n11.74\n-0.13\n\n\n17.95\n0.71\n\n\n13.86\n0.16\n\n\n13.04\n0.05\n\n\n7.35\n-0.73\n\n\n20.92\n1.12\n\n\n13.67\n0.13\n\n\n21.82\n1.24\n\n\n6.29\n-0.87\n\n\n9.40\n-0.45\n\n\n5.60\n-0.96\n\n\n2.05\n-1.45\n\n\n24.88\n1.65\n\n\n\n\n\n\n\nSe calcularmos a média e o desvio padrão das variáveis, veremos que CPUE mantém os valores originais, enquanto CPUE_z terá média igual a \\(0\\) e desvio padrão igual a \\(1\\).\n\ndf_z |&gt; \n  summarize(CPUE_media = mean(CPUE),\n            CPUE_dp = sd(CPUE),\n            CPUE_z_media = round(mean(CPUE_z), 2),\n            CPUE_z_dp = round(sd(CPUE_z), 2)) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_media\nCPUE_dp\nCPUE_z_media\nCPUE_z_dp\n\n\n\n\n12.70097\n7.3701\n0\n1"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/escorez.html#valores-esperados-de-z-em-uma-distribuição-normal-padronizada",
    "href": "conteudo/estatistica_descritiva/escorez.html#valores-esperados-de-z-em-uma-distribuição-normal-padronizada",
    "title": "Medidas de posição: transformação Z",
    "section": "4 Valores esperados de \\(Z\\) em uma distribuição normal padronizada",
    "text": "4 Valores esperados de \\(Z\\) em uma distribuição normal padronizada\nA interpretação de \\(Z\\) faz sentido quando desejamos posicionar uma determinada observação \\(X_i\\) como função da média e desvio padrão de seu grupo. Adicionalmente, se uma variável \\(X\\) puder ser descrita adequadamente por uma distribuição normal de probabilidades, existe uma regra empírica que permite determinar os percentuais das observações acima e abaixo de limites conhecidos.\n\n\nCódigo\n# Ver função completa no arquivo 'scripts/normal_empirica_gg.r'\nnormal_empirica_gg()\n\n\n\n\n\n\n\n\nFigura 2: Áreas de probabilidade em uma distribuição normal.\n\n\n\n\n\nNa Figura 2, vemos que existe uma probabilidade de aproximadamente \\(68\\%\\) de que uma observação tomada ao acaso esteja entre os limites de \\(-1\\) e \\(1\\) desvios padrões da média. Existe ainda uma probabilidade de aproximadamente \\(95\\%\\) de que uma observação esteja entre \\(-2\\) e \\(2\\) desvios padrões da média. Por outro lado, é muito improvável encontrarmos ao acaso uma observação a mais de \\(3\\) desvios padrões distantes da média. Isto deverá ocorrer em somente cerca de \\(0,26\\%\\) dos casos em que sortearmos uma amostra aleatoriamente.\n\n\n\n\n\n\nUso da distribuição normal empírica\n\n\n\nSuponha que a distribuição de altura de homens adultos siga uma distribuição normal com média \\(\\mu = 175\\) cm e desvio padrão \\(\\sigma = 10\\) cm.\n\n\nCódigo\nmH &lt;- 175\nsdH &lt;- 10\nlim &lt;- 2\nlinf &lt;- round(mH - lim * sdH, 2)\nlsup &lt;- round(mH + lim * sdH, 2)\n\n\nNeste caso, se tomarmos os limites entre \\(-2\\) e \\(+2\\) desvios padrões teremos:\n\\(\\mu - 2 \\times \\sigma = 175 - 2 \\times 10 = 155\\) cm\ne\n\\(\\mu + 2 \\times \\sigma = 175 + 2 \\times 10 = 195\\) cm\nEstes resultados sugerem que nesta população temos somente cerca de \\(5\\%\\) dos homens adultos com mais de \\(195\\) cm ou menos de \\(155\\) cm de altura."
  },
  {
    "objectID": "conteudo/inferencia_estatistica/teorema_central_limite.html",
    "href": "conteudo/inferencia_estatistica/teorema_central_limite.html",
    "title": "Distribuição das médias amostrais",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nsource(\"scripts/tcl_simetry.r\")"
  },
  {
    "objectID": "conteudo/inferencia_estatistica/teorema_central_limite.html#teorema-central-do-limite",
    "href": "conteudo/inferencia_estatistica/teorema_central_limite.html#teorema-central-do-limite",
    "title": "Distribuição das médias amostrais",
    "section": "1.1 Teorema Central do Limite",
    "text": "1.1 Teorema Central do Limite\nSeja uma população estatística com média \\(\\mu\\) e desvio padrão \\(\\sigma\\). A distribuição das médias amostrais desta população tenderá a apresentar uma distribuição normal de probabilidades com média \\(\\mu\\) e desvio padrão \\(\\frac{\\sigma}{\\sqrt(n)}\\) à medida que o tamanho amostral \\(n\\) aumenta, ainda que a distribuição das observações originais não possua uma distribuição normal.\nSegundo o TCL, as médias amostrais \\(\\overline{X}\\) de um experimento distribuem-se como:\n\\[\\overline{X} \\sim \\mathcal{N}(\\mu_{\\overline{X}},\\,\\sigma^{2}_{\\overline{X}})\\]\nem que \\(\\mu_{\\overline{X}} = \\mu\\) \\(\\sigma^{2}_{\\overline{X}} = \\frac{\\sigma^2}{n}\\)\nNote que a variância de \\(\\overline{X}\\) depende do tamanho amostral \\(n\\).\n\n1.1.1 Probabilidades na amostra original e na distribuição de médias\nSeja uma variável \\(X\\) qualquer com \\(\\mu = 50\\) e \\(\\sigma = 10\\). As figuras abaixo comparam as probabilidades acima de \\(x_1 = 55\\) para as observações originais e para as distribuições de médias amostrais de tamanho \\(n_1 = 2\\) e \\(n_2 = 10\\).\n\n\n\n\n\n\n\n\nFigura 3: Probabilidades na amostra original e na distribuição de médias.\n\n\n\n\n\nNote que existe uma probabilidade razoável de que uma determinada observação em \\(X\\) esteja acima de \\(55\\), \\(P(X \\leq 55) = 0.309\\). No entando se tomarmos ao acaso uma amostra de tamanho \\(n_1 = 2\\), a probabilidade de que a média destas duas amostras esteja acima de \\(55\\) diminui para \\(P(\\overline{X} \\leq 55) = 0.24\\). Se tormarmos uma amostra ainda maior (\\(n_2 = 10\\)), a probabilidade se reduz ainda mais para \\(P(\\overline{X} \\leq 55) = 0.057\\).\nVemos portanto que a precisão de um experimento aumenta à medida que aumentamos o tamanho amostral, pois para amostras grandes, a probabilidade de obtermos um \\(\\overline{X}\\) distante de \\(\\mu\\) torna-se cada vez menor.\n\n\n1.1.2 Distribuições não-normais\nO TCL é válido inclusive para distribuições não-normais. Isto torna a distribuição normal uma das mais importantes em inferência estatística, pois ainda que o resultado de um experimento particular seja descrito por qualquer outro modelo de probabilidades, as médias das amostras deste experimento seguirão uma distribuição normal, à medida que \\(n\\) aumenta. Isto justifica muitos dos processos de análise e inferência estatística que serão descritos nos capítulos posteriores.\nA Figura 4, simula a distribuição de médias amostrais para variáveis com diferentes distribuições de probabiidades e tamanhos crescentes de \\(n\\). Podemos observar que independente do formato da distribuição original, a distribuição das médias amostrais tende à normalidade. O padrão normal aparece mais rápido se a distribuição original é simétrica. Ainda que para populações estatísticas com distribuições assimétricas, seja necessário um tamanho amostral maior para que se alcance a normalidade, a figura mostra que a partir de \\(n = 30\\) todas as distribuições parecem se aproximar do que seria esperado um modelo normal.\n\n\n\n\n\n\n\n\nFigura 4: Demostração empírica do Teorema Central do Limite, mostrando a tendência à normalidade de \\(\\bar{X}\\) à medina que \\(n\\) aumenta."
  },
  {
    "objectID": "conteudo/inferencia_estatistica/teorema_central_limite.html#exercícios-resolvidos",
    "href": "conteudo/inferencia_estatistica/teorema_central_limite.html#exercícios-resolvidos",
    "title": "Distribuição das médias amostrais",
    "section": "1.2 Exercícios resolvidos:",
    "text": "1.2 Exercícios resolvidos:\n\n1.2.1 Tamanho médio de robalos no mercado de peixes\nEm 2014 no estuário do rio Itanhaém - SP foi pescado o “maior robalo já encontrado” (G1 Santos). O peixe tinha \\(133\\) cm e \\(27,8\\) kg . Em 2018 em Bertioga, também no litoral de SP: “Robalo ‘gigante’ quebra recordes e vira atração” (G1 Santos) pesando \\(33\\) kg. Finalmente, em “uma das salas da Colônia de Pesca Z2 de Atafona” RJ está uma imagem de um robalo de \\(28\\) kg capturado muitas décadas atrás (Ambiente Cult)\n\n\n\n\n\n\n\n\n\n\n\n(a) Itanhaém 2014\n\n\n\n\n\n\n\n\n\n\n\n(b) Bertioga 2018\n\n\n\n\n\n\n\n\n\n\n\n(c) Atafona, sem data\n\n\n\n\n\n\n\nFigura 5: Grandes robalos\n\n\n\nEstas capturas viram notícias pois são inusitadas. Dados de desembarque sugerem que a distribuição de tamanho de robalos comumente capturados está muito abaixo destes limites (Figura 6) (Ximenes-Carvalho 2006) ( Acesse aqui o trabalho completo).\n\n\nCódigo\nTabelaI &lt;- data.frame(\n  compmedio = c(25.2, 34.4, 40.7, 46.3, 51.7, 56.5, 61.2, 65.8, 70.3, 73.4, 76.2),\n  N = c(130,130,112,100,82,64,47,30,18,12,6)\n)\n\ntabelaI_plt &lt;- ggplot(data = TabelaI, mapping = aes(x = compmedio, y = N)) +\n   geom_col(fill = 'dodgerblue4', color = 'black') +\n   labs(y = 'Número de indivíduos analisados',\n        x = 'Comprimento médio (cm)') +\n   theme_classic()\n\n\n\n\nCódigo\ntabelaI_plt\n\n\n\n\n\n\n\n\nFigura 6: Dados de desembarque no Mercado de São Pedro (Niterói, RJ). Extraídos de XIMENES-CARVALHO, 2006.\n\n\n\n\n\nA distribuição de tamanhos na Figura 6 é altamente assimétrica e claramente não-normal. Um dos motivos para este forte grau de assimetria deve-se ao limite inferior de captura, pois a captura e comercialização de animais muito pequenos é proibida.\nSuponha que o comprimento de robalos (\\(L\\)) disponíveis para compra tenha média \\(\\mu = 44.1\\) e desvio padrão \\(\\sigma = 13.4\\). Você compra 10 robalos escolhidos ao acaso dos que estão disponíveis. Qual a probabilidade de que:\n\nO tamanho médio de uma compra esteja acima de \\(52,4\\) cm, isto é \\(P(\\overline{L} &gt; 52,4)\\)?\nEm \\(95\\%\\) das vezes que fizer a compra, determine o intervalo simétrico que conterá o tamanho médio dos robalos selecionados, isto é \\(P(a \\le \\overline{L} \\le b) = 0,95\\)\nResponda novamente aos itens i. e ii. no caso de sua compra constar de \\(4\\) robalos.\n\nRESOLUÇÃO\nAinda que a distribuição original claramente não siga uma distribuição normal, podemos utilizar o TCL para estimarmos as probabilidades de obter uma média amostral \\(\\overline{X}\\) a determinada distância de \\(\\mu\\). Para isto, no entanto devemos recordar que o desvio padrão das médias amostrais será dado por: \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\).\n\n\n\n\n\n\ni. \\(P(\\overline{L} &gt; 52,4)\\)\n\n\n\n\n\nCom base no TCL, uma amostra de \\(n = 10\\) terá média normalmente distribuída com parâmetros \\(\\mu\\) e \\(\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{10}}\\). Podemos assim, realizar a transformação \\(Z\\) como segue:\n\\(Z_{\\overline{L}} = \\frac{\\overline{L} - \\mu}{\\sigma_{\\mu}} = \\frac{\\overline{L} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(Z_{\\overline{L}} = \\frac{52,4 - 44.1}{\\frac{13.4}{\\sqrt{10}}} = 1.96\\)\nSe buscarmos na Tabela Z, veremos que a área da distribuição normal padronizada abaixo de \\(1.96\\) é de \\(0,975\\). Consequentemente \\(P(\\overline{L} &gt; 52,4) = 1 - 0,975 = 0,025\\)\n\n\n\n\n\n\n\n\n\nii. \\(P(a \\le \\overline{L} \\le b) = 0,95\\)\n\n\n\n\n\nSe o intervalo é simétrico e contém \\(0,95\\) das observações, restam \\(0,025\\) em cada uma das caudas. Vimos no item anterior que \\(z = 1,96\\) que delimita \\(0,025\\) da cauda superior. Portanto os limites aqui serão dados por: \\(a = -1.96\\) e \\(b = 1.96\\). Na distribuição original estes limites resultarão em:\n\\(-1,96 = \\frac{a - 44.1}{\\frac{13.4}{\\sqrt{10}}}:: a = 44.1 -1,96 \\frac{13.4}{\\sqrt{10}} = 35.8\\) cm\ne\n\\(+1,96 = \\frac{b - 44.1}{\\frac{13.4}{\\sqrt{10}}}:: b = 44.1 +1,96 \\frac{13.4}{\\sqrt{10}} = 52.4\\) cm\n\n\n\n\n\n\n\n\n\nReduzindo para \\(n = 4\\) robalos\n\n\n\n\n\nAqui você deve repetir exatamente os passos de i. e ii. substituindo \\(n = 10\\) por \\(n = 4\\).\n\n\n\n\n\n\n\n\n\nRESOLUÇÃO no R:\n\n\n\n\n\n\n\\(P(\\overline{L} &gt; 52,4)\\)\n\n\npnorm(52.4, \n      mean = 44.1, \n      sd = 13.4/sqrt(10), \n      lower.tail = FALSE)\n\n[1] 0.02507255\n\n\n\n\\(P(a \\le \\overline{L} \\le b) = 0,95\\)\n\n\na &lt;- qnorm((1-0.95)/2, \n      mean = 44.1, \n      sd = 13.4/sqrt(10), \n      lower.tail = TRUE)\na\n\n[1] 35.79475\n\nb &lt;- qnorm((1-0.95)/2, \n      mean = 44.1, \n      sd = 13.4/sqrt(10), \n      lower.tail = FALSE)\nb\n\n[1] 52.40525\n\n\n\nRepita os comandos acima para \\(n = 4\\)\n\n\n\n\n\n\n\n\n\n\nAtenção\n\n\n\nNeste exercício aplicamos o que aprendemos sobre o TCL para estimar probabilidades de eventos, assumindo a distribuição normal das médias amostaris. É importante resaltar, no entanto, que a distribuição altamente assimétrica do comprimento dos robalos (Figura 6) e um tamanho amostral reduzido (\\(n = 10\\) e \\(n = 4\\)) dificilmente justificariam o uso do TCL como vemos na Figura 4.\n\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/introducao_r/data-frames.html",
    "href": "conteudo/introducao_r/data-frames.html",
    "title": "(Básico da) Manipulação de data frames",
    "section": "",
    "text": "Embora seja possivel criar um data frame entrando com os dados diretamente via linha de comando, é mais eficiente importá-los a partir de arquivos texto (.csv, .txt)."
  },
  {
    "objectID": "conteudo/introducao_r/data-frames.html#importando-arquivos-.csv",
    "href": "conteudo/introducao_r/data-frames.html#importando-arquivos-.csv",
    "title": "(Básico da) Manipulação de data frames",
    "section": "1 Importando arquivos .csv",
    "text": "1 Importando arquivos .csv\nUm arquivo do tipo .csv pode ser lido com a função read.csv. Faça o download do conjunto de dados dbenv.csv disponível no repositório datasets e salve-o em sua pasta de trabalho (ex. \"C:/seu_caminho/Introducao_R\"). Ao abrir o arquivo em algum editor de texto verá que ele é composto por \\(30\\) linhas por \\(11\\) colunas.\nApós fazer o download, você pode importar o conjunto de dados utilizando o comando:\n\ndbenv &lt;- read.csv(file = \"C:/seu_caminho/Introducao_R/dbenv.csv\", \n                 header = TRUE, dec = '.', sep = ',')\n\nA função read.csv possui diferentes argumentos. A argumento header define se a primeira linha consiste do cabeçalho (TRUE) ou não (FALSE). O argumento dec define se o separador decimal consiste de vírgula ou ponto e o argumento sep informa sobre qual é o caracter separador de colunas utilizado no arquivo. No arquivo em questão as colunas são separadas por vírgulas. Outros tipos de separadores comuns são ponto-e-vírgula ou tabulações.\nConfira os nomes das \\(11\\) variáveis (cabeçalho), a dimensão da tabela (número de linhas e colunas) e sua estrutura (um data.frame formado por \\(11\\) vetores numéricos).\n\ndbenv\n\n    dfs alt   slo  flo pH har pho nit amm oxy bdo\n1     3 934 6.176   84 79  45   1  20   0 122  27\n2    22 932 3.434  100 80  40   2  20  10 103  19\n3   102 914 3.638  180 83  52   5  22   5 105  35\n4   185 854 3.497  253 80  72  10  21   0 110  13\n5   215 849 3.178  264 81  84  38  52  20  80  62\n6   324 846 3.497  286 79  60  20  15   0 102  53\n7   268 841 4.205  400 81  88   7  15   0 111  22\n8   491 792 3.258  130 81  94  20  41  12  70  81\n9   705 752 2.565  480 80  90  30  82  12  72  52\n10  990 617 4.605 1000 77  82   6  75   1 100  43\n11 1234 483 3.738 1990 81  96  30 160   0 115  27\n12 1324 477 2.833 2000 79  86   4  50   0 122  30\n13 1436 450 3.091 2110 81  98   6  52   0 124  24\n14 1522 434 2.565 2120 83  98  27 123   0 123  38\n15 1645 415 1.792 2300 86  86  40 100   0 117  21\n16 1859 375 3.045 1610 80  88  20 200   5 103  27\n17 1985 348 1.792 2430 80  92  20 250  20 102  46\n18 2110 332 2.197 2500 80  90  50 220  20 103  28\n19 2246 310 1.792 2590 81  84  60 220  15 106  33\n20 2477 286 2.197 2680 80  86  30 300  30 103  28\n21 2812 262 2.398 2720 79  85  20 220  10  90  41\n22 2940 254 2.708 2790 81  88  20 162   7  91  48\n23 3043 246 2.565 2880 81  97 260 350 115  63 164\n24 3147 241 1.386 2976 80  99 140 250  60  52 123\n25 3278 231 1.792 3870 79 100 422 620 180  41 167\n26 3579 214 1.792 3910 79  94 143 300  30  62  89\n27 3732 206 2.565 3960 81  90  58 300  26  72  63\n28 3947 195 1.386 4320 83 100  74 400  30  81  45\n29 4220 183 1.946 6770 78 110  45 162  10  90  42\n30 4530 172 1.099 6900 82 109  65 160  10  82  44\n\ncolnames(dbenv)\n\n [1] \"dfs\" \"alt\" \"slo\" \"flo\" \"pH\"  \"har\" \"pho\" \"nit\" \"amm\" \"oxy\" \"bdo\"\n\ndim(dbenv)\n\n[1] 30 11\n\n\n\n\n\n\n\n\nAcessando .csv em uma url\n\n\n\nComo este arquivo está em um repositório na nuvem, poderia ser lido acessando diretamente sua url, sem a necessidade de fazer o download:\n\ndbenv &lt;- read.csv(file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/dbenv.csv\", \n                 header = TRUE, dec = '.', \n                 sep = ',')\n\n\n\n\n\n\n\n\n\nIniciando uma seção de trabalho\n\n\n\nUma seção no R, se refere ao ambiente em que ficam armazenados os objetos (vetores, matrizes, data frames, etc.) criados durante o processo de manipulação e análise de dados. Ao fechar uma seção do R (ex. ao sair do RStudio), esta pode ser salva guardando os objetos criados. O arquivo de uma seção é salvo com extensão .RData.\nAo abrir um novo script (com extensão .r) em um editor de texto é importante definir o diretório de trabalho, que será o local onde ficarão dados e onde serão salvos os resultados do trabalho (ex. figuras, tabelas, etc.). No RStudio, um novo script pode ser aberto via menu Arquivo --&gt; Novo script. Ao iniciar o R-Studio abre-se uma nova seção. O diretório desta seção pode ser verificado pelo comando:\n\ngetwd()\n\nPara criar uma pasta (ex. Introducao_R) e direcionar a seção de trabalho para esta pasta utiliza-se a função setwd():\n\nsetwd(\"C:/seu_caminho/Introducao_R\")\n\nA função getwd() pode ser utilizada para verificar se a alteração de diretório foi realizada\n\ngetwd()\n\n\n\nC:/seu_caminho/Introducao_R\n\n\nA partir deste momento o R irá ler e salvar aquivos sempre a partir desse diretório."
  },
  {
    "objectID": "conteudo/introducao_r/data-frames.html#manipulação-de-data-frames",
    "href": "conteudo/introducao_r/data-frames.html#manipulação-de-data-frames",
    "title": "(Básico da) Manipulação de data frames",
    "section": "2 Manipulação de data frames",
    "text": "2 Manipulação de data frames\n\n2.1 Seleção de linhas e colunas em data frames\nNo data frame os nomes das colunas e linhas podem ser acessados por:\n\ncolnames(dbenv)\n\n [1] \"dfs\" \"alt\" \"slo\" \"flo\" \"pH\"  \"har\" \"pho\" \"nit\" \"amm\" \"oxy\" \"bdo\"\n\n\n\nrownames(dbenv)\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\"\n\n\nOs números “entre aspas” significam que estão sendo lidos como caracteres.\nColunas específicas podem ser acessadas por meio de seus nomes:\n\ncolunas &lt;- c(\"dfs\", \"flo\", \"oxy\")\ndbenv[,colunas]\n\n    dfs  flo oxy\n1     3   84 122\n2    22  100 103\n3   102  180 105\n4   185  253 110\n5   215  264  80\n6   324  286 102\n7   268  400 111\n8   491  130  70\n9   705  480  72\n10  990 1000 100\n11 1234 1990 115\n12 1324 2000 122\n13 1436 2110 124\n14 1522 2120 123\n15 1645 2300 117\n16 1859 1610 103\n17 1985 2430 102\n18 2110 2500 103\n19 2246 2590 106\n20 2477 2680 103\n21 2812 2720  90\n22 2940 2790  91\n23 3043 2880  63\n24 3147 2976  52\n25 3278 3870  41\n26 3579 3910  62\n27 3732 3960  72\n28 3947 4320  81\n29 4220 6770  90\n30 4530 6900  82\n\n\nOu por suas posições:\n\ncolunas_num &lt;- c(1, 3, 4)\ndbenv[,colunas_num]\n\n    dfs   slo  flo\n1     3 6.176   84\n2    22 3.434  100\n3   102 3.638  180\n4   185 3.497  253\n5   215 3.178  264\n6   324 3.497  286\n7   268 4.205  400\n8   491 3.258  130\n9   705 2.565  480\n10  990 4.605 1000\n11 1234 3.738 1990\n12 1324 2.833 2000\n13 1436 3.091 2110\n14 1522 2.565 2120\n15 1645 1.792 2300\n16 1859 3.045 1610\n17 1985 1.792 2430\n18 2110 2.197 2500\n19 2246 1.792 2590\n20 2477 2.197 2680\n21 2812 2.398 2720\n22 2940 2.708 2790\n23 3043 2.565 2880\n24 3147 1.386 2976\n25 3278 1.792 3870\n26 3579 1.792 3910\n27 3732 2.565 3960\n28 3947 1.386 4320\n29 4220 1.946 6770\n30 4530 1.099 6900\n\n\nO mesmo é válido para as linhas.\n\nlinhas &lt;- c(\"3\", \"7\", \"9\")\ndbenv[linhas,]\n\n  dfs alt   slo flo pH har pho nit amm oxy bdo\n3 102 914 3.638 180 83  52   5  22   5 105  35\n7 268 841 4.205 400 81  88   7  15   0 111  22\n9 705 752 2.565 480 80  90  30  82  12  72  52\n\n\n\nlinhas_num &lt;- c(3, 7, 9)\ndbenv[linhas_num,]\n\n  dfs alt   slo flo pH har pho nit amm oxy bdo\n3 102 914 3.638 180 83  52   5  22   5 105  35\n7 268 841 4.205 400 81  88   7  15   0 111  22\n9 705 752 2.565 480 80  90  30  82  12  72  52\n\n\nSub-conjunto do data frame podem ser selecionados combinando esses procedimentos.\n\ndbenv[linhas,colunas]\n\n  dfs flo oxy\n3 102 180 105\n7 268 400 111\n9 705 480  72\n\n\n\n\n2.2 Adicionando novas colunas\nEste conjunto de dados mostra medidas físicas e químicas obtidas em um riacho amostrado desde a cabeceira até a foz. O ponto mais alto (934 m de altitude) está a 3 km da cabeceira enquanto o ponto mais baixo está a 172 m de altitude e a 4530 km da cabeceira. Vamos criar uma nova variável categorizando os trechos do rio em Alto, Medio e Baixo assumindo a seguinte relação:\n\n\\(0\\) a \\(300\\) m: Baixo;\n\\(300\\) a \\(600\\) m: Médio;\nAcima de \\(600\\) m: Alto.\n\n\nelv_cat &lt;- cut(dbenv$alt, breaks = c(0, 300, 600, 1000), \n              labels = c(\"Baixo\", \"Medio\", \"Alto\"))\n\nA inserção do novo objeto elv_cat no data frame pode ser feito simplesmente por:\n\ndbenv$trecho &lt;- elv_cat\n\nA nova coluna denominada trecho foi inserida no data frame, como pode ser visto:\n\nhead(dbenv)\n\n  dfs alt   slo flo pH har pho nit amm oxy bdo trecho\n1   3 934 6.176  84 79  45   1  20   0 122  27   Alto\n2  22 932 3.434 100 80  40   2  20  10 103  19   Alto\n3 102 914 3.638 180 83  52   5  22   5 105  35   Alto\n4 185 854 3.497 253 80  72  10  21   0 110  13   Alto\n5 215 849 3.178 264 81  84  38  52  20  80  62   Alto\n6 324 846 3.497 286 79  60  20  15   0 102  53   Alto\n\n\nO mesmo pode ser realizado com a função transform(). Vamos utilizá-la para criar uma nova variável categórica a partir do oxigênio dissolvido, considerando 3 níveis de satuação: Pobre (\\(0\\) a \\(5\\)), Médio (\\(5\\) a \\(8\\)) e Saturado (acima de \\(8\\)).\n\ndbenv &lt;- transform(dbenv,  \n saturacao = cut(dbenv$oxy, breaks = c(0, 40, 109, 124), \n           labels = c(\"Pobre\", \"Medio\", \"Saturado\")))\n\nVeja agora o data frame\n\ndbenv\n\n    dfs alt   slo  flo pH har pho nit amm oxy bdo trecho saturacao\n1     3 934 6.176   84 79  45   1  20   0 122  27   Alto  Saturado\n2    22 932 3.434  100 80  40   2  20  10 103  19   Alto     Medio\n3   102 914 3.638  180 83  52   5  22   5 105  35   Alto     Medio\n4   185 854 3.497  253 80  72  10  21   0 110  13   Alto  Saturado\n5   215 849 3.178  264 81  84  38  52  20  80  62   Alto     Medio\n6   324 846 3.497  286 79  60  20  15   0 102  53   Alto     Medio\n7   268 841 4.205  400 81  88   7  15   0 111  22   Alto  Saturado\n8   491 792 3.258  130 81  94  20  41  12  70  81   Alto     Medio\n9   705 752 2.565  480 80  90  30  82  12  72  52   Alto     Medio\n10  990 617 4.605 1000 77  82   6  75   1 100  43   Alto     Medio\n11 1234 483 3.738 1990 81  96  30 160   0 115  27  Medio  Saturado\n12 1324 477 2.833 2000 79  86   4  50   0 122  30  Medio  Saturado\n13 1436 450 3.091 2110 81  98   6  52   0 124  24  Medio  Saturado\n14 1522 434 2.565 2120 83  98  27 123   0 123  38  Medio  Saturado\n15 1645 415 1.792 2300 86  86  40 100   0 117  21  Medio  Saturado\n16 1859 375 3.045 1610 80  88  20 200   5 103  27  Medio     Medio\n17 1985 348 1.792 2430 80  92  20 250  20 102  46  Medio     Medio\n18 2110 332 2.197 2500 80  90  50 220  20 103  28  Medio     Medio\n19 2246 310 1.792 2590 81  84  60 220  15 106  33  Medio     Medio\n20 2477 286 2.197 2680 80  86  30 300  30 103  28  Baixo     Medio\n21 2812 262 2.398 2720 79  85  20 220  10  90  41  Baixo     Medio\n22 2940 254 2.708 2790 81  88  20 162   7  91  48  Baixo     Medio\n23 3043 246 2.565 2880 81  97 260 350 115  63 164  Baixo     Medio\n24 3147 241 1.386 2976 80  99 140 250  60  52 123  Baixo     Medio\n25 3278 231 1.792 3870 79 100 422 620 180  41 167  Baixo     Medio\n26 3579 214 1.792 3910 79  94 143 300  30  62  89  Baixo     Medio\n27 3732 206 2.565 3960 81  90  58 300  26  72  63  Baixo     Medio\n28 3947 195 1.386 4320 83 100  74 400  30  81  45  Baixo     Medio\n29 4220 183 1.946 6770 78 110  45 162  10  90  42  Baixo     Medio\n30 4530 172 1.099 6900 82 109  65 160  10  82  44  Baixo     Medio\n\n\n\n\n2.3 Família apply e aggregate\nEm muitas situações temos interesse aplicar uma determinada função a cada linha ou a cada coluna de um data frame ou ainda para grupos distintos de linhas.\nObserve por exemplo que se extraímos a média aritmética da coluna pH (\\(\\times 10\\)).\n\nmean(dbenv$pH)  # média aritmética\n\n[1] 80.5\n\n\nO resultado é calculado para toda a coluna.\n\n\nFunção tapply\nPodemos estar interessados no entanto, em extrair as médias separadamente para os trechos alto, médio e baixo do rio. A função tapply() é útil nestas situações.\n\ntapply(dbenv$pH, dbenv$trecho, mean)\n\n   Baixo    Medio     Alto \n80.27273 81.22222 80.10000 \n\n\nA função acima, pode ser lida do modo:\n\nSelecione a coluna pH;\nAgrupe os elementos em função dos níveis em trecho (Baixo, Medio, Alto);\nCalcule a média aritmética para cada sub-grupo.\n\nNote que o resultado foi um vetor em que cada elemento corresponde à média de um sub-grupo. Funções que retornam mais de um valor resultam em um objeto no formato de lista. A função range() por exemplo, retorna dois valores (mínimo e máximo). Ao utilizá-la junto à função tapply() termos como resultado uma lista composta por um vetor para cada subgrupo.\n\ntapply(dbenv$pH, dbenv$trecho, range)\n\n$Baixo\n[1] 78 83\n\n$Medio\n[1] 79 86\n\n$Alto\n[1] 77 83\n\n\n\n\nFunção apply\nPodemos aplicar uma determinada função a todas as linhas ou colunas de um data frame (ou matriz).\n\napply(dbenv[,1:5], MARGIN = 2, mean)\n\n        dfs         alt         slo         flo          pH \n1879.033333  481.500000    2.757733 2220.100000   80.500000 \n\n\nO argumento MARGIN = 2 diz que desejamos aplicar a função ás colunas da matriz. Com MARGIN = 1 aplicamos a função às linhas da matriz.\n\n\nFunção lapply\nSe o objeto é do formato lista, o comando lapply() aplica uma função a cada elemento da lista. Considere a lista:\n\nnossalista &lt;- list(Ilha = c(\"Ilhabela\", \"Anchieta\", \"Cardoso\"), \n                  Areaskm2 = c(347.5, 8.3, 131), \n                  Bioma = rep(\"Mata Atlantica\",3),\n                  Lat = c(23, 25, 23),\n                  Long = c(45, 47, 45))\n\nVeja os resultados dos comandos abaixo:\n\nlapply(nossalista, sort)\n\n$Ilha\n[1] \"Anchieta\" \"Cardoso\"  \"Ilhabela\"\n\n$Areaskm2\n[1]   8.3 131.0 347.5\n\n$Bioma\n[1] \"Mata Atlantica\" \"Mata Atlantica\" \"Mata Atlantica\"\n\n$Lat\n[1] 23 23 25\n\n$Long\n[1] 45 45 47\n\n\n\n\n\n\n\n\nNota\n\n\n\nExistem outras funções neste grupo, Veja o help() destas funções pois são extremamente úteis na manipulação de data frames e listas.\n\n?tapply\n?apply\n?lapply\n?mapply\n?replicate\n\n\n\n\n\nFunção aggregate\nA função tapply() aplica uma função a subgrupos de uma única coluna. A função aggregate() faz o mesmo, porém para múltiplas colunas agrupadas de acordo com uma ou mais categorias. O comando abaixo calcula os valores médios das variáveis para os trechos alto, médio e baixo combinados com níveis de \\(pH\\).\n\nmedia.trecho &lt;- aggregate(dbenv[, 1:11], \n                         by = list(TRECHO = dbenv$trecho,\n                                   ALCALINO = dbenv$pH &gt;= 80),\n                         FUN = mean)\nmedia.trecho\n\n  TRECHO ALCALINO      dfs      alt      slo       flo       pH      har\n1  Baixo    FALSE 3472.250 222.5000 1.982000 4317.5000 78.75000 97.25000\n2  Medio    FALSE 1324.000 477.0000 2.833000 2000.0000 79.00000 86.00000\n3   Alto    FALSE  439.000 799.0000 4.759333  456.6667 78.33333 62.33333\n4  Baixo     TRUE 3402.286 228.5714 1.986571 3786.5714 81.14286 95.57143\n5  Medio     TRUE 1754.625 393.3750 2.501500 2206.2500 81.50000 91.50000\n6   Alto     TRUE  284.000 847.7143 3.396429  258.1429 80.85714 74.28571\n        pho       nit        amm       oxy      bdo\n1 157.50000 325.50000 57.5000000  70.75000 84.75000\n2   4.00000  50.00000  0.0000000 122.00000 30.00000\n3   9.00000  36.66667  0.3333333 108.00000 41.00000\n4  92.42857 274.57143 39.7142857  77.71429 73.57143\n5  31.62500 165.62500  7.5000000 111.62500 30.50000\n6  16.00000  36.14286  8.4285714  93.00000 40.57143"
  },
  {
    "objectID": "conteudo/introducao_r/data-frames.html#exportando-um-data-frame",
    "href": "conteudo/introducao_r/data-frames.html#exportando-um-data-frame",
    "title": "(Básico da) Manipulação de data frames",
    "section": "3 Exportando um data frame",
    "text": "3 Exportando um data frame\nFinalmente, podemos exportar o data frame media.trecho obtido acima para um arquivo Mediaportecho.csv.\n\nwrite.table(media.trecho, file = \"C:/seu_caminho/Introducao_R/Mediaportecho.csv\", \n            sep = \",\", dec = '.', row.names = FALSE, \n            col.names = TRUE)"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html",
    "title": "Gráficos em camadas",
    "section": "",
    "text": "O pacote ggplot2 no R é baseado na gramática de gráficos (Grammar of Graphics), que permite a construção de visualizações de dados de maneira declarativa. Com ele, é possível criar uma ampla variedade de gráficos, desde simples gráficos de barras e dispersão até complexas visualizações com múltiplas camadas e facetas. O ggplot2 facilita a personalização detalhada dos gráficos, incluindo temas, cores e anotações, tornando-o uma escolha popular entre estatísticos, cientistas de dados e analistas para comunicar informações de maneira clara e eficiente.\nO ggplot2 gera gráficos a partir das colunas de um data frame, o que significa que o domínio de ferramentas de transformação de data frames é fundamental para a criação de visualizações eficazes. Cada elemento do gráfico, como eixos, pontos, linhas e barras, é mapeado a partir das variáveis presentes no data frame. Isso permite grande flexibilidade na representação gráfica.\nA estrutura em camadas do ggplot2 fornece uma base coesa e flexível para a codificação de gráficos. Por exemplo, é possível começar com uma camada base que define o sistema de coordenadas e os eixos, adicionar uma camada de pontos para criar um gráfico de dispersão, e então sobrepor camadas adicionais para ajustar a estética, adicionar linhas de tendência, ou incluir etiquetas. Essa abordagem em camadas facilita a personalização e a atualização dos gráficos, tornando o processo de criação de visualizações complexas mais organizado e intuitivo. Isso não só melhora a clareza e a legibilidade do código, mas também promove uma maior capacidade de experimentação e exploração dos dados, permitindo que analistas e pesquisadores ajustem e aprimorem suas visualizações de forma eficiente.\nAqui faremos uma introdução aos elementos princiais do ggplot2. Para saber mais verifique as referências abaixo:\nInstale o pacote gglot2 e carregue-o com os demais pacotes utilizados nessa seção. O pacote será adicionado para compor múltiplos gráficos em uma mesma figura.\ninstall.packages(\"ggplot2\")\ninstall.packages(\"patchwork\")\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#histogramas",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#histogramas",
    "title": "Gráficos em camadas",
    "section": "1 Histogramas",
    "text": "1 Histogramas\nFaça um histograma dos dados de vazão da tabela HubbardBrook.csv (datasets).\n\nhbrook &lt;- read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/HubbardBrook.csv\")\nhbrook\n\n# A tibble: 62 × 4\n    Year Treatment   Flow Precipitation\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n 1  1958 Deforested  645.         1168.\n 2  1959 Deforested 1012.         1483.\n 3  1960 Deforested  825.         1321.\n 4  1961 Deforested  470.          980.\n 5  1962 Deforested  777.         1232.\n 6  1963 Deforested  774.         1139.\n 7  1964 Deforested  712.         1175.\n 8  1965 Deforested  599.         1115.\n 9  1966 Deforested 1189.         1222.\n10  1967 Deforested 1132.         1315.\n# ℹ 52 more rows\n\n\n\nggplot(data = hbrook, mapping = aes(x = Flow)) +\n  geom_histogram(color = \"blue\", fill = \"lightblue\")\n\n\n\n\n\n\n\n\nO comando acima contém duas camadas, separadas pelo símbolo +, que indica o fim de uma camada e o início de outra. No ggplot2, cada camada adiciona ou formata um elemento do gráfico. A ordem das camadas geralmente não importa, mas organizá-las bem facilita a leitura do código. No exemplo, temos:\n\nFunção ggplot(): Define a estrutura básica do gráfico. O argumento data = especifica o data frame que contém os dados. O argumento mapping = define a estética do gráfico definida pela função aes(x = Flow), indicando que o eixo \\(x\\) representará a variável Flow.\nFunção geom_histogram(): Define a geometria do gráfico, aqui um histograma. A cor da borda é definida por color = \"blue\" e o preenchimento por fill = \"lightblue\".\n\nFormatações adicionais:\n\nggplot(data = hbrook, mapping = aes(x = Flow)) +\n  geom_histogram(color = \"blue\", fill = \"lightblue\") +\n  labs(title = \"Histograma de vazão\", \n       x = bquote(Vazao (m^3/s)),\n       y = \"Contagem\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nUm título foi inserido, os nomes para os eixos \\(x\\) e \\(y\\) foram definidos e o título foi centralizado por theme(plot.title = element_text(hjust = 0.5)).\nO histograma anterior, combina dados de vazão anual na bacia Deforested e Reference, identificadas pela variável Treatment. Para verificar histogramas separados de acordo com os níveis desta variável podemos usar a função facet_grid()\n\nggplot(data = hbrook, mapping = aes(x = Flow)) +\n  geom_histogram(color = \"blue\", fill = \"lightblue\") +\n  labs(title = \"Histograma de vazão\", \n       x = bquote(Vazao (m^3/s)),\n       y = \"Contagem\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_grid(rows = vars(Treatment))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutras geometrias gráficas\n\n\n\nAlém dos histogramas, existem muitas outras geometrias gráficas do tipo geom_NOME(). Algumas das mais utilizadas são: geom_abline(), geom_bar(), geom_boxplot(), geom_line(), geom_point(), geom_smooth(), geom_text(), entre muitas outras."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#boxplots",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#boxplots",
    "title": "Gráficos em camadas",
    "section": "2 Boxplots",
    "text": "2 Boxplots\nA princípio, a distribuição das vazões não são muito diferentes entre os tratamentos. Um boxplot pode ser utilizado para visualizar estas distribuições.\n\nggplot(data = hbrook, mapping = aes(y = Flow, x = Treatment)) +\n  geom_boxplot() +\n  labs(y = bquote(Vazao (m^3/s)),\n       x = \"\")\n\n\n\n\n\n\n\n\nO boxplot exige que sejam definidas uma variável contínua, neste caso Flow em \\(y\\) como função de uma variável categórica, neste caso Treatment em \\(x\\)."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#gráfico-de-dispersão",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#gráfico-de-dispersão",
    "title": "Gráficos em camadas",
    "section": "3 Gráfico de dispersão",
    "text": "3 Gráfico de dispersão\nPara verificar a relação entre vazão e precipitação, pode-se plotar um gráfico de dispersão entre Flow e Precipitation.\n\nggplot(data = hbrook, mapping = aes(y = Flow, x = Precipitation)) +\n  geom_point(shape = 21) +\n  labs(y = bquote(Vazão (m^3/s)),\n       x = bquote(Precipitação (m^3/ano)))\n\n\n\n\n\n\n\n\nO Treatment pode ser adicinado a esta figura como cores diferentes.\n\nggplot(data = hbrook, \n       mapping = aes(y = Flow, \n                     x = Precipitation, \n                     fill = Treatment)) +\n  geom_point(shape = 21, size = 3) +\n  labs(y = bquote(Vazão (m^3/s)),\n       x = bquote(Precipitação (m^3/ano))) +\n  guides(fill=guide_legend(title=\"Estado da área\")) +\n  scale_fill_manual(values = c(\"blue\", \"green\"))"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#séries-temporais",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#séries-temporais",
    "title": "Gráficos em camadas",
    "section": "4 Séries temporais",
    "text": "4 Séries temporais\nO operador pipe pode ser combinado com a função ggplot() para filtrar as bacia Deforested e representar a vazão em uma série temporal.\n\nhbrook |&gt;  \n  filter(Treatment == \"Deforested\") |&gt;  \n  ggplot(mapping = aes(y = Flow, x = Year)) +\n    geom_line() +\n    labs(y = bquote(Vazão (m^3/s)),\n         x = \"Ano\")\n\n\n\n\n\n\n\n\nPodem ser vistas as séries temporais para os dois tratamentos, representando-os em figuras diferentes.\n\nggplot(data = hbrook, mapping = aes(y = Flow, x = Year)) +\n  geom_line() +\n  labs(y = bquote(Vazão (m^3/s)),\n       x = \"Ano\") +\n  facet_grid(rows = vars(Treatment))\n\n\n\n\n\n\n\n\nOu na mesma figura em cores diferentes.\n\nggplot(data = hbrook, \n       mapping = aes(y = Flow, x = Year, color = Treatment)) +\n  geom_line() +\n  labs(y = bquote(Vazão (m^3/s)),\n       x = \"Ano\") +\n  scale_color_manual(values = c(\"blue\", \"green\"))\n\n\n\n\n\n\n\n\nO desmatamento da bacia Deforested ocorreu em \\(1965\\), e intervenções para impedir o desenvolvimento da vegetação foram realizadas até \\(1970\\). Esse intervalo pode ser representado por um retângulo no gráfico.\n\nggplot(data = hbrook, \n       mapping = aes(y = Flow, x = Year, color = Treatment)) +\n  geom_line() +\n  labs(y = bquote(Vazão (m^3/s)),\n       x = \"Ano\") +\n  scale_color_manual(values = c(\"blue\", \"green\")) +\n  scale_x_continuous(breaks = seq(1955, 1990, by = 5)) +\n  annotate(\"rect\", \n           xmin = 1965, xmax = 1970, \n           ymin = -Inf, ymax = Inf, \n           alpha = 0.2, fill = \"red\") +\n  theme_test()\n\n\n\n\n\n\n\n\nComo as vazões foram mensuradas nos mesmos anos, é possível calcular a diferença de vazão entre os tratamentos e representar essas diferenças graficamente.\n\nhbrook_largo &lt;- hbrook |&gt;\n  select(-Precipitation) |&gt;\n  pivot_wider(names_from = Treatment, values_from = Flow) |&gt; \n  mutate(diffDR = Deforested - Reference)\nhbrook_largo\n\n# A tibble: 31 × 4\n    Year Deforested Reference diffDR\n   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1  1958       645.      567.   77.8\n 2  1959      1012.      918.   93.8\n 3  1960       825.      752.   73.2\n 4  1961       470.      436.   33.8\n 5  1962       777.      699.   78.0\n 6  1963       774.      663.  111. \n 7  1964       712.      630.   81.7\n 8  1965       599.      547.   52.2\n 9  1966      1189.      727.  463. \n10  1967      1132.      781.  351. \n# ℹ 21 more rows\n\n\n\nggplot(data = hbrook_largo, \n       mapping = aes(y = diffDR, x = Year)) +\n  geom_line() +\n  geom_point(shape = 19) +\n  labs(y = bquote(Diferença~de~Vazão (m^3/s)),\n       x = \"Ano\") +\n  scale_x_continuous(breaks = seq(1955, 1990, by = 5)) +\n  annotate(\"rect\", \n           xmin = 1965, xmax = 1970, \n           ymin = -Inf, ymax = Inf, \n           alpha = 0.2, fill = \"red\") +\n  theme_test()"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#gráfico-de-barras",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#gráfico-de-barras",
    "title": "Gráficos em camadas",
    "section": "5 Gráfico de barras",
    "text": "5 Gráfico de barras\nSerá criada uma variável categórica Vazao_cat contendo os níveis Extrema (se Flow &gt;= 1000 m^3/s) e Normal caso contrário. Em seguida, será contado o número de observações com vazão extrema.\n\nextremo &lt;- 1000\n\nhbrook2 &lt;- hbrook  |&gt;\n  mutate(Vazao_cat = if_else(Flow &gt;= extremo, \n                             true = \"Extrema\", \n                             false = \"Normal\"))\n\nggplot(data = hbrook2, mapping = aes(x = Vazao_cat)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nSe a variável estiver no eixo \\(y\\), aes(y = Vazao_cat), o gráfico será desenhado na horizontal.\n\nggplot(data = hbrook2, mapping = aes(y = Vazao_cat)) +\n  geom_bar()"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#temas-no-ggplot2",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#temas-no-ggplot2",
    "title": "Gráficos em camadas",
    "section": "6 Temas no ggplot2",
    "text": "6 Temas no ggplot2\nO ggplot2 oferece uma série de temas pré-formatados para facilitar a personalização dos gráficos. Para aplicar um tema, basta adicionar uma camada com o nome do tema desejado usando theme_NOME(). Veja um exemplo com o tema theme_classic():\n\nggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_classic()\n\n\n\n\n\n\n\n\nOs temas básicos disponíveis no ggplot2 incluem:\n\n\nCódigo\ng1 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_grey() +\n  labs(title = \"theme_grey()\")\n\ng2 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_gray() +\n  labs(title = \"theme_gray()\")\n\ng3 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_bw() +\n  labs(title = \"theme_bw()\")\n\ng4 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_linedraw() +\n  labs(title = \"theme_linedraw()\")\n\ng5 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_light() +\n  labs(title = \"theme_light()\")\n\ng6 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_dark() +\n  labs(title = \"theme_dark()\")\n\ng7 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"theme_minimal()\")\n\ng8 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_classic() +\n  labs(title = \"theme_classic()\")\n\ng9 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_void() +\n  labs(title = \"theme_void()\")\n\ng10 &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_test() +\n  labs(title = \"theme_test()\")\n\n\n\n\nCódigo\ngcol1 &lt;- g1 / g2 / g3 / g4 / g5\ngcol2 &lt;- g6 / g7 / g8 / g9 / g10\n\ngcol1 | gcol2"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#salvando-uma-figura-gerada-pelo-gglot2.",
    "href": "conteudo/manipulacao-dados-R/grafico_ggplot2.html#salvando-uma-figura-gerada-pelo-gglot2.",
    "title": "Gráficos em camadas",
    "section": "7 Salvando uma figura gerada pelo gglot2.",
    "text": "7 Salvando uma figura gerada pelo gglot2.\nPara salvar um gráfico gerado com ggplot2, utiliza-se a função ggsave(). Veja o exemplo abaixo:\n\nggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_classic()\n\nggsave(filename = \"Exemplo_ggsave.png\", \n       width = 20, height = 20, units = \"cm\")  \n\nPor padrão, a função ggsave() salva o último gráfico criado. Caso seja necessário salvar um gráfico específico, pode-se usar o argumento plot = objeto_grafico.\n\nobjeto_grafico &lt;- ggplot(hbrook, mapping = aes(x = Year, y = Flow, color = Treatment)) +\n  geom_line() +\n  theme_classic()\n\nggsave(filename = \"objeto_grafico_plt.png\", \n       plot = objeto_grafico,\n       device = \"png\",\n       width = 20, \n       height = 20,\n       units = \"cm\",\n       dpi = 480)"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/import_export.html",
    "href": "conteudo/manipulacao-dados-R/import_export.html",
    "title": "Importando/Exportando dados",
    "section": "",
    "text": "O pacote responsável pela importação de dados no tidyverse é o readr. Este pacote permite importar arquivos de texto nos formatos .csv ou .txt.\nExistem diversas funções no pacote readr(veja aqui). A função read_csv() importa arquivos texto em que as colunas são separadas por vírgulas. A função read_tsv() importa arquivos texto em que as colunas são separadas por tabulações."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/import_export.html#importando-dados-de-arquivos-texto",
    "href": "conteudo/manipulacao-dados-R/import_export.html#importando-dados-de-arquivos-texto",
    "title": "Importando/Exportando dados",
    "section": "1 Importando dados de arquivos texto",
    "text": "1 Importando dados de arquivos texto\nA função read_delim() oferece mais controle sobre o tipo de delimitador de colunas (vírgulas, tabulações, ponto-e-vírgula, entre outros) ou o identificador decimal (vírgulas ou ponto).\nCarrege o pacote readr e importe o arquivo Reservatorios_Parana_parcial.csv disponível no repositório datasets do . É possível importar o arquivo diretamente do repositório:\n\nlibrary(readr)\nres = read_delim(file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n                  delim = ',',\n                  locale = locale(decimal_mark = '.',\n                                  encoding = 'latin1'))\n\n\n\n\n\n\n\nNota\n\n\n\nSe optar por fazer o download do arquivo, basta acessar pelo link (Reservatorios_Parana_parcial.csv), salvá-lo em seu diretório de trabalho e importar com o comando:\n\nres = read_delim(file = \"Reservatorios_Parana_parcial.csv.csv\", delim = \",\")\n\n\n\nVerifique o objeto importado.\n\nres\n\n# A tibble: 31 × 11\n   Reservatorio Bacia  Fechamento   Area Trofia    pH Condutividade Alcalinidade\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n 1 Cavernoso    Iguacu       1965   2.9  Oligo…   7.4          33.1        140. \n 2 Curucaca     Iguacu       1982   2    Oligo…   7            32.4        126. \n 3 Foz do Areia Iguacu       1980 139    Oligo…   7.3          35.5         97  \n 4 Irai         Iguacu       2000  15    EutrÃ…   6.9          50.2          3.3\n 5 JMF          Iguacu       1970   0.45 Mesot…   7.3          40.2          3.7\n 6 Jordao       Iguacu       1996   3.4  Oligo…   7.1          23.7        153. \n 7 Passauna     Iguacu       1978  14    Oligo…   8.8         126.         526  \n 8 Piraquara    Iguacu       1979   3.3  Oligo…   7.1          22.8         50.7\n 9 Salto Caxias Iguacu       1998 124    Oligo…   7.3          39.6        106  \n10 Salto do Vau Iguacu       1959   2.9  Oligo…   6.5          23.2        279  \n# ℹ 21 more rows\n# ℹ 3 more variables: P.total &lt;dbl&gt;, Riqueza &lt;dbl&gt;, CPUE &lt;dbl&gt;\n\n\nO objeto é do tipo tibble com 31 linhas por 11 colunas. Uma tibble é uma versão moderna do data.frame que preserva aspectos eficazes para manipulação, visualização e transformação de dados.\n\nclass(res)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/import_export.html#exportando-um-data-frame",
    "href": "conteudo/manipulacao-dados-R/import_export.html#exportando-um-data-frame",
    "title": "Importando/Exportando dados",
    "section": "2 Exportando um data frame",
    "text": "2 Exportando um data frame\nA função para exportar data frames no pacote readr é write_delim() e outras funções análogas. Para exportar uma parte do data frame utiliza-se o comando:\n\nwrite_delim(res[1:10, 3:5],\n            file = \"Reservatorios_Parana_parcial.csv\", \n            delim = ',')"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/tidyverse.html",
    "href": "conteudo/manipulacao-dados-R/tidyverse.html",
    "title": "Os pacotes em tidyverse",
    "section": "",
    "text": "O Tidyverse é uma coleção de pacotes em R projetados para ciência de dados que inclui ferramentas para importar, arrumar, transformar, visualizar e modelar dados, todas integradas de forma coesa para tornar a análise de dados mais eficiente e intuitiva. O Tidyverse facilita a prática de uma ciência de dados mais limpa, clara e reproduzível. Entre os pacotes mais populares do Tidyverse estão ggplot2 para visualização de dados, dplyr para manipulação de dados e tidyr para arrumação de dados.\nAlém destes existem ainda outros que se integram bem à filosofia do tidyverse como o lubridade (manipulação de datas), o readxl (leitura de arquivos .xls e .xlsx), além de muitos outros. Veremos algumas funções e práticas úteis utilizando estes pacotes. Para uma visão geral de cada pacote, verifique as Cheatsheets que oferecem um resumo sobre as funções principais."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/tidyverse.html#instalando-os-pacotes",
    "href": "conteudo/manipulacao-dados-R/tidyverse.html#instalando-os-pacotes",
    "title": "Os pacotes em tidyverse",
    "section": "1 Instalando os pacotes",
    "text": "1 Instalando os pacotes\nCada um dos pacotes incorporados no tidyverse pode ser instalado individualmente. Por exemplo:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\n\nEntretando, ao instalar o tidyverse, todos são instalados de uma única vez:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/tidyverse.html#carregando-os-pacotes",
    "href": "conteudo/manipulacao-dados-R/tidyverse.html#carregando-os-pacotes",
    "title": "Os pacotes em tidyverse",
    "section": "2 Carregando os pacotes",
    "text": "2 Carregando os pacotes\nAo iniciar uma seção, você deve sempre carregar os pacotes que irá utilizar. No caso do tidyverse, você pode carregar cada pacote individualmente:\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nOu todos de uma única vez:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html",
    "title": "Introdução ao teste de hipóteses",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nUm dos objetivos centrais em estatística é fazer inferências válidas para a população examinando as características de uma amostra. Considere as afirmações abaixo:\nTodas estas afirmações são na realidade hipóteses, sobre um ou mais parâmetros de uma população estatística que podem ser testadas por meio de experimentos adequados. A experimentação nos permite tirar conclusões sobre determitada hipótese com base na amostra. Mais especificamente, queremos saber se os dados em mãos nos permitem ou não refutar uma hipótese inicial. Portanto, se desejamos fazer uma inferência sobre um parâmetro da população estatística (ex.: sua média \\(\\mu\\)), devemos iniciar com uma afirmação sobre a posição deste parâmetro, que denominamos de hipótese nula (\\(H_0\\))."
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html#probabilidade-e-teste-de-hipóteses",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html#probabilidade-e-teste-de-hipóteses",
    "title": "Introdução ao teste de hipóteses",
    "section": "1 Probabilidade e teste de hipóteses",
    "text": "1 Probabilidade e teste de hipóteses\nA média \\(\\overline{X}\\) de uma amostra será nossa melhor evidência a respeito de \\(\\mu\\). Tendo este valor, podemos nos perguntar:\n\nO valor obtido de \\(\\overline{X}\\) é condizente com o esperado segundo \\(H_0\\)?\n\nCaso \\(\\overline{X}\\) esteja muito próximo a \\(\\mu\\), não há evidências para rejeitar \\(H_0\\). Por outro lado, um valor de \\(\\overline{X}\\) muito distante de \\(\\mu\\) irá colocar em dúvida a afirmação estabelecida em \\(H_0\\). O ponto relevante aqui é decidirmos quão distante de \\(\\mu\\) deve estar \\(\\overline{X}\\) para que rejeitemos \\(H_0\\)? Esta resposta poderá ser respondida somente com o auxílio de um modelo probabilístico aplicado ao experimento em questão.\nSeja \\(H_0\\) verdadeira, é esperado que a probabilidade de \\(\\overline{X}\\) estar próximo a \\(\\mu\\) é alta. Portanto, uma pergunta melhor formulada seria:\n\nSendo \\(H_0\\) verdadeira, qual é a probabilidade de que uma determinada média amostral \\(\\overline{X}\\) esteja tão ou mais distante de \\(\\mu\\) quanto o observado em nossa amostra particular?\n\n\n1.1 Um modelo de distribuição das médias amostrais para testar \\(H_0\\)\nA pergunta feita acima é de natureza probabilística, de modo que para respondê-la iremos precisar estabelecer um modelo probabilístico para a distribuição das médias amostrais. De acordo com o que temos discutido até este ponto, Teorema Central do Limite (TCL) estabelece que a distribuição normal é um bom modelo neste situação.\nDesta forma, para um \\(H_0\\) verdadeiro, seria esperado que a distribuição das médias amostrais resultantes de um procedimento experimental tivesse o formato de um distribuição normal, centrada em \\(110\\) mm. Segundo o TCL, a distribuição seria centrada em \\(\\mu\\) e o desvio padrão seria definido pelo erro padrão da média, isto é, \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\).\nDigamos ainda que o modelo climático estabeleça que desvio padrão para a quantidade de chuva seja \\(\\sigma = 30\\). Neste caso, o erro padrão seria de \\(\\sigma_{\\mu} = \\frac{30}{\\sqrt{n}}\\).\nFeito isto, temos em mãos o modelo probabilístico que, aliado a uma amostra particular, nos permitirá concluir se há evidências para rejeitar \\(H_0\\) em favor de \\(H_a\\).\n\n\n1.2 Definindo o limite de rejeição para \\(H_0\\): nivel de significância \\(\\alpha\\)\nSegundo a distribuição normal, a probabilidade do valor observado \\(\\overline{X}\\) estar tão ou mais distante de \\(\\mu\\) na distribuição \\(Z\\) é calculando por:\n\\[z = \\frac{\\overline{X} - \\mu}{\\sigma_{\\overline{X}}}\\]\nO valor de \\(z\\) calculado é chamado de estatitica do teste. Com o uso da Tabela \\(Z\\), esta estatística será utilizada para encontrar:\n\\[P(Z \\ge z) = P(\\overline{X} \\ge \\mu)\\] A probabilidade \\(P(Z \\ge z)\\) é encontrada na distribuição normal padronizada em que \\(Z \\sim \\mathcal{N}(0,\\,\\frac{\\sigma}{\\sqrt{n}})\\) e como nossa pergunta se refere à distância entre \\(\\overline{X}\\) e \\(\\mu\\), devemos encontar também \\(P(Z \\le -z)\\), de modo que a probabilidade que nos interessa é denominada de valor de p de um teste de hipóteses:\n\\[p = P(Z \\ge |z|)\\]\nO valor de \\(p\\) é a área destacada em vermelho da distribuição normal padronizada:\n\n\n\n\n\n\n\n\nFigura 1: Representação da área de rejeição na distribuição Z.\n\n\n\n\n\nA área destacada em vermelho diminui conforme \\(\\overline{X}\\) se distancia de \\(\\mu\\) e aumenta se \\(\\overline{X}\\) está próximo a \\(\\mu\\).\n\n\n\n\n\n\nO valor de p e nível de significância\n\n\n\nMede a probabilidade de encontrarmos \\(\\overline{X}\\) tão ou mais distante de \\(\\mu\\), assumindo que \\(H_0\\) seja verdadeira. Se \\(p\\) for muito pequeno, a probabilidade de que \\(\\overline{X}\\) seja condizente com \\(H_0\\) diminui. Neste caso dizemos que é improvável que \\(\\overline{X}\\) seja proviniente de \\(H_0\\), o que nos leva levando a rejeitar a hipótese nula em favor de \\(H_a\\).\nA decisão de rejeitar \\(H_0\\) depende do limite de rejeição \\(\\alpha\\), também chamado de nivel crítico ou nível de significância. A definir o valor de \\(\\alpha\\), a conclusão de um teste estatístico se dá por (Figura 2):\n\nSe \\(p &gt; \\alpha\\) –&gt; ACEITAMOS \\(H_0\\), \\(\\overline{X}\\) está próximo de \\(\\mu\\)\nSe \\(p \\le \\alpha\\) –&gt; REJEITAMOS \\(H_0\\), \\(\\overline{X}\\) está distante de \\(\\mu\\). A assumimos \\(H_a\\) como verdadeira.\n\n\n\n\n\n\n\n\n\nFigura 2: Efeito do nível de significancia sobre a área de rejeição em um teste de hipótese."
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html#exemplificando-um-teste-de-hipóteses-o-teste-z",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html#exemplificando-um-teste-de-hipóteses-o-teste-z",
    "title": "Introdução ao teste de hipóteses",
    "section": "2 Exemplificando um teste de hipóteses: o teste \\(Z\\)",
    "text": "2 Exemplificando um teste de hipóteses: o teste \\(Z\\)\n\n\n\n\n\n\nDescrição do problema\n\n\n\nDigamos que o número de batimentos cardíacos por minuto de um adulto em repouso tenha média \\(\\mu = 65\\) e desvio padrão \\(\\sigma = 9\\). Você imagina que o sedentarismo altera o batimento médio de um adulto.\n\n\n\nHipóteses estatíticas:\n\n\\(H_0: \\mu = 65\\) batimentos por minuto\n\\(H_a: \\mu \\ne 65\\) batimentos por minuto\n\nLimite de rejeição: determinamos o nível de significância (\\(\\alpha\\)) do teste como \\(\\alpha = 0,05\\).\n\n\nIMPORTANTE: O nível de significância \\(\\alpha\\) deve ser determinado antes da tomada de dados.\n\n\nExperimento: selecionamos uma amostra aleatória selecionando ao acaso \\(n = 15\\) pessoas de hábito sedentário e medimos seus batimentos cardíacos. Os resultados são:\n\nAmostra: 65, 73, 56, 71, 69, 69, 68, 59, 73, 68, 69, 64, 67, 64, 66\nque nos dá uma média amostral de:\n\\(\\overline{X} = \\frac{\\sum{X_i}}{n} = \\frac{65+73+56+71+69+69+68+59+73+68+69+64+67+64+66}{15} = 66.73\\) batimentos por minuto;\ne um erro padrão de:\n\\(\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{9}{3.87} = 2.32\\)\n\nTeste de hipóteses: com estes resultados encontramos o valor correspondente de Z.\n\n\\(z = \\frac{\\overline{X} - \\mu}{\\sigma_{\\mu}} = \\frac{66.73 - 65}{2.32} = 0.75\\)\ne utilizando a Tabela Z , encontramos a probabilidade de obtermos valores tão ou mais extremos que \\(-0.75\\) e \\(+0.75\\).\n\n\n\n\n\n\n\n\nFigura 3: Área de rejeição para z = 0,75.\n\n\n\n\n\nCom isto, a probabilidade de encontarmos valores tão ou mais extermos que \\(\\overline{X} = 66.73\\) foi calculada em \\(0.227 + 0.227 =\\) 0.453.\nNeste exemplo, a estatística do teste foi \\(z = 0.75\\) o a probabilidade associada \\(p = 0.453\\).\n\n\n\n\n\n\nTeste Z no R\n\n\n\n\nX &lt;- c(65, 73, 56, 71, 69, 69, 68, 59, 73, 68, 69, 64, 67, 64, 66)\nXm &lt;- mean(X)\npnorm(q = Xm, mean = 65, sd = 9/sqrt(15), lower.tail = FALSE) * 2\n\n[1] 0.4557231"
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html#tomada-de-decisão-sobre-h_0-nível-de-significância",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html#tomada-de-decisão-sobre-h_0-nível-de-significância",
    "title": "Introdução ao teste de hipóteses",
    "section": "3 Tomada de decisão sobre \\(H_0\\): nível de significância",
    "text": "3 Tomada de decisão sobre \\(H_0\\): nível de significância\nNo exemplo anterior, obtivemos \\(p =\\) 0.453. Isto significa que:\n\nsendo \\(H_0\\) verdadeira, existe uma probabilidade igual a \\(0.453\\) de que a média de uma amostra com \\(n = 15\\) esteja tão ou mais distante de \\(\\mu = 65\\) como observado neste experimento.\n\nSe aceitarmos que esta probabilidade é alta, então não há motivo para buscar por outras explicações. Por outro lado, se concluirmos que esta probabilidade é baixa, estamos dizendo que resultado obtido é improvável segundo a hipótese nula. Neste caso, devemos recorrer à hipótese alternativa para explicar o fenômeno.\nPara decidir se a probabilidade obtida é alta ou baixa, devemos compará-la ao nível de significância \\(\\alpha\\) pré-estabelecido. \\(H_0\\) será aceita somente se a probabilidade encontrada for maior que \\(\\alpha\\). Por outro lado, se nossa probabilidade for menor ou igual a \\(\\alpha\\), considerarmos os resultados improváveis segundo a hipótese nula e rejeitamos \\(H_0\\) em favor de \\(H_a\\).\nUm nível crítico comumente utilizado é \\(\\alpha = 0.05\\). No exemplo acima a probabilidade foi de 0.453, um valor muito acima de \\(0.05\\). Dizemos portanto, que a média amostral \\(\\overline{X}\\) não está tão distante do \\(\\mu\\) a ponto de rejeitarmos \\(H_0\\).\n\nConcluimos que, neste exemplo, \\(\\overline{X} = 66.73\\) não nos fornece evidência suficiente para rejeitar \\(H_0\\)."
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html#erros-de-decisão-em-um-teste-de-hipóteses",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html#erros-de-decisão-em-um-teste-de-hipóteses",
    "title": "Introdução ao teste de hipóteses",
    "section": "4 Erros de decisão em um teste de hipóteses",
    "text": "4 Erros de decisão em um teste de hipóteses\nA interpretação da probabilidade final esta associada à situação em que \\(H_0\\) seja verdadeira.\nIsto nos leva perguntar: o que esperar caso \\(H_0\\) seja falsa?\nComo não sabemos de fato, de \\(H_0\\) é verdadeira ou não, a tomada de decisão sobre um resultado de um teste estatístico pode nos levar às seguintes situações:\n\n\n\nTabela 1: Erros de decisão em um teste de hipótestes\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\) Verdadeira\n\\(H_0\\) Falsa\n\n\n\n\n\\(H_0\\) é rejeitada\n\\(\\alpha\\) (\\(\\textbf{Erro Tipo I}\\))\nDecisão correta (\\(1-\\beta\\))\n\n\n\\(H_0\\) é aceita\nDecisão correta (\\(1-\\alpha\\))\n\\(\\beta\\) (\\(\\textbf{Erro Tipo II}\\))\n\n\n\n\n\n\nA Tabela 1 nos mostra os tipos de erros aos quais estamos sujeitos ao realizar um teste de hipótese. Podemos rejeitar \\(H_0\\), ainda que ela seja verdadeira. O nivel de significância adotado, estabele que a probabilidade disto acontecer é \\(\\alpha\\). Se rejeitarmos \\(H_0\\) quando ela é verdadeira, estaremos incorrendo em um erro de decisão que denominamos de Erro Tipo I. Consequentemente, temos uma probabilidade de \\(1 - \\alpha\\) de aceitar corretamente \\(H_0\\) quando ela é verdadeira. Estabelecer um \\(\\alpha = 0,05\\) nos garante que iremos incorrer no erro do tipo I em somente \\(5\\%\\) das vezes que o experimento for realizado.\nUm outra situação ocorre quando aceitamos erroneamente a hipótese nula que é falsa, incorrendo no Erro Tipo II. O erro do tipo II tem probabilidade \\(\\beta\\) de acontecer. O complementar desta probabilidade (\\(1-\\beta\\)) é denominado de Poder do Teste. Um teste poderoso é portanto, aquele que tem elevada probabilidade de rejeitar \\(H_0\\) quando ela é falsa.\nAs figuras abaixo representam as distribuições das médias amostrais e os erros do tipos I e II quando o \\(H_0\\) é verdadeira (\\(\\mu_a = \\mu\\)) e quando \\(H_0\\) é falsa (\\(\\mu_a &gt; \\mu\\)).\n\n\n\n\n\n\n\n\nFigura 4: Relação entre o erro tipo I (\\(\\alpha\\)) e o erro tipos II (\\(\\beta\\)), ao aceitar ou rejeitar \\(H_0\\).\n\n\n\n\n\nIdealmente em um teste estatístico, seria interessante reduzir ao máximo os erros do tipo I e II. Ao reduzirmos o erro do tipo I, diminuindo \\(\\alpha\\) teremos um teste mais rigoroso que raramente iria errar ao rejeitar um \\(H_0\\) verdadeiro (Figura A). Entretanto, este teste também raramente iria rejeitar \\(H_0\\) ainda que ele seja falso (Figura B). Consequentemente, ao diminuir o valor de \\(\\alpha\\) ficamos menos propensos a cometer o erro do tipo I, porém mais propensos a incorrer no erro tipo II, isto é, não rejeitar uma \\(H_0\\) falsa.\nDadas estas características, o único modo que reduzir os dois tipos de erros simultaneamente é aumentando o tamanho amostral \\(n\\) pois, neste caso, reduzimos o erro padrão (\\(\\sigma_{\\overline{X}}\\)) e consequentemente a sobreposição entre as duas curvas acima."
  },
  {
    "objectID": "conteudo/teste_hipoteses/intro_testehipot.html#estabelecendo-a-hipótese-alternativa-testes-bilaterais-vs-unilaterais",
    "href": "conteudo/teste_hipoteses/intro_testehipot.html#estabelecendo-a-hipótese-alternativa-testes-bilaterais-vs-unilaterais",
    "title": "Introdução ao teste de hipóteses",
    "section": "5 Estabelecendo a hipótese alternativa: testes bilaterais vs unilaterais",
    "text": "5 Estabelecendo a hipótese alternativa: testes bilaterais vs unilaterais\nA hipótese alternativa estabelece nossa expectativa para a explicação dos resultados de um experimento no caso de \\(H_0\\) ser falsa. Os testes que descrevemos acima são chamados testes bilaterais ou bicaudais. Isto significa que sendo \\(H_0\\) falsa, podemos esperar que a média populacional esteja tanto acima quanto abaixo de \\(\\mu\\). Existem situações, no entanto, para as quais já temos uma expectativa a priori com base no conhecimento prévio sobre o fenêmeno estudado.\nVoltemos ao exemplo sobre a frequência cardíaca. Sabemos que o sedentarismo, tende a elevar a frequência cardíaca em repouso. Deste modo, o problema poderia ser estabelecido da seguinte forma.\n\n\n\n\n\n\nDescrição do problema\n\n\n\nDigamos que o número de batimentos cardíacos por minuto de um adulto em repouso tenha média \\(\\mu = 65\\) e desvio padrão \\(\\sigma = 9\\). A literatura sugere que o sedentarismo aumenta o batimento médio de um adulto.\n\n\nO problema agora estabelece que no caso de rejeição de \\(H_0\\), a frequência cardíaca deveia ser maior que 65 batimentos por minuto. Deste modo teremos como hipóteses estatísticas:\n\nHipóteses estatíticas:\n\n\\(H_0: \\mu = 65\\) batimentos por minuto\n\\(H_a: \\mu \\gt 65\\) batimentos por minuto\nA mudança aqui está em \\(H_a\\) que estabelece que na hipótese de rejeição de \\(H_0\\), esperamos somente que a frequencia cardíaca aumente.\nEsta modificação na construção das hipóteses estatísticas tem implicação na definição do limite de rejeição.\n\nLimite de rejeição: se definimos \\(\\alpha = 0,05\\), e \\(H_a: \\mu \\gt 65\\), temos que a área de rejeição será expressa acima de 65 batimentos por minuto.\n\n\n\n\n\n\n\n\n\nFigura 5: Área de rejeição em um teste unilateral com \\(\\alpha = 0,05\\).\n\n\n\n\n\nNote portanto, que a diferença entre um teste bilateral e um teste unilateral está na definição dá área que expressa a zona de rejeição, \\(\\alpha\\) para \\(H_0\\). Nos teste bilaterais, a área de rejeição é distribuída acima e abaixo de \\(\\mu\\) (Figura 2), enquanto nos teste unilaterais, a área estará toda acima ou abaixo de \\(\\mu\\), a depender do que foi estabelecido em \\(H_a\\) (Figura 5).\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_variancia.html",
    "href": "conteudo/teste_hipoteses/teste_variancia.html",
    "title": "Comparando variâncias",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas no capítulo\n\n\n\n\n\n\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(patchwork)\nA mesma lógica para testar uma hipótese sobre a média populacional \\(\\mu\\) pode ser utilizada para testar uma hipótese sobre a variância populacional \\(\\sigma^2\\). Veja o exemplo a seguir.\nA Tabela 1 é proviniente de um estudo em que foi analizada a riqueza de espécies da macro-fauna da zona entre-marés em nove praias costa da Holanda. Neste exemplo vamos usar as praias 5 e 8.\nCódigo\nRIKZ &lt;- read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/RIKZ.csv')\n\nS &lt;- RIKZ |&gt; \n  subset(Beach == 5 | Beach == 8) |&gt; \n  select(Richness, Beach)\n\nS |&gt; \n  gt() |&gt; \n  cols_width(\n    Richness ~ px(150),\n    Beach ~ px(150)\n  )\n\n\n\n\nTabela 1: Riqueza da macro-fauna em duas praias da costa Holandesa. Dados completos no arquivo “RIKZ” do pacote “AED”.\n\n\n\n\n\n\n\n\n\nRichness\nBeach\n\n\n\n\n3\n5\n\n\n22\n5\n\n\n6\n5\n\n\n0\n5\n\n\n6\n5\n\n\n3\n8\n\n\n5\n8\n\n\n7\n8\n\n\n5\n8\n\n\n0\n8\nPodemos nos perguntar se em uma praia a riqueza de espécies varia mais que na outra, ou seja, se em uma das praias amostras difere uma das outras em maior grau que na outra praia. Neste caso, estamos interessados em testar as variâncias, não as médias. Para testarmos a homogeneidade de variâncias entre as praias podemos estabelecer as seguintes hipóteses:\n1. Estabelcer as hipóteses estatísticas\n\\(H_0: \\sigma^2_5 = \\sigma^2_8\\)\n\\(H_a: \\sigma^2_5 \\ne \\sigma^2_8\\)\nA hipótese nula \\(H_0\\) estabelece aqui que as variâncias populacionais na praia 1 (\\(\\sigma^2_5\\)) seja igual a variância populacional da praia 2 (\\(\\sigma^2_8\\)), enquento \\(H_a\\) estabelece que são diferentes.\n2. Definir o nível de significância\nNeste caso podemos estabelecer \\(\\alpha = 0,05\\) como de costume.\n3. Definir a estatística do teste\nExistem várias formas possíveis de testar a homogeneidade de variâncias, a mais simples é o teste de razão de variâncias que tem como estatística:\n\\[F = \\frac{s^2_{maior}}{s^2_{menor}}\\]\nem que \\(s^2_{maior}\\) é a maior variância amostral e \\(s^2_{menor}\\) é a menor variância amostral.\n4. Calcular a estatística do teste \\(F_{calc}\\)\nCódigo\nS_var = S |&gt; \n  group_by(Beach) |&gt; \n  summarise(Variancias = var(Richness),\n            n = n())\nsmax = max(S_var[,2])\nsmin = min(S_var[,2])\nn1 = as.numeric(S_var[1,3])\nn2 = as.numeric(S_var[2,3])\n\nFcalc = smax/smin\np = pf(Fcalc, df1 = n1-1, df2 = n2-1, lower.tail = FALSE)\nNeste exemplo, as variâncias amostrais são:\nPraia 5: \\(s^2_{1} = 72.8\\) e,\nPraia 8: \\(s^2_{2} = 7\\)\nO \\(F_{calc}\\) fica:\n\\[F_{calc} = \\frac{72.8}{7} = 10.4\\] mostrando de a variância na praia 5 é \\(10.4\\) vezes maior que na praia 8.\n5. Calcular o valor de p para a distribuição estatística apropriada\nNo teste de hipótese para uma média a distribuição estatística apropriada para a estatística \\(Z\\) era a distribuiução normal padronizada. No caso do teste de razão de variâncias, a distribuição apropriada é chamada de ditribuição \\(F\\), que tem um formato assimétrico à direita. Em nosso exemplo, o valor de \\(p = 0.022\\).\nFigura 1: Distribuição F de Fisher para 4 graus de liberdade no numerador e 4 graus de liberdade no denominador.\nO formato da distribuição F varia em função dos graus de liberdade do numerador e do denominador. Para este exemplo, os graus de liberdade do numerador e denominador são calculados como \\(gl = n-1\\). Como foram tomadas 5 amostras em cada uma das praias, \\(gl_{numerador} = gl_{denominador} = n-1 = 4\\).\nTomada de decisão sobre \\(H_0\\)\nAssumindo que o valor de \\(p = 0.022\\) é menor que o nível de significância adotado \\(\\alpha = 0,05\\), rejeitamos \\(H_0\\) e concluímos que a distribuição da riqueza de espécies na praia 5 é mais heterogênea que na praia 8."
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_variancia.html#teste-de-razão-de-variâncias-no-r",
    "href": "conteudo/teste_hipoteses/teste_variancia.html#teste-de-razão-de-variâncias-no-r",
    "title": "Comparando variâncias",
    "section": "1 Teste de razão de variâncias no R",
    "text": "1 Teste de razão de variâncias no R\nOs cálculos acima podem ser replicados no R com o comando var.test().\n\nvar.test(Richness ~ Beach, data = S, alternative = 'greater')\n\n\n    F test to compare two variances\n\ndata:  Richness by Beach\nF = 10.4, num df = 4, denom df = 4, p-value = 0.02173\nalternative hypothesis: true ratio of variances is greater than 1\n95 percent confidence interval:\n 1.627993      Inf\nsample estimates:\nratio of variances \n              10.4 \n\n\n\n\n\n\n\n\nDistribuição \\(F\\) para outros graus de liberdade\n\n\n\n\n\n\n\n\n\n\n\nFigura 2: Distribuição F de Fisher para diferentes combinações de graus de liberdade no numerador e denominador."
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html",
    "href": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html",
    "title": "Probabilidade condicional e independência",
    "section": "",
    "text": "Pacotes e funções utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(ggVennDiagram)\nsource(\"scripts/conditional_tree.r\")"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#probabilidade-condicional",
    "href": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#probabilidade-condicional",
    "title": "Probabilidade condicional e independência",
    "section": "1 Probabilidade Condicional",
    "text": "1 Probabilidade Condicional\nConsideremos o experimento “virar uma estrutura (folha ou galho) e contar o número de itens”:\n\\(\\Omega = \\{(F0), (F1), (F2), (F3), (F4), (F5), (F6), (G0), (G1), (G2), (G3), (G4)\\}\\)\nSejam definidos os eventos:\n\n\\(A\\): “virar uma folha”:\n\\[A = \\{\\text{(F0), (F1), (F2), (F3), (F4), (F5), (F6)}\\}.\\]\n\\(B\\): “obter 3 ou mais itens”:\n\\[B = \\{\\text{(F3), (F4), (F5), (F6), (G3), (G4)}\\}.\\]\n\nQue podem ser representados no diagrama de Venn:\n\n\n\n\n\n\n\n\n\nPodemos perguntar:\n\nConsiderando que tenha sido virada uma folha, qual a probabilidade de que tenham sido obtidos mais de 3 itens?\n\nAo informar que a estrutura era uma folha, sabemos que nem todos os eventos de \\(\\Omega\\) podem ter ocorrido. Neste exemplo, somente as 7 observações do evento e \\(A\\) consistem de uma folha.\nDestas, apenas 4 possuem mais de 3 itens, de modo a resposta à pergunta seria \\(\\frac{4}{7}\\). Este resultado é conhecido como probabilidade condicional, denotada pelo símbolo (\\(|\\)). Neste exemplo específico estamos perguntando:\n\nDado que \\(A\\) OCORREU, qual a probabilidade de que \\(B\\) ocorra? Simbolicamente, esta questão é escrita como \\(P(B|A)\\) e lida como probabilidade de \\(B\\) dado \\(A\\).\n\n\\[P(B|A) = \\frac{4}{7} = 0.57\\]\nEsta probabilidade condicional foi calculada pelo número de observações favoráveis à intersecção de \\(A\\) e \\(B\\) (\\(A \\cap B\\)) relativa ao número de observações do evento \\(A\\). Isto significa que ao sabermos parte dos resultados, o espaço amostral inicial foi reduzido, neste caso, ao espaço coincidente com \\(A\\).\nDeste modo, a probabilidade condicional pode ser expressacomo:\n\\[P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}\\]\nque neste exemplo será:\n\\[P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{4}{7} = 0.57\\]"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#representação-de-eventos-diagrama-de-árvore",
    "href": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#representação-de-eventos-diagrama-de-árvore",
    "title": "Probabilidade condicional e independência",
    "section": "2 Representação de eventos: diagrama de árvore",
    "text": "2 Representação de eventos: diagrama de árvore\nQuando lidamos com experimentos em etapas ou eventos sequenciais, um diagrama de árvore ajuda a visualizar cada estágio, indicando as probabilidades e as condicionais:\n\n\n\n\n\n\n\n\n\nNesse diagrama, cada ramo representa um cenário: por exemplo, ao ocorrer \\(A\\), \\(B\\) pode acontecer com \\(P(B \\mid A)\\), resultando na intersecção \\(A \\cap B\\). Assim, o diagrama possibilita mapear todos os cenários possíveis de maneira organizada."
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#eventos-independentes",
    "href": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#eventos-independentes",
    "title": "Probabilidade condicional e independência",
    "section": "3 Eventos independentes",
    "text": "3 Eventos independentes\nDois eventos \\(A\\) e \\(B\\) são independentes quando conhecer a ocorrência de um deles não altera a probabilidade do outro, ou seja, conhecer \\(A\\) não nos diz nada sobre a probabilidade de ocorrência de \\(B\\), de modo que \\(P(B) = P(B \\mid A)\\).\nNo experimento “virar uma estrutura e contar o número de itens”, temos por exemplo.\n\\(P(B) = 0.5\\)\ne que\n\\(P(B \\mid A) = 0.57\\)\nPortanto, ao sabermos que a estrutura virada foi uma folha, a probalidade de que tenham sido observados 3 ou mais items foi alterada.\nOs eventos \\(A\\) e \\(B\\) são portanto eventos dependentes em que \\(P(B) \\neq P(B \\mid A)\\).\n\n3.1 Exemplo de eventos independentes\nSuponha que foram investigadas 600 pessoas, classificadas por idade e local de origem. Nesse contexto temos os eventos:\n\n\\(A\\): ter até 20 anos; \\(\\overline{A}\\): ter mais de 20 anos.\n\n\\(B\\): ser da cidade; \\(\\overline{B}\\): ser de fora da cidade.\n\nConsidere a matriz:\n\n\n\n\n\n\n\n\nidade\nDa cidade\nDe fora da cidade\n\n\n\n\nAté 20\n30\n170\n\n\nMais de 20\n60\n340\n\n\n\n\n\n\n\nAs probabilidades são:\n\\(P(A) = \\frac{200}{600} = 0.33\\)\n\\(P(\\overline{A}) \\;=\\; \\frac{400}{600} = 0.67\\)\n\\(P(B) = \\frac{90}{600} = 0.15\\)\n\\(P(\\overline{B}) \\;=\\; \\frac{510}{600} = 0.85\\)\nSabendo, por exemplo, que a pessoa tem mais de 20 anos, a probabilidade de ela ser da cidade é:\n\\[P(B \\mid A) = \\frac{60}{400} = 0.15.\\]\nUma vez que \\(P(B) = P(B \\mid A)\\), então \\(A\\) e \\(B\\) são eventos independentes."
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#eventos-independentes-vs-mutuamente-exclusivos",
    "href": "conteudo/fundamentos_probabilidade/probabilidade_condicional.html#eventos-independentes-vs-mutuamente-exclusivos",
    "title": "Probabilidade condicional e independência",
    "section": "4 Eventos independentes vs mutuamente exclusivos",
    "text": "4 Eventos independentes vs mutuamente exclusivos\n\nDois eventos são mutuamente exclusivos quando \\(P(A \\cap B) = 0\\). Se ambos ocorrerem, excluem-se mutuamente. Logo, se \\(A\\) ocorre, \\(B\\) não pode ocorrer. Nesse caso, \\(P(B \\mid A) = 0\\), caracterizando dependência, pois a informação de \\(A\\) determina que \\(B\\) não ocorrerá.\nDois eventos são independentes se \\(P(A \\cap B) = P(A)\\times P(B)\\). Isso significa que conhecer \\(A\\) não altera a probabilidade de \\(B\\). Se \\(P(A)\\) e \\(P(B)\\) forem não nulos, então não podem ser ao mesmo tempo mutuamente exclusivos e independentes.\n\nA representação de eventos mutuamente exclusivos no diagrama de árvore é ilustrada por \\(P(B \\mid A)=0\\), pois, ao ocorrer \\(A\\), já se sabe que \\(B\\) não ocorrerá. Assim, eventos mutuamente exclusivos implica serem eventos dependentes. Se não há exclusividade, os eventos podem ou não ser independentes, dependendo de \\(P(B)\\) em relação a \\(P(B \\mid A)\\)."
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/espaco_amostral.html",
    "href": "conteudo/fundamentos_probabilidade/espaco_amostral.html",
    "title": "Espaço de possibilidades de um experimento",
    "section": "",
    "text": "Pacotes e funções utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nConsidere uma pesquisa para determinar os locais de ocorrência de uma espécie de peixe endêmica de riachos costeiros de Mata Atlântica no sudeste do Brasil. A pesquisa envolve amostrar trechos de riachos em diferentes bacias hidrográficas da região. Ao amostrar um determinado riacho, o pesquisador não sabe antecipadamente se irá ou não encontrar a espécie. Em probabilidade, chamamos esse ato de experimento aleatório, pois o resultado só é conhecido após a realização.\nEmbora não saibamos o resultado de um experimento específico, sabemos quais são os resultados possíveis. Neste exemplo, vamos assumir que existem apenas dois resultados para o ato de amostrar um riacho: ou a espécie ocorre, ou não ocorre.\nNo caso em questão:\n\\(\\Omega = {(ocorre), (não-ocorre)}\\)"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/espaco_amostral.html#probabilidades-de-um-evento",
    "href": "conteudo/fundamentos_probabilidade/espaco_amostral.html#probabilidades-de-um-evento",
    "title": "Espaço de possibilidades de um experimento",
    "section": "1 Probabilidades de um evento",
    "text": "1 Probabilidades de um evento\nMesmo sem saber o resultado de um experimento particular, podemos perguntar sobre a chance de cada evento ocorrer. Em termos probabilísticos, estamos interessados em \\(P(ocorre)\\). Quando \\(P(ocorre) = 0\\), significa que a espécie jamais ocorre nos riachos; quando \\(P(ocorre) = 1\\), significa que a espécie ocorre em todos os riachos. Na prática, a probabilidade ficará entre esses extremos: \\(0 \\le P(ocorre) \\le 1\\).\nPodemos estimar essa probabilidade empiricamente. Suponha que planejamos amostrar um determinado número de riachos, observando quantas vezes a espécie é capturada.\nDigamos que em certo dia foram amostrados 10 riachos e a espécie foi registrada em 4 deles. Nossa estimativa da probabilidade de ocorrência será:\n\\[P(ocorre) = \\frac{\\#ocorres}{\\#riachos} = \\frac{4}{10} = 0.4\\]\nNaturalmente, como os dois eventos no espaço amostral são \\((ocorre)\\) e \\((não-ocorre)\\), a probabilidade de não-ocorrência é:\n\\[P(não-ocorre) = 1 - \\frac{\\#não-ocorre}{\\#riachos} = 1 - \\frac{4}{10} = 0.6\\]\ne, sendo eventos mutuamente exclusivos e exaustivos (não podem ocorrer juntos e são as únicas possibilidades), temos:\n\\[P(ocorre) + P(não-ocorre) = 1 = P(\\Omega)\\]\nA probabilidade de não-ocorrência também é conhecida como complemento de \\(P(ocorre)\\), frequentemente denotado por \\(P(\\overline{ocorre})\\):\n\\[P(não-ocorre) = P(\\overline{ocorre})\\]\n\n1.1 Estimando probabilidades\nA estimativa acima descreve o resultado para um conjunto fixo de 10 riachos. No entanto, se continuarmos a amostrar novos riachos, essa estimativa pode variar, pois eventualmente encontraremos mais (ou menos) riachos com a espécie presente. Assim, com um número finito de observações, nossa estimativa não será exatamente igual à probabilidade real.\nSuponha que repetimos o experimento em 30 riachos. A cada nova amostra coletada, calculamos a fração acumulada de ocorrências:\n\n\n\n\n\n\n\n\nObservações\nOcorrência acumulada\nP(ocorre)\n\n\n\n\n1\n0\n0.0000000\n\n\n2\n0\n0.0000000\n\n\n3\n1\n0.3333333\n\n\n4\n1\n0.2500000\n\n\n5\n1\n0.2000000\n\n\n6\n2\n0.3333333\n\n\n7\n2\n0.2857143\n\n\n8\n3\n0.3750000\n\n\n9\n4\n0.4444444\n\n\n10\n4\n0.4000000\n\n\n11\n4\n0.3636364\n\n\n12\n4\n0.3333333\n\n\n13\n4\n0.3076923\n\n\n14\n4\n0.2857143\n\n\n15\n4\n0.2666667\n\n\n16\n5\n0.3125000\n\n\n17\n5\n0.2941176\n\n\n18\n5\n0.2777778\n\n\n19\n5\n0.2631579\n\n\n20\n5\n0.2500000\n\n\n21\n5\n0.2380952\n\n\n22\n5\n0.2272727\n\n\n23\n5\n0.2173913\n\n\n24\n5\n0.2083333\n\n\n25\n5\n0.2000000\n\n\n26\n6\n0.2307692\n\n\n27\n6\n0.2222222\n\n\n28\n6\n0.2142857\n\n\n29\n6\n0.2068966\n\n\n30\n7\n0.2333333\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote como a estimativa de \\(P(ocorre)\\) oscila. Espera-se que este valor gradualmente aproxime-se para probabilidade real à medida que o número de observações cresce.\n\n\n\n\n\n\nLei dos Grandes Números\n\n\n\nA Lei dos Grandes Números afirma que, à medida que o número de repetições de um experimento aleatório cresce, a frequência relativa de um evento tende a se aproximar da probabilidade real desse evento. Portanto, se continuarmos amostrando mais riachos, a proporção de vezes em que a espécie ocorre deve convergir para a probabilidade verdadeira de ocorrência."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html",
    "href": "conteudo/medidas_associacao/biquali.html",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "",
    "text": "Pacotes e funções utilizadas no capítulo\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(gridExtra)\nsource('scripts/assoc_municipies.r')\nImagine que haverá uma obra de revitalização de uma área na região central da cidade. A obra implicará na melhoria de acesso, de segurança e na oferta de serviços. Entretanto como levará tempo para ser concluída, haverá ações de remoção de moradias irregulares, interdição de ruas e avenidas por longos períodos, etc. A prefeitura encomenda uma pesquisa para saber a opinião dos munícipes. A cada entrevistado são feitas duas perguntas:\nA base de dados completa está disponível em datasets\nCom estas entrevistas desejamos responder à seguinte questão:\nImporte a base de dados\nmun = read_delim('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Entrevista_municipes.csv',\n                  delim = ',')\n\nn_amostra = 12\nn = nrow(mun)  # Número de entrevistados\nApós entrevistar 200 pessoas selecionadas ao acaso de uma lista da moradores da cidade, foi construída uma tabela com três colunas: Entrevistado (sequência numérica do primeiro ao último respondente), Opinião e Moradia.\nVeja os primeiros 12 resultados das entrevistas:\nTabela 1: Respostas dos primeiros munícipies entrevistados.\n\n\n\n\n\n\n\n\n\nEntrevistado\nOpiniao\nMoradia\n\n\n\n\n1\nA favor\nResidente\n\n\n2\nA favor\nResidente\n\n\n3\nA favor\nResidente\n\n\n4\nA favor\nResidente\n\n\n5\nContra\nNão-Residente\n\n\n6\nContra\nResidente\n\n\n7\nA favor\nNão-Residente\n\n\n8\nContra\nNão-Residente\n\n\n9\nA favor\nNão-Residente\n\n\n10\nA favor\nNão-Residente\n\n\n11\nA favor\nResidente\n\n\n12\nA favor\nNão-Residente\nEstão descritos na Tabela 1 os resultados das primeiras 12 entrevistas, onde é possível ver ao menos uma combinação de todas as possíveis respostas. O entrevistado pode ser:"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#tabelas-de-frequência-e-gráficos-de-barras",
    "href": "conteudo/medidas_associacao/biquali.html#tabelas-de-frequência-e-gráficos-de-barras",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "1 Tabelas de frequência e gráficos de barras",
    "text": "1 Tabelas de frequência e gráficos de barras\nInicialmente, vamos representar cada uma das variáveis por meio de uma tabela de frequência dos 200 entrevistados.\n\n\nCódigo\nresumo_opiniao = mun |&gt; \n  group_by(Opiniao) |&gt; \n  summarise(Op_n = n()) |&gt; \n  mutate(Op_rel = Op_n/sum(Op_n))\n\nresumo_opiniao |&gt; \n  gt()\n\n\n\n\n\n\n\n\nOpiniao\nOp_n\nOp_rel\n\n\n\n\nA favor\n144\n0.72\n\n\nContra\n56\n0.28\n\n\n\n\n\n\n\nDas \\(200\\) respostas tivemos \\(144\\) pessoas A favor (\\(72\\%\\)) e \\(56\\) pessoas Contra (\\(28\\%\\)).\n\n\nCódigo\nresumo_opiniao = mun |&gt; \n  group_by(Opiniao) |&gt; \n  summarise(Op_n = n()) |&gt; \n  mutate(Op_rel = Op_n/sum(Op_n))\n\nresumo_opiniao |&gt; \n  gt()\n\n\n\n\n\n\n\n\nOpiniao\nOp_n\nOp_rel\n\n\n\n\nA favor\n144\n0.72\n\n\nContra\n56\n0.28\n\n\n\n\n\n\n\nCom relação ao local de residência:\n\n\nCódigo\nresumo_morad = mun |&gt; \n  group_by(Moradia) |&gt; \n  summarise(Morad_n = n()) |&gt; \n  mutate(Morad_rel = Morad_n/sum(Morad_n))\n\nresumo_morad |&gt; \n  gt()\n\n\n\n\n\n\n\n\nMoradia\nMorad_n\nMorad_rel\n\n\n\n\nNão-Residente\n117\n0.585\n\n\nResidente\n83\n0.415\n\n\n\n\n\n\n\nResponderam à entrevistas um total de \\(117\\) pessoas Não-Residente (\\(58.5\\%\\)) e \\(83\\) pessoas Residente (\\(41.5\\%\\))\nSe visualizarmos estes totais em gráficos de barras individuais teremos:\n\n\nCódigo\nplt_op = ggplot(mun, aes(x = Opiniao)) +\n  geom_bar(fill = 'darkblue', color = 'white') +\n  coord_cartesian(ylim = c(0, 150)) +\n  labs(y = 'Número de respostas') +\n  theme_classic(base_size = 15)\n\nplt_morad = ggplot(mun, aes(x = Moradia)) +\n  geom_bar(fill = 'darkred', color = 'white') +\n  coord_cartesian(ylim = c(0, 150)) +\n  labs(y = 'Número de respostas') +\n  theme_classic(base_size = 15)\n\nplt_op + plt_morad\n\n\n\n\n\n\n\n\nFigura 1: Respostas dos munícipies entrevistados para cada questão separadamente.\n\n\n\n\n\nExiste portanto um predomínio de pessoas A Favor e um ligeiro predomínio de entrevistados Não-Residentes.\nPara responder à questão do capítulo, precisamos verificar se existe alguma associação entre as respostas dadas às duas perguntas explorando a distribuição conjunta dos totais respondidos."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#tabelas-de-contingência",
    "href": "conteudo/medidas_associacao/biquali.html#tabelas-de-contingência",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "2 Tabelas de contingência",
    "text": "2 Tabelas de contingência\nTabelas de contigência são organizadas para verificarmos a associação entre duas variáveis qualitativas. São conhecidas também como tabelas de dupla entrada. Nas colunas estão os níveis da variável \\(X\\) e nas linhas os níveis da variável \\(Y\\).\nPara nosso exemplo, podemos fazer simplesmente:\n\ntcont = table(mun$Opiniao, mun$Moradia)\ntcont\n\n         \n          Não-Residente Residente\n  A favor            81        63\n  Contra             36        20\n\n\nTemos:\n\n81 - A favor e Não-Residente;\n63 - A favor e Residente;\n36 - Contra e Não-Residente;\n20 - Contra e Residente\n\nPodemos ver os totais marginais das linhas:\n\ntcont_linhas = apply(tcont, 1, sum)\ntcont_linhas\n\nA favor  Contra \n    144      56 \n\n\nOu os totais marginais das colunas:\n\ntcont_colunas = apply(tcont, 2, sum)\ntcont_colunas\n\nNão-Residente     Residente \n          117            83 \n\n\nQue são justamente os totais que verificamos nas distribuições individuais.\nSe quisermos ver as frequências relativas marginais podemos fazer:\n\ntrel_linha = prop.table(tcont, 1)\ntrel_linha\n\n         \n          Não-Residente Residente\n  A favor     0.5625000 0.4375000\n  Contra      0.6428571 0.3571429\n\n\nNeste caso estamos vendo as frequências relativas das linhas, isto é, cada linha nesta tabela soma \\(1\\). O que vemos nesta tabela é que:\n\ndos \\(144\\) entrevistados que são A favor, cerca de \\(56.25\\%\\) são Não-Residente, enquanto os demais \\(43.75\\%\\) são Residente\ndos \\(56\\) entrevistados que são Contra, cerca de \\(64.29\\%\\) são Não-Residente, enquanto os demais \\(35.71\\%\\) são Residente\n\nPodemos fazer exatamente o mesmo olhando para as frequências marginais por colunas:\n\ntrel_coluna = prop.table(tcont, 2)\ntrel_coluna\n\n         \n          Não-Residente Residente\n  A favor     0.6923077 0.7590361\n  Contra      0.3076923 0.2409639\n\n\nNeste caso são as colunas que somam \\(1\\), portanto:\n\ndos \\(117\\) entrevistados que são Não-Residente, cerca de \\(69.23\\%\\) são A favor, enquanto os demais \\(30.77\\%\\) são Contra\ndos \\(83\\) entrevistados que são Residente, cerca de \\(75.9\\%\\) são A favor, enquanto os demais \\(75.9\\%\\) são Contra\n\nPodemos finalmente ver a frequência relativa conjunta:\n\ntrel_conjunta = prop.table(tcont)\ntrel_conjunta\n\n         \n          Não-Residente Residente\n  A favor         0.405     0.315\n  Contra          0.180     0.100\n\n\nEm que o somatório das linhas é igual a:\n\ntcont_linhas / sum(tcont_linhas)\n\nA favor  Contra \n   0.72    0.28 \n\n\nindicando os valores relativos das opiniões A Favor e Contra.\nO somatório das colunas é igual a:\n\ntcont_colunas / sum(tcont_colunas)\n\nNão-Residente     Residente \n        0.585         0.415 \n\n\nindicando os valores relativos de Não-Residentes e Residentes.\nNa tabela de frequência relativa conjunta, o somatório total da tabela deve ser igual a \\(1\\)."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#o-gráfico-de-barras-para-duas-variáveis-qualitativas",
    "href": "conteudo/medidas_associacao/biquali.html#o-gráfico-de-barras-para-duas-variáveis-qualitativas",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "3 O gráfico de barras para duas variáveis qualitativas",
    "text": "3 O gráfico de barras para duas variáveis qualitativas\nExistem várias formas de gerar um gráfico de barras combinando as duas variáveis. Se quisermos utilizar a própria tabela de contingência obtida a partir do comando table(mun$Opiniao, mun$Moradia), podemos utilizar o comando barplot(). Por outro lado, se quisermos utilizar a tabela original de dados (objeto mun) podemos fazer uso do pacote ggplot2:\n\n\nCódigo\nplt_bar1 = ggplot(mun) +\n  aes(x = Moradia, fill = Opiniao) +\n  geom_bar(color = 'white', position = 'dodge') +\n  scale_fill_manual(values = c('Contra' = 'darkred',\n                               'A favor' = 'darkblue')) +\n  coord_cartesian(ylim = c(0, 80)) +\n  labs(y = 'Número de respostas') +\n  theme_classic(base_size = 15)\n\nplt_bar1\n\n\n\n\n\n\n\n\nFigura 2: Respostas dos munícipies entrevistados combinando local de moradia e opinião.\n\n\n\n\n\nVeja que nesta figura, existem mais opiniões A favor, independente do entrevistado ser ou não residente da região central. Este padrão é o mesmo que observamos no gráfico da variável Opinião isoladamente, o que sugere não haver associação entre as variáveis Opinião e Moradia. Ao que parece, a opinião de um entrevistado sobre a construção da obra não depende de seu local de moradia.\n\n\n\n\n\n\nExemplos de associações entre duas variáveis\n\n\n\nAbaixo são apresentadas quatro situações em que existe associação Opinião e Moradia.\n\n\n\n\n\n\n\n\n\n\n\nEm todos estes exemplos, note que a relação as opiniões A favor ou Contra dependem se o entrevistado é ou não Residente na região. Esses padrões configuram diferentes tipos de associação entre as variáveis Opinião e Moradia, a saber:\n\nFigura A: Não-Residentes tendem a ser A favor e Residentes são em sua maioria Contra;\nFigura B: todos tendem a ser Contra, mas a diferença de opiniões é maior entre os Residentes;\nFigura C: Não-Residentes tendem a ser Contra, enquanto não parece haver diferenças entre os Residentes;\nFigura D: Residentes tendem a ser A favor, enquanto não parece haver diferenças entre os Residentes;"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#medindo-a-discrepância-com-o-índice-de-chi2-de-pearson",
    "href": "conteudo/medidas_associacao/biquali.html#medindo-a-discrepância-com-o-índice-de-chi2-de-pearson",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "4 Medindo a discrepância com o índice de \\(\\chi^2\\) de Pearson",
    "text": "4 Medindo a discrepância com o índice de \\(\\chi^2\\) de Pearson\nO índice de qui-quadrado (\\(\\chi^2\\)) mede a discrepância entre os valores observados e os valores esperados em uma tabela de contingência.\nDigamos que um município tenha \\(20\\%\\) de sua população morando em área Rural e os outros \\(80\\%\\) em área Urbana. Se fizermos uma amostragem ao acaso dos moradores é esperado que esta frequência relativa se reflita na amostra. Neste caso se sorteamos \\(200\\) pessoas, seria esperado:\n\nZona Rural: \\(40\\) moradores\nZona Urbana \\(160\\) moradores\n\nEntretando, se fazemos um sorteio ao acaso, haverá alguma variação ao redor destes valores, de modo que as frequências observadas (\\(o\\)) deverão ser diferentes das esperadas (\\(e\\)). O \\(\\chi^2\\) mede a discrepância entre \\(o\\) e \\(e\\) para cada célula de uma tabela de contigência como:\n\\[\\chi^2 = \\sum_{i=1}^{n}\\frac{(o_i - e_i)^2}{e_i}\\] Para uma tabela de frequências, devemos determinar portanto os valores de \\(o_i\\) e \\(e_i\\).\nSuponha que uma amostra de \\(200\\) moradores tenha resultado em:\n\n\nCódigo\nset.seed(10)\nnor = sum(rbinom(n = n, size = 1, prob = pr))\nMoradia_obs = data.frame(Regiao = c(rep('Rural', nor),\n                           rep('Urbana', n - nor)))\n\ntb_dfo = table(Moradia_obs)\ntb_dfo\n\n\nRegiao\n Rural Urbana \n    31    169 \n\n\nAs frequências observadas e esperadas serão:\n\nZona Rural:\n\n\\(o_{Rural} = 31\\)\n\\(e_{Rural} = 0.2 \\times 200 = 40\\)\n\nZona Urbana\n\n\\(o_{Urbana} = 169\\)\n\\(e_{Urbana} = 0.8 \\times 200 = 160\\)\nDe modo que o valor de \\(\\chi^2\\) será:\n\\(\\chi^2 = \\frac{(31 - 40)^2}{40} + \\frac{(169 - 160)^2}{160} = \\frac{(-9)^2}{40} + \\frac{(9)^2}{160} = 2.025 + 0.50625 = 2.53125\\)"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#o-índice-de-chi2-em-uma-tabela-de-contigência",
    "href": "conteudo/medidas_associacao/biquali.html#o-índice-de-chi2-em-uma-tabela-de-contigência",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "5 O índice de \\(\\chi^2\\) em uma tabela de contigência",
    "text": "5 O índice de \\(\\chi^2\\) em uma tabela de contigência\nNo exemplo acima, as contagens esperadas foram definidas a partir de um modelo que dizia que as populações rurais e urbanas se dividiam nas proporções \\(20\\%:80\\%\\). Em uma tabela de contigência, a hipótese em verificação é a de que não há associação entre \\(X\\) e \\(Y\\). Se for assim, é esperado que as frequências conjuntas sejam porporcionais às frequências marginais. Vamos apresentar esta ideia utilizando uma notação geral para tabelas de contingência e, em seguida, discutir com um exemplo.\nA tabela Tabela 2 apresenta \\(r\\) linhas por \\(s\\) colunas com as contagens de todas as combinações dos níveis da variável \\(X\\) (Níveis \\(A_{1}\\) a \\(A_{r}\\)) e da variável \\(Y\\) (Níveis \\(B_{1}\\) a \\(B_{s}\\)). Os totais marginais de \\(X\\) e \\(Y\\) são expressos respectivamente na última coluna e na última linha.\n\n\n\nTabela 2: Representação de uma tabela de contigência.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX ⟍ Y\n\\(B_{1}\\)\n\\(B_{2}\\)\n\\(\\cdots\\)\n\\(B_{j}\\)\n\\(\\cdots\\)\n\\(B_{s}\\)\nTotais em \\(X\\)\n\n\n\n\n\\(A_{1}\\)\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(\\cdots\\)\n\\(n_{1j}\\)\n\\(\\cdots\\)\n\\(n_{1s}\\)\n\\(n_{1.}\\)\n\n\n\\(A_{2}\\)\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(\\cdots\\)\n\\(n_{2j}\\)\n\\(\\cdots\\)\n\\(n_{2s}\\)\n\\(n_{2.}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(A_{i}\\)\n\\(n_{i1}\\)\n\\(n_{i2}\\)\n\\(\\cdots\\)\n\\(n_{ij}\\)\n\\(\\cdots\\)\n\\(n_{is}\\)\n\\(n_{i.}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(A_{r}\\)\n\\(n_{r1}\\)\n\\(n_{r2}\\)\n\\(\\cdots\\)\n\\(n_{rj}\\)\n\\(\\cdots\\)\n\\(n_{rs}\\)\n\\(n_{r.}\\)\n\n\nTotais em \\(Y\\)\n\\(n_{.1}\\)\n\\(n_{.2}\\)\n\\(\\cdots\\)\n\\(n_{.j}\\)\n\\(\\cdots\\)\n\\(n_{rs}\\)\n\\(n\\)\n\n\n\n\n\n\nSob a hipótese de não-associação entre \\(X\\) e \\(Y\\) teremos que:\n\\(\\frac{n_{i1}}{n_{.1}} = \\frac{n_{i2}}{n_{.2}} = \\cdots = \\frac{n_{is}}{n_{.s}} = \\frac{n_{i.}}{n}\\)\ne assim:\n\\(\\frac{n_{ij}}{n_{.j}} = \\frac{n_{i.}}{n}\\)\nDeste modo:\n\\(n_{ij}^{e} = \\frac{n_{i.} \\times n_{.j}}{n}\\)\n\n\n\n\n\n\nObservações\n\n\n\nA notação \\(n_{ij}^{e}\\) está sendo utilizada para denotar que a expressão acima determina a contagem de cada célula da tabela sob a hipótese de não associação e portanto, se refere ao valor esperado de \\(n_{ij}\\).\n\n\nTendo definido os valores esperados em uma tabela de contingência de \\(r \\times s\\), o \\(\\chi^2\\) é dado por:\n\\[\\chi^2 = \\sum_{i=1}^{r}\\sum_{j=1}^{s}\\frac{(n_{ij} - n_{ij}^{e})^2}{n_{ij}^{e}}\\]\nRetornando ao exemplo das entrevistas\nA tabela de contingência contendo os dados observados do início do capítulo pode ser escrita como:\n\n\n\nTabela 3: Resultados das entrevistas dos 200 munícipies.\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Residente\nResidente\nTotal Opinião\n\n\n\n\nA favor\n81\n63\n144\n\n\nContra\n36\n20\n56\n\n\nTotal Moradia\n117\n83\n200\n\n\n\n\n\n\nOs Valores esperados na linha \\(i\\) e coluna \\(j\\) são:\n\nLinha \\(1\\) - Coluna \\(1\\) (Não-Residente - A favor):\n\n\\(n_{ii}^{e} = \\frac{n_{1.} \\times n_{.1}}{n} =  \\frac{144 \\times 117}{200} = 84.24\\)\n\nLinha \\(1\\) - Coluna \\(2\\) (Residente - A favor):\n\n\\(n_{ii}^{e} = \\frac{n_{1.} \\times n_{.2}}{n} =  \\frac{144 \\times 83}{200} = 59.76\\)\n\nLinha \\(2\\) - Coluna \\(1\\) (Não-Residente - Contra):\n\n\\(n_{ii}^{e} = \\frac{n_{2.} \\times n_{.1}}{n} =  \\frac{56 \\times 117}{200} = 32.76\\)\n\nLinha \\(2\\) - Coluna \\(2\\) (Residente - Contra):\n\n\\(n_{ii}^{e} = \\frac{n_{2.} \\times n_{.2}}{n} =  \\frac{56 \\times 83}{200} = 23.24\\)\nDe modo que a tabela com os valores esperados será:\n\n\n\nTabela 4: Valores esperados na hipótese de não-associação entre Opiniao e locals de Moradia dos 200 munícipies entrevistados.\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Residente\nResidente\nTotal Opinião\n\n\n\n\nA favor\n84.24\n59.76\n144\n\n\nContra\n32.76\n23.24\n56\n\n\nTotal Moradia\n117\n83\n200\n\n\n\n\n\n\nFinalmente, o valor de \\(\\chi^2\\) pode ser calculado por:\n\\(\\chi^2 = \\frac{(81 - 84.24)^2}{84.24} + \\frac{(36 - 32.76)^2}{84.24} + \\frac{(63 - 59.76)^2}{84.24} + \\frac{(20 - 23.24)^2}{84.24} = 1.072\\)"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#valores-de-chi2-quando-existe-associação",
    "href": "conteudo/medidas_associacao/biquali.html#valores-de-chi2-quando-existe-associação",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "6 Valores de \\(\\chi^2\\) quando existe associação",
    "text": "6 Valores de \\(\\chi^2\\) quando existe associação\nO valor de \\(\\chi^2\\) será zero somente se os valores observados forem exatamente iguais aos valores esperados. Pequenas discrepâncias irão gerar valores de \\(\\chi^2\\) acima de zero, que se tornarão mais altos à medida que aumentam as diferenças entre \\(n_{ij}\\) e \\(n_{ij}^e\\).\n\n\n\n\n\n\nQuantificando as associações\n\n\n\nAbaixo estão diferentes exemplos em que existe associação entre Opinião e Moradia. Compare os valores e os gráficos abaixo aos que fizemos no exemplo do capítulo e veja que todos os valores de \\(\\chi^2\\) são mais elevados.\n\n\n\n\n\n\n\n\n\n\n\nTente aplicar a fórmula do \\(\\chi^2\\) para chegar aos resultados apresentados em cada exemplo."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#variações-do-índice-de-chi2",
    "href": "conteudo/medidas_associacao/biquali.html#variações-do-índice-de-chi2",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "7 Variações do índice de \\(\\chi^2\\)",
    "text": "7 Variações do índice de \\(\\chi^2\\)\nO valor de \\(\\chi^2\\) aumenta com o tamanho da amostra, o que torna difícil comparações entre diferentes estudos. Para corrigir este efeito existe o coeficiente de contigência de Pearson (\\(C\\)) que é baseado no resultado de \\(\\chi^2\\)\n\\[C = \\sqrt{\\frac{\\chi^2}{\\chi^2 + n}}\\]\nem que \\(n\\) é o tamanho da amostra.\nO valor máximo de \\(C\\) depende do número de linhas (\\(r\\)) e de colunas (\\(s\\)) na tabela de contingêcia. Podemos definir um coeficiente que esteja limitado entre \\(0\\) e \\(1\\):\n\\[T = \\sqrt{\\frac{\\frac{\\chi^2}{n}}{(r-1) \\times (s-1)}}\\] O valor \\(T = 0\\) ocorre quando não há associação (\\(\\chi^2 = 0\\)) e o valor máximo de \\(T = 1\\) só será atingido se houver associação e \\(r = s\\)"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquali.html#obtendo-o-índice-de-chi2-de-uma-tabela-de-dados",
    "href": "conteudo/medidas_associacao/biquali.html#obtendo-o-índice-de-chi2-de-uma-tabela-de-dados",
    "title": "Associação entre duas variáveis qualitativas",
    "section": "8 Obtendo o índice de \\(\\chi^2\\) de uma tabela de dados",
    "text": "8 Obtendo o índice de \\(\\chi^2\\) de uma tabela de dados\nA função para o cálculo do \\(\\chi^2\\) no R é chisq.test e pode ser utilizada a partir da tabela de contigência gerada pela função table:\n\ntcont = table(mun$Opiniao, mun$Moradia)\nchisq.test(tcont)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tcont\nX-squared = 0.76697, df = 1, p-value = 0.3812\n\n\nO resultado mostra o o valor de \\(\\chi^2\\) calculado (X-squared) e outras duas quantias denominadas de graus de liberdade (df) e valor de p (p-value), tópicos abordados em inferência estatística.\nNote que o resultado é diferente do que obtivemos neste capítulo. Isto ocorre pois, por padrão, a função utiliza a correção de Yates, em que \\(\\chi_{Yates}^{2}\\) é calculado por:\n\\[\\chi_{Yates}^{2} = \\sum_{i=1}^{r}\\sum_{j=1}^{s}\\frac{(|n_{ij} - n_{ij}^{e}| - 0,5)^2}{n_{ij}^{e}}\\]\nO termo \\(|n_{ij} - n_{ij}^{e}|\\) se refere ao módulo da distância entre os valores observados e calculados.\nSe quisermos obter exatamente os resultados descritos no exemplo deste capítulo, basta fazermos:\n\nchisq.test(tcont, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tcont\nX-squared = 1.0724, df = 1, p-value = 0.3004"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquanti.html",
    "href": "conteudo/medidas_associacao/biquanti.html",
    "title": "Associação entre duas variáveis quantitativas",
    "section": "",
    "text": "Pacotes e funções utilizadas no capítulo\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(mvtnorm)\nIremos medir o grau de associação entre duas variáveis quantitativas \\(X\\) e \\(Y\\) por meio dos coeficientes de covariância e correlação linear. Não estamos interessados em verificar se \\(Y\\) depende funcionalmente de \\(X\\) ou vice-versa. Estamos interessados somente em medir a intensidade de associação linear entre as duas variáveis. Ao calcularmos a covariância entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)), por exemplo, poderíamos inverter a ordem fazendo \\(s_{XY}\\) e teríamos exatamente os mesmo resultados. O mesmo vale para o coeficiente de correlação (\\(r_{YX} = r_{XY}\\)). Dizemos que existe uma simetria ao calcular estes coeficientes.\nEstamos interessados em diferenciar três situações que podem ser visualizadas nos gráficos de dispersão abaixo:"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquanti.html#covariância-entre-y-e-x",
    "href": "conteudo/medidas_associacao/biquanti.html#covariância-entre-y-e-x",
    "title": "Associação entre duas variáveis quantitativas",
    "section": "1 Covariância entre \\(Y\\) e \\(X\\)",
    "text": "1 Covariância entre \\(Y\\) e \\(X\\)\nA variância amostral de \\(Y\\) pode ser obtida subtraindo cada observação em \\(Y\\) de sua média (\\(\\overline{Y}\\)) e elevando esta subtração ao quadrado \\((Y_i - \\overline{Y})^2\\). Ao somar para todos os valores de \\(Y_i\\) teremos o somatório dos quadrados de \\(Y\\) (\\(SQ_Y\\)).\n\\[SQ_Y = \\sum_{i-1}^{n} (Y_i - \\overline{Y})^2 = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (Y_i - \\overline{Y})\\]\nDividindo \\(SQ_Y\\) por \\(n-1\\) teremos a variância amostral de \\(Y\\) (\\(s^2_Y\\)).\n\\[s^2_Y = \\frac{\\sum_{i-1}^{n} (Y_i - \\overline{Y})^2}{n-1}\\]\nA variância amostral é representada por \\(s^2\\). Aqui vamos usar a notação (\\(s^2_Y\\)), pois haverá outros estimadores de variância envolvidos, de modo que deveremos ser mais claros a respeito de qual estimador estaremos nos referindo.\nAdotando o mesmo procedimento para \\(X\\), podemos calcular o somatório dos quadrados de \\(X\\) (\\(SQ_X\\)).\n\\[SQ_X = \\sum_{i-1}^{n} (X_i - \\overline{X})^2 = \\sum_{i-1}^{n}(X_i - \\overline{X}) (X_i - \\overline{X})\\]\ne a variância amostral de \\(X\\) (\\(s^2_X\\)).\n\\[s^2_X = \\frac{\\sum_{i-1}^{n} (X_i - \\overline{X})^2}{n-1}\\]\nCombinando as duas ideias, teremos o produto cruzado de \\(Y\\) e \\(X\\) (\\(SQ_{YX}\\))\n\\[SQ_{YX} = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})\\]\ne finalmente a covariância amostral entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)).\n\n\n\n\n\n\nCovariância amostral\n\n\n\n\\[s_{YX} = \\frac{\\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})}{n-1}\\]"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquanti.html#coeficiente-de-correlação-linear-de-pearson-r",
    "href": "conteudo/medidas_associacao/biquanti.html#coeficiente-de-correlação-linear-de-pearson-r",
    "title": "Associação entre duas variáveis quantitativas",
    "section": "2 Coeficiente de correlação linear de Pearson \\(r\\)",
    "text": "2 Coeficiente de correlação linear de Pearson \\(r\\)\nAssim como a covariância, o coeficiente de correlação de Pearson (\\(r\\)) mede a intensidade da associação linear entre \\(Y\\) e \\(X\\). A covariância entretanto, não tem limite superior ou inferior, pois sua magnitude depende da ordem de grandeza das variáveis envolvidas. O coeficiente de correlação \\(r\\) é calculado como a covariância entre \\(Y\\) e \\(X\\) padronizada pelo produto dos desvios padrões de \\(Y\\) e de \\(X\\).\n\\[r = \\frac{s_{YX}}{s_Y s_X} = \\frac{\\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{n-1}} {\\sqrt{\\frac{\\sum{(Y_i - \\overline{Y})^2}}{n-1}}  \\times \\sqrt{\\frac{\\sum{(X_i - \\overline{X})^2}}{n-1}}}\\]\n\n\n\n\n\n\nCoeficiente de correlação\n\n\n\n\\[r = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sqrt{\\sum{(Y_i - \\overline{Y})^2 \\sum{(X_i - \\overline{X})^2}}}}\\]\n\n\nEsta padronização garante que \\(r\\) pode variar entre \\(-1\\) (correlação perfeitamente linear e negativa) e \\(+1\\) (correlação perfeitamente linear e positiva), se aproximando de zero quando não existe correlação."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquanti.html#exemplo",
    "href": "conteudo/medidas_associacao/biquanti.html#exemplo",
    "title": "Associação entre duas variáveis quantitativas",
    "section": "3 Exemplo",
    "text": "3 Exemplo\nA Tabela 1 apresenta dados da pesca do camarão tigre e do camarão rei entre nos anos de 1976 a 1987 (Haddon 2010). O camarão tigre constitui a espécie alvo da pesca, enquanto o camarão rei aparece como uma espécie acidental.\nImporte a base de dados ctigre_haddon.csv\n\ntigre = read_delim('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/ctigre_haddon.csv')\nrtk = cor(tigre$Tiger, tigre$King)\nstk = cov(tigre$Tiger, tigre$King)\n\n\n\n\n\nTabela 1: Captura do camarão tigre e do camarão rei (ton) entre 1976 e 1987.\n\n\n\n\n\n\n\n\n\nAno\nCamarão tigre\nCamarão rei\n\n\n\n\n1976\n566\n10\n\n\n1977\n1437\n22\n\n\n1978\n1646\n42\n\n\n1979\n2056\n33\n\n\n1980\n3171\n64\n\n\n1981\n2743\n34\n\n\n1982\n2838\n59\n\n\n1983\n4434\n146\n\n\n1984\n4149\n78\n\n\n1985\n3480\n75\n\n\n1986\n2375\n81\n\n\n1987\n3355\n52\n\n\n\n\n\n\n\n\n\n\nNas figuras abaixo temos as abundâncias das espécies ao longo dos anos e o gráfico de dispersão.\n\n\nCódigo\nc1 = ggplot(tigre, aes(x = Year)) +\n  geom_line(aes(y = Tiger), color = 'red') +\n  geom_point(aes(y = Tiger), color = 'red', \n             shape = 19, size = 4) +\n  geom_line(aes(y = King), color = 'blue') +\n  geom_point(aes(y = King), color = 'blue', \n             shape = 19, size = 4) +\n  geom_segment(x = 1976, xend = 1976.3, \n               y = 4000, yend = 4000, \n               color = 'red') +\n  geom_segment(x = 1976, xend = 1976.3, \n               y = 3700, yend = 3700, \n               color = 'blue') +\n  geom_text(x = 1976.4, y = 4000, \n            label = 'Camarão tigre', hjust = 0) +\n  geom_text(x = 1976.4, y = 3700, \n            label = 'Camarão rei', hjust = 0) +\n  scale_x_continuous(breaks = tigre$Year) +\n  labs(title = 'A', \n       y = 'Abundância (Ton)') +\n  theme_classic(base_size = 12)\n\nc2 = ggplot(tigre, aes(y = King, x = Tiger)) +\n  geom_point(shape = 19, size = 4) +\n  scale_y_continuous(breaks = seq(0, 150, by = 20)) +\n  scale_x_continuous(breaks = seq(500, 5000, by = 500)) +\n  labs(title = 'B',\n       x = 'Camarão tigre (Ton)', \n       y = 'Camarão rei  (Ton)') +\n  theme_classic(base_size = 12)\n\nc1 | c2\n\n\n\n\n\n\n\n\nFigura 1: A - Captura do camarão tigre e do camarão-rei (ton) entre 1976 e 1987. B - Associação positiva nas capturas anuais entre 1976 e 1987.\n\n\n\n\n\nA captura em toneladas do camarão tigre é sempre mais elevada. Entretanto, a figura da direita sugere haver uma associação linear entre as capturas. Nos anos em que houve maiores capturas do camarão tigre parece ter havido também um aumento nas capturas do camarão rei. Dizemos as capturas covariam positivamente. Portanto existe uma correlação positiva entre a captura das duas espécies.\nEm nenhum momento estamos dizendo que a captura de uma espécie resulta no aumento na captura da outra. Muito provavelmente, as abundâncias das duas espécies estão relacionadas a um terceiro fator que gera um comportamento similar na variação das capturas ano a ano. Estamos interessados em mensurar o grau de associação seja pela covariância ou pelo coeficiente de correlação de Pearson.\na covariância entre as abundâncias dos camarões tigre e rei é positiva (\\(s_{tigre-rei} = 3.3293\\times 10^{4}\\)) e consequentemente a correlação de Pearson também é positiva (\\(r = 0.82\\)). Confira os cálculos utilizando as expressões apresentadas no capítulo.\nNo R, a covariância entre \\(Y\\) e \\(X\\) pode ser obtida pela função cov:\n\ncov(tigre$Tiger, tigre$King)\n\n[1] 33293\n\n\nE a correlação pela função cor:\n\ncor(tigre$Tiger, tigre$King)\n\n[1] 0.8196913"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#conteúdo-da-aula",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#conteúdo-da-aula",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Conteúdo da Aula",
    "text": "Conteúdo da Aula\n\n\nIntrodução à Regressão Linear Simples\nDefinição dos Resíduos\nMétodo dos Mínimos Quadrados\nRepresentação Vetorial dos Resíduos\nGeometria da Solução de Mínimos Quadrados\nSolução Matricial do Método dos Mínimos Quadrados\nValores preditos\nSoma dos quadrados dos resíduos\nSoma dos quadrados dos totais\nCoeficiente de determinação"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#introdução-à-regressão-linear-simples",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#introdução-à-regressão-linear-simples",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Introdução à Regressão Linear Simples",
    "text": "Introdução à Regressão Linear Simples\nA regressão linear simples é um método para modelar a relação entre uma variável dependente \\(y\\) e uma variável independente \\(x\\). A equação da reta ajustada é dada por:\n\\[ \\hat{y} = \\beta_0 + \\beta_1 x \\]\n\n\n\n\n\n\nObservação\n\\(x_i\\)\n\\(y_i\\)\n\n\n\n\n\\(1\\)\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(2\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(3\\)\n\\(x_3\\)\n\\(y_3\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_n\\)\n\\(y_n\\)"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#definição-dos-resíduos",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#definição-dos-resíduos",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Definição dos Resíduos",
    "text": "Definição dos Resíduos\nNa figura abaixo, os resíduos \\(e_i\\) representam as diferenças entre os valores observados \\(y_i\\) e os valores ajustados \\(\\hat{y}_i\\) pela reta de regressão:\n\\[ e_i = y_i - (\\beta_0 + \\beta_1 x_i) \\]\nPortando na regressão linear, assume-se que o valor observado em \\(y_i\\) é dado por:\n\\[ y_i = \\beta_0 + \\beta_1 x_i + e_i\\]\n\n\n\n\n\n\nDica\n\n\n\n\nAcesse o link Regresão linear Geogebra\n\n\n\n\n\nObservação\n\\(x_i\\)\n\\(y_i\\)\n\n\n\n\n\\(1\\)\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(2\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(3\\)\n\\(x_3\\)\n\\(y_3\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_n\\)\n\\(y_n\\)"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#método-dos-mínimos-quadrados",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#método-dos-mínimos-quadrados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Método dos Mínimos Quadrados",
    "text": "Método dos Mínimos Quadrados\nO Método dos Mínimos Quadrados busca minimizar a soma dos quadrados dos resíduos:\n\\[ SQ_{res} = \\sum_{i=1}^{n} e_i^2 = e_1^2 + e_2^2 + \\cdots + e_n^2 \\]\nQue pode ser representada como:\n\\[\n\\begin{cases}\ne_1 = y_1 - (\\beta_0 + \\beta_1 x_1) \\\\\ne_2 = y_2 - (\\beta_0 + \\beta_1 x_2) \\\\\n\\vdots \\\\\ne_n = y_n - (\\beta_0 + \\beta_1 x_n) \\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#representação-vetorial-dos-resíduos",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#representação-vetorial-dos-resíduos",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Representação Vetorial dos Resíduos",
    "text": "Representação Vetorial dos Resíduos\nPodemos portanto representar os resíduos como vetor em que o vetor \\(\\vec{e}\\) é igual ao vetor \\(y\\) menos uma combinação linear dos vetores \\(\\vec{f}_0\\) e \\(\\vec{f}_0\\) com constantes \\(\\beta_0\\) e \\(\\beta_1\\).\n\n\\[ \\vec{e} = \\vec{y} - (\\beta_0 \\vec{f}_0 + \\beta_1 \\vec{f}_1) \\]\n\n\n\\[\n\\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\ny_1 - (\\beta_0 + \\beta_1 x_1) \\\\\ny_2 - (\\beta_0 + \\beta_1 x_2) \\\\\n\\vdots \\\\\ny_n - (\\beta_0 + \\beta_1 x_n) \\\\\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array} \\right]\n-\n\\left(\n\\beta_0\n\\left[ \\begin{array}{c}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{array} \\right]\n+\n\\beta_1\n\\left[ \\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{array} \\right]\n\\right)\n\\]\nOnde:\n\\[\\vec{e} =\n\\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right];\n\\vec{y} =\n\\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\n\\vec{f}_0 =\n\\left[ \\begin{array}{c}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{array} \\right];\n\\vec{f}_1 =\n\\left[ \\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{array} \\right]\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#geometria-da-solução-de-mínimos-quadrados",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#geometria-da-solução-de-mínimos-quadrados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Geometria da Solução de Mínimos Quadrados",
    "text": "Geometria da Solução de Mínimos Quadrados\nA Soma dos quadrados dos resíduos (\\(SQ_{res}\\)) pode ser obtida pela norma ao quadrado do vetor \\(\\vec{e}\\):\n\\[SQ_{res} = \\Vert\\vec{e}\\Vert^{2}=\\vec{e}\\cdot\\vec{e}=e_{1}^{2}+e_{2}^{2}+\\cdots+e_{n}^{2}\\]\n\n\n\n\n\n\n\n\nRepresentação da Solução do MMQ no GeoGebra\n\n\nO Método dos Mínimos Quadrados determina \\(\\beta_0\\) e \\(\\beta_1\\) de modo a minimizar o comprimento (a norma) do vetor \\(\\vec{e}\\) que pode ser obtida impondo que o vetor \\(\\vec{e}\\) seja ortogonal aos vetores \\(\\vec{f_0}\\) e \\(\\vec{f_1}\\), ou seja:\n\\[ \\vec{f_0} \\cdot \\vec{e} = 0 \\] \\[ \\vec{f_1} \\cdot \\vec{e} = 0 \\]\nLink para solução vetorial do MMQ"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#geometria-da-solução-de-mínimos-quadrados-1",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#geometria-da-solução-de-mínimos-quadrados-1",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Geometria da Solução de Mínimos Quadrados",
    "text": "Geometria da Solução de Mínimos Quadrados\n\\[\n\\left\\{\\begin{array} {c}\n\\vec{f_0} \\cdot \\vec{e} = 0 \\Leftrightarrow \\vec{f_0}\\cdot(\\vec{y}-\\beta_0\\vec{f_0}-\\beta_1\\vec{f_1})=0\\\\\n\\vec{f_1} \\cdot \\vec{e} = 0 \\Leftrightarrow \\vec{f_1}\\cdot(\\vec{y}-\\beta_0\\vec{f_0}-\\beta_1\\vec{f_1})=0\n\\end{array} \\right.\n\\] que é equivalente a: \\[\n\\left\\{\\begin{array} {c}\n\\beta_0\\vec{f_0}\\cdot\\vec{f_0}+\\beta_1\\vec{f_0}\\cdot\\vec{f_1}=\\vec{f_0}\\cdot\\vec{y}\\\\\n\\beta_0\\vec{f_1}\\cdot\\vec{f_0}+\\beta_1\\vec{f_1}\\cdot\\vec{f_1}=\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right.\n,\n\\] que ainda pode ser escrito na forma matricial: \\[\n\\left[ \\begin{array}{cc}\n\\vec{f_0}\\cdot\\vec{f_0} & \\vec{f_0}\\cdot\\vec{f_1}\\\\\n\\vec{f_1}\\cdot\\vec{f_0} & \\vec{f_1}\\cdot\\vec{f_1}\n\\end{array} \\right]\n\\left[ \\begin{array}{c}\n\\beta_0\\\\\n\\beta_1\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\n\\vec{f_0}\\cdot\\vec{y}\\\\\n\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#solução-matricial-do-método-dos-mínimos-quadrados",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#solução-matricial-do-método-dos-mínimos-quadrados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Solução Matricial do Método dos Mínimos Quadrados",
    "text": "Solução Matricial do Método dos Mínimos Quadrados\nA combinação linear:\n\\[\n\\left[ \\begin{array}{cc}\n\\vec{f_0}\\cdot\\vec{f_0} & \\vec{f_0}\\cdot\\vec{f_1}\\\\\n\\vec{f_1}\\cdot\\vec{f_0} & \\vec{f_1}\\cdot\\vec{f_1}\n\\end{array} \\right]\n\\left[ \\begin{array}{c}\n\\beta_0\\\\\n\\beta_1\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\n\\vec{f_0}\\cdot\\vec{y}\\\\\n\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right]\n\\]\npor ser expressa pelas matrizes:\n\\[X = \\left[ \\begin{array}{ccc}\n\\vec{f_0} & \\vdots & \\vec{f_1}\n\\end{array} \\right] =\n\\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right];\nY = \\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\nB = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n\\]\nE finalmente:\n\\[B = (X^{T} X)^{-1}(X^{T}Y)\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#calculando-os-valores-preditos-haty",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#calculando-os-valores-preditos-haty",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Calculando os valores preditos (\\(\\hat{y}\\))",
    "text": "Calculando os valores preditos (\\(\\hat{y}\\))\nDefinimos \\(\\mathbf{F}\\) como a matriz coluna que contém os valores preditos de \\(y\\) (denominados \\(\\hat{y}\\)), isto é, aquela que contém os pontos em \\(y\\) que se sobrepõem à reta da regressão linear. Podemos obter \\(\\mathbf{F}\\) por meio da operação matricial abaixo:\n\n\\[\\mathbf{F} = \\mathbf{X}\\mathbf{B}\\]\n\n\\[\\mathbf{F} = \\left[ \\begin{array}{c}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots & \\vdots \\\\\n\\hat{y}_n \\\\\n\\end{array} \\right]; \\mathbf{X} = \\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right]; \\mathbf{B} = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#vetor-de-resíduos-e",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#vetor-de-resíduos-e",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Vetor de resíduos (\\(e\\))",
    "text": "Vetor de resíduos (\\(e\\))\nFinalmente, o vetor de resíduos é obtido por:\n\n\\[e = \\mathbf{Y} - \\mathbf{F}\\]\n\nAgora temos todos os componentes da regressão linear estabelecida inicialmente:\n\\[ \\hat{y_i} = \\beta_0 + \\beta_1 x_i \\]\ne\n\\[ y_i = \\hat{y_i} + e_i \\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#soma-dos-quadrados-dos-resíduos-sq_res",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#soma-dos-quadrados-dos-resíduos-sq_res",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Soma dos quadrados dos resíduos (\\(SQ_{res}\\))",
    "text": "Soma dos quadrados dos resíduos (\\(SQ_{res}\\))\nA Soma dos quadrados dos resíduos foi definida pela expressão abaixo:\n\\[SQ_{res} = \\Vert\\vec{e}\\Vert^{2}=\\vec{e}\\cdot\\vec{e}=e_{1}^{2}+e_{2}^{2}+\\cdots+e_{n}^{2}\\]\nConsiderando \\(\\vec{e}\\) como a matriz coluna \\(\\mathbf{e}\\):\n\\[\\mathbf{e} = \\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right]\n\\]\nPodemos fazer:\n\n\\[SQ_{res} = \\mathbf{e}^\\top \\mathbf{e}\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#soma-dos-quadrados-totais-sq_tot",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#soma-dos-quadrados-totais-sq_tot",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Soma dos quadrados totais (\\(SQ_{tot}\\))",
    "text": "Soma dos quadrados totais (\\(SQ_{tot}\\))\n\\(SQ_{tot}\\) pode ser definido como:\n\n\\[SQ_{tot} = \\sum_{i}^{n}{(y_i - \\overline{y})^{2}} = (y_1 - \\overline{y})^{2} + (y_2 - \\overline{y})^{2} + \\cdots + (y_n - \\overline{y})^{2}\\]\nem que \\(\\overline{y}\\) é a média aritmética de \\(y\\)\n\n\n\nPodemos definir a matrix coluna \\(\\mathbf{D}\\)\n\\[\\mathbf{D} = \\left[ \\begin{array}{c}\n(y_1 - \\overline{y})^{2} \\\\\n(y_2 - \\overline{y})^{2} \\\\\n\\vdots\\\\\n(y_n - \\overline{y})^{2} \\\\\n\\end{array} \\right]\n\\]\n\nE obter \\(SQ_{tot}\\) por:\n\\[SQ_{tot} = \\mathbf{D}^\\top \\mathbf{D}\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#coeficiente-de-determinação-r2",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#coeficiente-de-determinação-r2",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Coeficiente de determinação (\\(R^2\\))",
    "text": "Coeficiente de determinação (\\(R^2\\))\nA qualidade do ajuste pode ser determinada pelo coeficiente de determinação (\\(R^2\\)), um índice que varia entre 0 e 1.\n\n\\[R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#coeficiente-de-determinação-r2-1",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#coeficiente-de-determinação-r2-1",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Coeficiente de determinação (\\(R^2\\))",
    "text": "Coeficiente de determinação (\\(R^2\\))"
  },
  {
    "objectID": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#método-dos-mínimos-quadrados-resumo-dos-passos",
    "href": "conteudo/regressao_linear/metodo-minimos-quadrados-apresentacao.html#método-dos-mínimos-quadrados-resumo-dos-passos",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "Método dos mínimos quadrados: Resumo dos passos",
    "text": "Método dos mínimos quadrados: Resumo dos passos\n\n\n\n\n\n\n\n\n\nResolução do MMQ\n\n\n\nDefinição das matrizes do sistema\n\n\\[X = \\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right];\nY = \\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\nB = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n\\]\n\nCálculo dos coeficientes\n\n\\[B = (X^{T} X)^{-1}(X^{T}Y)\\]\n\nValores preditos\n\n\\[\\mathbf{F} = \\mathbf{X}\\mathbf{B}\\]\n\nMatriz coluna de Resíduos\n\n\\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{F}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nQualidade do ajuste\n\n\n\nSoma dos quadrados dos resíduos\n\n\\[SQ_{res} = \\mathbf{e}^\\top \\mathbf{e}\\]\n\nSoma dos quadrados totais\n\n\\[SQ_{tot} = \\mathbf{D}^\\top \\mathbf{D}\\]\n\nCoeficiente de determinação\n\n\\[R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html",
    "title": "Regressão linear simples",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(knitr)\nUm modelo de regressão linear nos permite verificar se há uma relação funcional entre variáveis quantitativas. Nesta relação, uma variável é denominada dependente (ou variável resposta - \\(Y\\)) e as demais independentes (ou variáveis preditoras - \\(X\\)). Portanto, ao ajustar um modelo de regressão linear, estamos assumindo que existe uma relação estatística de dependencia de \\(Y\\) como função das variáveis preditoras em \\(X\\). No modelo de regressão linear simples temos somente uma variável preditora e sua relação funcional com \\(Y\\) é dada por:\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\nCódigo\nst &lt;- read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/HubbardBrook_wide.csv\") |&gt;\n    rename(FlowD = WS2_Flow_Defosrested, FlowR = WS3_Flow_reference, \n           RainD = WS2_precipitation, RainR = WS3_precipitation) |&gt;\n    mutate(FlowD = FlowD / 100, FlowR = FlowR / 100,\n           RainD = RainD / 10, RainR = RainR / 10) |&gt;\n    (\\(df) df[-31, ])()\nConsidere novamente os dados sobre pluviosidade anual e vazão em uma bacia hidrográfica americada, medidos entre os anos de 1958 e 1987 (disponível em: tiee.esa.org). Vamos avaliar a relação entre a vazão na bacia e os volumes de chuva.\nYear\nFlowD\nFlowR\nDiference\nRainD\nRainR\n\n\n\n\n1958\n645.15\n567.36\n7779\n1167.5\n1161.0\n\n\n1959\n1012.05\n918.23\n9382\n1482.6\n1479.1\n\n\n1960\n825.22\n752.06\n7316\n1321.3\n1325.3\n\n\n1961\n470.05\n436.25\n3380\n979.7\n978.9\n\n\n1962\n777.31\n699.29\n7802\n1232.2\n1230.6\n\n\n1963\n773.64\n662.58\n11106\n1138.6\n1151.7\n\n\n1964\n712.15\n630.45\n8170\n1175.4\n1175.2\n\n\n1965\n598.85\n546.69\n5216\n1115.2\n1120.6\n\n\n1966\n1189.34\n726.73\n46261\n1222.3\n1223.2\n\n\n1967\n1131.85\n780.76\n35109\n1315.1\n1296.8\n\n\n1968\n1056.54\n762.84\n29370\n1268.2\n1285.2\n\n\n1969\n1347.61\n998.68\n34893\n1368.5\n1403.5\n\n\n1970\n905.47\n697.53\n20794\n1184.1\n1201.5\n\n\n1971\n800.56\n676.19\n12437\n1164.2\n1173.4\n\n\n1972\n1005.90\n885.91\n11999\n1431.3\n1424.0\n\n\n1973\n1585.73\n1396.43\n18930\n1804.0\n1792.8\n\n\n1974\n998.20\n890.45\n10775\n1406.8\n1408.9\n\n\n1975\n1086.33\n939.52\n14681\n1422.4\n1448.6\n\n\n1976\n1142.59\n1022.06\n12053\n1511.4\n1516.0\n\n\n1977\n966.25\n843.75\n12250\n1382.7\n1388.2\n\n\n1978\n722.04\n613.79\n10825\n1087.9\n1085.7\n\n\n1979\n1136.17\n1036.93\n9924\n1417.0\n1432.7\n\n\n1980\n585.22\n548.28\n3694\n1087.9\n1101.1\n\n\n1981\n1129.09\n1093.91\n3518\n1631.5\n1664.9\n\n\n1982\n802.73\n756.12\n4661\n1088.2\n1114.4\n\n\n1983\n917.13\n889.35\n2778\n1436.6\n1451.8\n\n\n1984\n1000.54\n970.65\n2989\n1396.8\n1403.5\n\n\n1985\n634.76\n627.84\n692\n1128.4\n1137.2\n\n\n1986\n987.99\n960.94\n2705\n1364.0\n1372.3\n\n\n1987\n790.47\n797.09\n-662\n1222.1\n1234.6\nÉ razoável supor que em anos de mais chuva, seriam esperadas maiores vazões e que anos mais secos resultassem menores volumes de vazão. Para verificar esta suposição vamos fazer um gráfico de dispersão entre vazão e chuva.\nO gráfico sugere que a suposição faz sentido. Volumes baixos de chuva estão associados a volumes baixos de vazão e vice versa. O gráfico sugrere ainda que a relação funcional é linear. Nestas condições, faz sentido tentar modelar a relação entre estas variáveis por meio de um modelo de regressão linear simples.\nAo ajustar um modelo de regressão, vemos que a linha em azul é a que melhor descreve a relação linear entre as variáveis.\nEsta linha nos permite obter uma estimativa sobre a vazão esperada (\\(Y\\)) para qualquer dado volume de chuva (\\(X\\)). Neste exemplo, a equação que melhor associa vazão e chuva é:\n\\[Y_i = -571.98 + 1.05 X_i\\]\nO valor de \\(\\beta_1 = 1.05\\) nos diz que para um aumento de 1 mm/area/ano de chuva, a vazão aumentará 1.05 mm/area/ano. \\(\\beta_1\\) é conhecido como coeficiente de inclinação da reta e nos fornece magnitude da variação em \\(Y\\) para um aumento de 1 unidade em \\(X\\).\nEsta equação prevê por exemplo, que para um volume de chuva igual a 1400 mm/area/ano a vazão na bacia será de 898 mm/area/ano. Faça as contas para conferir.\n\\[898.02 = -571.98 + 1.05 \\times 1400\\]\nA reta descreve portanto os valores preditos de vazão para cada nível de chuva."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#modelo-geral-de-regressão",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#modelo-geral-de-regressão",
    "title": "Regressão linear simples",
    "section": "1 Modelo geral de regressão",
    "text": "1 Modelo geral de regressão\nA estrutura de um modelo de regressão é dada por:\n\\[Y_i = f(X_i, \\beta) + \\epsilon_i\\]\nonde \\(f(X_i, \\beta)\\) representa a parte determinística e \\(\\epsilon\\) a parte estocástica. O sulfixo i nos diz que esta expressão é dada para cada par de observação \\((Y,X)\\).\n\n1.1 Porção determinística\nA porção determinística é um modelo matemático que descreve a relação funcional entre \\(X\\) e \\(Y\\). Os parâmetros \\(\\beta\\)’s determinam a intensidade do efeito de \\(X\\) sobre \\(Y\\). Na regressão linear simples temos somente uma variável \\(X\\), e a relação funcional é dada pela equação da reta. No modelo de regressão linear múltipla existe mais de uma variável \\(X\\). Finalmente, nos modelos de regressão não-lineares a relação funcional pode ser representada por outros modelos matemáticos (ex. função potência \\(Y = \\beta_0X^{\\beta_1}\\)).\nNa regressão linear simples, o parâmetro \\(\\beta_1\\) é geralmente o de maior interesse. Este parâmetro nos dirá se a relação será crescente (\\(\\beta_1 &gt; 0\\)), decrescente (\\(\\beta_1 &lt; 0\\)) ou nula (\\(\\beta_1 = 0\\)). \\(\\beta_0\\) é o \\(\\textbf{intercepto}\\) e expressa o ponto em \\(Y\\) em que a reta cruza o eixo das ordenadas.\n\n\n\n\n\n\n\n\n\n\n\n1.2 Porção estocástica\nA porção estocástica, é representada pelo resíduo ou erro. A cada observação \\(Y_i\\) está associado um valor de resíduo correspondente (\\(\\epsilon_i\\)), dado pela distância vertical entre \\(Y_i\\) e o valor predito \\(\\hat{Y_i}\\) sobre a reta de regressão.\n\n\n\n\n\n\n\n\n\nNo modelo de regressão linear que veremos aqui, os resídos são uma variável aleatória prevenientes de uma distribuição normal de probabilidades com média \\(\\mu = 0\\) e variância \\(\\sigma^2\\) constante ao longo da reta de regressão, \\(N(0, \\sigma^2)\\).\n\n\n\n\n\n\nFigura 1: Resíduo normalmente distribuído ao longo da reta de regressão."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#ajuste-dos-dados-ao-modelo-de-regressão",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#ajuste-dos-dados-ao-modelo-de-regressão",
    "title": "Regressão linear simples",
    "section": "2 Ajuste dos dados ao modelo de regressão",
    "text": "2 Ajuste dos dados ao modelo de regressão\nO ajuste de dados observados a um modelo de regressão requer a obtenção de estimativas para \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma^2\\), denotadas respectivamente por \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). Note que o símbolo \\(\\hat{}\\) significa que estamos falando de estimativas obtidas a partir de dados amostrais e não dos parâmetros populacionais.\nAo obter estas estimativas, podemos encontrar valores ajustados de \\(Y\\) para um dados valor de \\(X\\). Os valores ajustados de \\(Y\\) são denotados por \\(\\hat{Y}\\).\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\]\n\n2.1 Método dos mínimos quadrados\nO Método dos Mínimos Quadrados (\\(MMQ\\)) é uma das formas disponíveis para calcularmos \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). O \\(MMQ\\) envolve encontrar a combinação de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) que minimiza a Soma dos Quadrados dos Resíduos (\\(SQ_{Resíduo}\\)), ou seja, que minimizam a quantia:\n\\[SQ_{Resíduo} = \\sum{(Y_i-\\hat{Y_ i})^2} = \\sum{(Y_i-(\\hat{\\beta_0} + \\hat{\\beta_1}X_i))^2}\\]\n\n\n\n\n\n\n\n\n\nNas figuras acima, a linha da esquerda (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) está claramente melhor ajustada à nuvem de pontos, o que se expressa em um menor somatório dos quadrados dos resíduos (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) quando comparado com o ajuste da figura à direita (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 145\\)).\n\n\n2.2 Variâncias, covariâncias e coeficientes da regressão\nPara estimarmos os coeficientes da regressão \\(\\beta_0\\) e \\(\\beta_1\\) devemos retomar o conceito de variância amostral e introduzir o conceito de covariância amostral.\nA variância amostral de \\(Y\\) por exemplo, pode ser obtida subtraindo cada observação em \\(Y\\) de sua média (\\(\\overline{Y}\\)) e elevando esta subtração ao quadrado \\((Y_i - \\overline{Y})^2\\). Ao somar para todos os valores de \\(Y_i\\) teremos o somatório dos quadrados de \\(Y\\) (\\(SQ_Y\\)).\n\\[SQ_Y = \\sum_{i-1}^{n} (Y_i - \\overline{Y})^2 = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (Y_i - \\overline{Y})\\]\nDividindo \\(SQ_Y\\) por \\(n-1\\) teremos a variância amostral de \\(Y\\) (\\(s^2_Y\\)).\n\\[s^2_Y = \\frac{\\sum_{i-1}^{n} (Y_i - \\overline{Y})^2}{n-1}\\]\nAdotando o mesmo procedimento para \\(X\\), podemos calcular o somatório dos quadrados de \\(X\\) (\\(SQ_X\\)).\n\\[SQ_X = \\sum_{i-1}^{n} (X_i - \\overline{X})^2 = \\sum_{i-1}^{n}(X_i - \\overline{X}) (X_i - \\overline{X})\\]\ne a variância amostral de \\(X\\) (\\(s^2_X\\)).\n\\[s^2_X = \\frac{\\sum_{i-1}^{n} (X_i - \\overline{X})^2}{n-1}\\]\nCombinando as duas ideias, teremos o produto cruzado de \\(Y\\) e \\(X\\) (\\(SQ_{YX}\\))\n\\[SQ_{YX} = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})\\]\ne a covariância amostral entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)).\n\\[s_{YX} = \\frac{\\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})}{n-1}\\]\nO estimador \\(\\hat{\\beta_1}\\) nada mais é que a covariância entre \\(Y\\) e \\(X\\) padronizada pela variância de \\(X\\).\n\\[\\hat{\\beta_1} = \\frac{s_{YX}}{s^2_X} = \\frac{\\frac{SQ_{XY}}{n-1}}{\\frac{SQ_X}{n-1}} = \\frac{SQ_{XY}}{SQ_X} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\]\n\\[\\hat{\\beta_1} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\]\nApós encontrar \\(\\hat{\\beta_1}\\), podemos calcular \\(\\hat{\\beta_0}\\) sabendo que a melhor reta de regressão passará necessariamente pelo ponto médio de \\(X\\) e de \\(Y\\). Deste modo temos:\n\\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X}\\]\nCalculados \\(\\hat{\\beta_1}\\) e \\(\\hat{\\beta_0}\\), podemos encontrar os valores ajustados de \\(Y\\) para cada valor de \\(X\\) que serão utilizados para construir a reta de regressão. \\(\\hat{Y_i}\\) será dado por:\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\]\nPor fim, a variância residual \\(s^2\\) é dada por:\n\\[s^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2}\\]\n\n\n2.3 Exemplo de ajuste ao modelo de regressão\n\nrk = read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/RIKZ.csv')\nrks = rk |&gt; \n  (\\(df) df[seq(3,43,by = 5),])()\n\nConsidere a tabela abaixo com os dados de riqueza da macro-fauna praial (número de espécies) e de um índice de exposição às ondas (NAP). Os dados foram obtidos em 2002 na costa da Holanda em nove praias (Zuur et al. 2009). Valores negativos de NAP se referem a locais mais expostos e valores positivos a locais menos expostos à ação das ondas.\n\nrks |&gt; \n  select(Richness, NAP) |&gt; \n  gt()\n\n\n\n\n\n\n\nRichness\nNAP\n\n\n\n\n13\n-1.336\n\n\n8\n0.635\n\n\n4\n-0.201\n\n\n3\n0.460\n\n\n6\n0.729\n\n\n1\n2.222\n\n\n1\n1.375\n\n\n7\n-1.005\n\n\n3\n-0.002\n\n\n\n\n\n\n\nO gráfico de dispersão sugere uma relação negativa e possivelmente linear, em que a riqueza de espécies diminui com o aumento no grau de exposição. Vamos ajustar um modelo de regressão a estes pontos calculando \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\).\n\n\n\n\n\n\n\n\n\nOs passos intermediários envolvem o cálculo do somatórios dos quadrados de X:\n\\[SQ_X = \\sum{(X_i - \\overline{X})^2}\\]\nde Y:\n\\[SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\]\ne do somatório dos produtos cruzados de X e Y:\n\\[SQ_{XY} = \\sum{(X_i - \\overline{X}) (Y_i - \\overline{Y})}\\]\nEstes passos são descritos na tabela a seguir.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRichness\nNAP\n\\((X_i - \\overline{X})\\)\n\\((Y_i - \\overline{Y})\\)\n\\((X_i - \\overline{X})^2\\)\n\\((Y_i - \\overline{Y})^2\\)\n\\((X_i - \\overline{X})(Y_i - \\overline{Y})\\)\n\n\n\n\n13\n-1.34\n-1.66\n7.89\n2.74\n62.23\n-13.06\n\n\n8\n0.64\n0.32\n2.89\n0.10\n8.35\n0.91\n\n\n4\n-0.20\n-0.52\n-1.11\n0.27\n1.23\n0.58\n\n\n3\n0.46\n0.14\n-2.11\n0.02\n4.46\n-0.30\n\n\n6\n0.73\n0.41\n0.89\n0.17\n0.79\n0.36\n\n\n1\n2.22\n1.90\n-4.11\n3.62\n16.90\n-7.82\n\n\n1\n1.38\n1.06\n-4.11\n1.11\n16.90\n-4.34\n\n\n7\n-1.00\n-1.32\n1.89\n1.75\n3.57\n-2.50\n\n\n3\n0.00\n-0.32\n-2.11\n0.10\n4.46\n0.68\n\n\n\n\n\nApós os cálculos, os valores estimados são:\n\\[\\hat{\\beta_1} = \\frac{\\sum{(X_i - \\overline{X})(Y_i - \\overline{Y})}}{\\sum{(X_i - \\overline{X})^2}} = \\frac{-25.49}{9.88} = -2.58\\]\n\\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X} = 5.11 -2.58 \\times 0.32 = 5.94\\]\n\\[\\hat{\\sigma}^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2} = \\frac{53.11}{7} = 7.59\\]\nDe modo que a melhor reta de regressão é dada por:\n\\[Richness = 5.94 -2.58 \\times NAP\\]"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#testes-de-hipóteses-na-regressão-linear-simples",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#testes-de-hipóteses-na-regressão-linear-simples",
    "title": "Regressão linear simples",
    "section": "3 Testes de hipóteses na regressão linear simples",
    "text": "3 Testes de hipóteses na regressão linear simples\nAté o momento, apresentamos uma discussão sobre o método para calcular os estimadores \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}\\). Entretanto, como nossas observações provêm de amostras, estas estimativas estão sujeitas à variação inerente às observações de que dispomos e certamente não serão iguais ao valor da população estatística. Devemos portanto, entender quais evidências estes estimadores nos fornecem para a existência de um efeito de \\(X\\) sobre \\(Y\\), ou seja, para rejeitarmos a hipótese nula em favor de \\(H_A\\).\n\n3.1 Teste sobre \\(\\beta_1\\)\nNa regressão linear simples, o efeito de \\(X\\) sobre \\(Y\\) depende do valor de \\(\\beta_1\\)\n\\(Y_i = \\beta + \\beta_1X_i + \\epsilon_i\\)\nA não existência de um efeito implica em \\(\\beta_1 = 0\\) e consequentemente:\n\\(Y_i = \\beta_0 + 0 \\times X_i + \\epsilon_i\\) \\(\\rightarrow\\) \\(Y = \\beta_0 + \\epsilon_i\\)\nPortanto, as hipóteses nula e alternativa seriam:\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\ne 0\\)\nSegundo \\(H_0\\), a inclinação da reta \\(populacional\\) não é diferente de zero e o valor estimado \\(\\hat{\\beta_1}\\) ocorreu puramente ao acaso, como efeito da variação amostral. Para testar esta hipótese, utilizamos a distribuição de t de modo que:\n\\[t = \\frac{\\hat{\\beta_1} - \\beta_1}{s_{\\hat{\\beta_1}}}\\]\nComo segundo \\(H_0\\), \\(\\beta_1  = 0\\) a expressão fica:\n\\[t = \\frac{\\hat{\\beta_1} - 0}{s_{\\hat{\\beta_1}}} = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}}\\]\n\\(s_{\\hat{\\beta_1}}\\) é o erro padrão de \\(\\beta_1\\) calculado por:\n\\[s_{\\hat{\\beta_1}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum{(X_i-\\overline{X})^2}}}\\]\nNo exemplo sobre a fauna praial estamos interessados em testar a hipótese de que a riqueza de espécies esteja associada ao grau de exposição às ondas. Em regressão linear, esta hipótese pode ser expressa por:\n\\[t = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}} = \\frac{-2.58}{0.88} = -2.944\\]\nQue na distribuição de t fica:\n\n\n\n\n\n\n\n\n\nSe nosso nível de significancia \\(\\alpha = 0.05\\), então a probabilidade \\(p = 0.011 + 0.011 = 0.022\\) indica que devemos rejeitar \\(H_0\\) e aceitar que existe uma relação entre Riqueza de espécies e NAP.\n\n\n3.2 Análise de variância da regressão\nComo já dizemos, a estrutura de um modelo de regressão é dada por um componente sistemático expresso como função de \\(X\\) (\\(\\beta_0 + \\beta_1X_i\\)) e um componente aleatório expresso pelos resíduos do modelo (\\(\\epsilon_i\\)). A variação total em \\(Y\\) no modelo de regressão portanto, pode ser atribuída a ambos os efeitos de \\(X\\) e do resíduo. Estas quantias de variação podem mensuradas pelos somatório dos quadrados abaixo.\nSoma dos quadrados totais:\n\\(SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\)\nSoma dos quadrados da regressão:\n\\(SQ_{Regressão}= \\sum{(\\hat{Y_i} - \\overline{Y})^2}\\)\nE soma dos quadrados do resíduo:\n\\(SQ_{Resíduo}= \\sum{(Y_i - \\hat{Y_i})^2}\\)\nPode-se mostrar ainda que vale a expressão:\n\\[SQ_Y = SQ_{Regressão} + SQ_{Resíduo}\\] A decomposição destas quantias é conhecida partição das somas dos quadrados e nos permitem comparar a influência de \\(X\\) com a influência do puro acaso sobre a variabilidade em \\(Y\\). Se todos os pontos estiverem perfeitamente sobre a reta, então toda a variação em \\(Y\\) seria atribuída à influência de \\(X\\). Por outro lado, à medida que aumenta a distância média dos pontos acima e abaixo da curva, aumenta a parcela atribuída ao acaso.\n\n\n\n\n\n\n\n\n\nEstes componentes de variação podem ser organizados em uma Tabela de Análise de Variância (ANOVA). \\(n\\) se refere ao número de amostras.\n\n\n\n\n\n\n\n\n\n\n\nFonte de variação\nSQ\ngl\nQM\nF\np\n\n\n\n\nRegressão\n\\(SQ_{Regressão}\\)\n\\(gl_{Regressão}\\)\n\\(QM_{Regressão} = \\frac{SQ_{Regressão}}{gl_{Regressão}}\\)\n\\(\\frac{QM_{Regressão}}{QM_{Resíduo}}\\)\nProbabilidade associada à cauda da distribuição F\n\n\nResíduo\n\\(SQ_{Resíduo}\\)\n\\(gl_{Resíduo}\\)\n\\(QM_{Resíduo} = \\frac{SQ_{Resíduo}}{gl_{Resíduo}}\\)\n\n\n\n\nTotal\n\\(SQ_{Y}\\)\n\\(gl_{Y}\\)\n\\(QM_{Y} = \\frac{SQ_{Y}}{gl_{Y}}\\)\n\n\n\n\n\nAs coluna \\(gl\\) se refer aos graus de liberdade nos modelo de regressão, a semelhança do que discutimos para o teste t de Student. A coluna QM (Quadrado médio) apresenta os estimadores de variância da regressão (\\(QM_{Regressão}\\)), do resíduo (\\(QM_{Resíduo}\\)) e total (\\(QM_{Y}\\)).\n\n3.2.1 A distribuição F\nO valor de \\(F\\) na tabela se refere a distribuição de probabilidade F. Esta distribuição de probabilidades é esperada para a razão entre duas variâncias amostrais. No caso da regressão linear, estas são a variância da regressão (\\(QM_{Regressão}\\) no numerador) e a variância residual (\\(QM_{Resíduo}\\) no denominador). Diferente da distribuiçao t, a distribuição F tem um formato assimétrico, sendo que o grau de assimetria depende dos graus de liberdade do numerador e do denominador. O valor de \\(p\\) na tabela se refere à área sob a distribuição F, acima do valor de \\(F\\) calculado. Na ANOVA da regressão, um valor de \\(p &lt; \\alpha\\) nos leva a rejeitar a hipótese nula e assumir que a variável \\(X\\) exerce algum efeito sobre \\(Y\\).\n\nO símbolo \\(F\\) foi dado em homenagem a Ronald Aylmer Fisher o estatístico e geneticista Britânico do início do séc. XX, que entre inúmeras outras contribuições, desenvolveu a Análise de Variância. Fisher é descrito como “a genius who almost single-handedly created the foundations for modern statistical science” (Halt 1998) e como “the single most important figure in 20th century statistics” (Efron 1998). Ver Ronald Aylmer Fisher.\n\n\n\n\n\n\n\n\n\n\nOs resultados da ANOVA para os dados da fauna praial nos dá os seguintes valores. Confira os cálculos.\n\n\n\n\n\nFonte de variação\nSQ\ngl\nQM\nF\np\n\n\n\n\nRegressão\n65.68\n1\n65.68\n8.64\n0.022\n\n\nResíduo\n53.21\n7\n7.60\nNA\nNA\n\n\nTotal\n118.89\n8\n14.86\nNA\nNA\n\n\n\n\n\nO valor de \\(p = 0.022\\) abaixo do nível de significância \\(\\alpha = 0.05\\), nos leva a rejeitar a hipótese nula em favor da alternativa, concluindo que o índice de exposição às ondas interfere sobre a riqueza da macro-fauna. O valor de \\(p\\) foi identico ao obtido no teste de hipóteses de \\(\\beta_1\\). No modelo de regressão linear simples isto é necessariamente verdadeiro, pois toda a variação associada à regressão é devida ao efeito do coeficiente \\(\\beta_1\\). Por outro lado, nos modelos de regressão múltipla, em que temos:\n\\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_1X_{i2} + \\cdots + \\beta_mX_{im} + \\epsilon_i\\]\nesta relação não é mais observada, pois existem múltiplos coeficientes agindo sobre a variação em \\(Y\\)."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#coeficiente-de-determinação-r2",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#coeficiente-de-determinação-r2",
    "title": "Regressão linear simples",
    "section": "4 Coeficiente de determinação \\(R^2\\)",
    "text": "4 Coeficiente de determinação \\(R^2\\)\nUma vez que toda a variação observada em Y pode ser alocada aos efeitos da reta de regressão e e do resíduo podemos fazer a seguinte questão:\n\nQual parcela da variação na Riqueza é explicada exclusivamente pelo modelo de regressão?\n\nEsta pergunta pode ser respondida calculando o que denominamos de coeficiente de determinção ou simplesmente \\(R^2\\):\n\\[R^2 = \\frac{SQ_{Regressão}}{SQ_Y} = 1 - \\frac{SQ_{Resíduo}}{SQ_Y}\\]\nO valor de \\(SQ_{Regressão}\\) mede a variação explicada exclusivamente pela regressão, \\(SQ_{Resíduo}\\) a variação residual e \\(SQ_Y\\) mede a variação total em \\(Y\\). Ao dividir \\(SQ_{Resíduo}\\) por \\(SQ_Y\\), o \\(r^2\\) nos informa sobre qual a fração da variação total é explicada somente pela reta de regressão.\nNpo exemplo da fauna praial:\n\\[R^2 = 1 - \\frac{53.11}{118.89} = 0.5533\\]\nO que significa que aproximadamente 55.33% da variação na riqueza é explicada pela variação no grau de exposição às ondas (NAP). Não sabemos a que se deve o restante da variação e, no contexto do modelo de regressão, assumimos ser uma variação aleatória inerente a cada observação (\\(\\epsilon_i\\)). Esta variação aleatória, como dito, segue uma distribuição normal com ponto central sobre a reta e variância data por \\(\\sigma^2\\). Este pressuposto é fundamental para a discussão do próximo ponto a respeito do intervalo de confiança de \\(Y\\)"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#intervalo-de-confiança-de-y",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#intervalo-de-confiança-de-y",
    "title": "Regressão linear simples",
    "section": "5 Intervalo de confiança de \\(Y\\)",
    "text": "5 Intervalo de confiança de \\(Y\\)\nComo nem todos os pontos caem perfeitamente sobre a reta, seria interessante que pudéssemos obter um intervalo de confiança de \\(Y\\) para um dado valor de \\(X\\). A amplitude deste intervalo irá depender da variância dos valores ajustados (\\(s^2_{Y|X}\\)) de \\(Y\\), calculada por:\n\\[s^2_{Y|X} = s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})\\]\ndo modo que:\n\\[s_{Y|X} = \\sqrt{s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})}\\]\nNote pela expressão acima que o \\(s_{Y|X}\\) diminui quanto:\n\na variância residual \\(s^2\\) diminui;\no tamanho amostral \\(n\\) aumenta.\no dado valor de \\(X_i\\) está próximo à média, pois neste caso \\((X_i-\\overline{X})\\) diminui.\n\nEncontrado \\(s_{Y|X}\\), o intervalo de confiança de \\(Y\\) é dado por:\n\\[IC_{Y} = \\hat{Y}\\pm t_{(\\alpha, n-2)} \\times s_{Y|X}\\]\nPara os dados da macrofauna, vamos exemplificar o cálculo de \\(IC_{95\\%}\\) para a \\(4^a\\) observação da tabela, em que Richness = 3 e NAP = 0.46.\nLembre-se que já estimamos anteriormente a variância residual destes dados (\\(s^2 = 7.59\\)). Como temos 9 observações, o valor de \\(t_{(\\alpha, n-2)} = 2.36\\), portanto:\n\\(s_{Y|X} = \\sqrt{s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})} = 0.93\\)\nO valor estimado de riqueza neste ponto é 4.75, portanto:\n\\(IC_{Y} = \\hat{Y} \\pm t_{(\\alpha, n-2)} \\times s_{Y|X} = 4.75 \\pm 2.36 \\times 0.93\\)\n\\(IC_{Y} = 4.75 \\pm 2.19\\)\n\\(IC_{Y_{limite superior}} = 6.94\\)\n\\(IC_{Y_{limite inferior}} = 2.56\\)\nPodemos calcular intervalos destes para todos os pontos observados como expresso na tabela abaixo.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRichness\nNAP\n\\(\\hat{Y}\\)\n\\(s_{Y \\mid X}\\)\n\\(IC_{inferior}\\)\n\\(IC_{superior}\\)\n\n\n\n\n13\n-1.34\n9.40\n1.72\n5.34\n13.46\n\n\n8\n0.64\n4.29\n0.96\n2.02\n6.56\n\n\n4\n-0.20\n6.46\n1.03\n4.04\n8.88\n\n\n3\n0.46\n4.75\n0.93\n2.56\n6.94\n\n\n6\n0.73\n4.06\n0.99\n1.73\n6.39\n\n\n1\n2.22\n0.21\n1.90\n-4.29\n4.71\n\n\n1\n1.38\n2.38\n1.30\n-0.70\n5.46\n\n\n7\n-1.00\n8.52\n1.48\n5.02\n12.02\n\n\n3\n0.00\n5.94\n0.96\n3.67\n8.21\n\n\n\n\n\nE representá-los graficamente, juntamente com os valores ajustados de Y.\n\n\n\n\n\n\n\n\n\nNote que na figura acima, estão representados os valores observados de riqueza de espécies (em preto), os valores ajustados (azul) e os intervalos a 95% (vermelho). Os valores ajustados são aqueles utilizados para construir a reta de regressão. O intervalo não costuma ser representados por pontos individuais, mas por uma banda que delimita a área que restringe o intervalo de confiança ao nível \\(1 - \\alpha\\) como na figura abaixo.\n\n\n\n\n\n\n\n\n\nA banda mais estreita próxima ao ponto médio de \\(X\\), reflete o ponto comentado anteriormente, de que quanto mais próximo ao centro da distibuição de pontos, mais confiança temos sobre os limites máximos e mínimos que um valor de \\(Y\\) pode assumir. Do mesmo modo, esta confiança diminui à medida que nos aproximamos dos extremos dos valores observados em \\(X\\)."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#pressupostos-da-regressão-linear-simples",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#pressupostos-da-regressão-linear-simples",
    "title": "Regressão linear simples",
    "section": "6 Pressupostos da regressão linear simples",
    "text": "6 Pressupostos da regressão linear simples\nAo realizar uma regressão linear simples, devemos assumir como verdadeiros alguns pressupostos.\n\nO modelo linear descreve adequadamente a relação funcional entre \\(X\\) e \\(Y\\);\nCada par de observação \\((X,Y)\\) é independente dos demais;\nA variável \\(X\\) é medida sem erros;\nOs resíduos têm distribuição normal, e;\nA variância residual \\(\\sigma^2\\) é constante ao longo dos valores de \\(X\\).\n\n\n6.1 Relação funcional linear\nCaso a relação funcional entre \\(X\\) e \\(Y\\) assuma uma forma diferente de \\(Y_i = \\beta_0 + \\beta_1X_i\\), o modelo de regressão não é mais válido, pois a estimativa de erro irá conter, além do componente aleatório residual, um componente sistemático. Este componente terá efeito sobre influência sobre a predição do modelo, sobretudo nos extremos das observações. Por modelo linear, entendemos aqueles em que os coeficientes \\(\\beta\\) aparecem de forma aditiva. Modelos em que os componentes aparecem de outro modo na equação como potência ou no denominador de uma equação são exemplos de modelos não-lineares. Abaixo estão dois exemplos de relações não-lineares comumente observadas em fenômenos ambientais:\nEquação potência: \\(Y_i = \\beta_0 X_i^{\\beta_1}\\)\nModelo de Michaelis-Menten: \\(Y_i = \\frac{\\beta_0 X_i}{\\beta_1 + X_i}\\)\n\n\n6.2 Independência\nA falta de independência pode ocorrer como resultado do delineamento amostral inapropriado para a questão em teste. A falta de independência torna crítico o uso de uma distribuição de probabilidade para o cálculo do intervalo de confiança (distribuição \\(t\\)) e para o teste de hipóteses (distribuições \\(t\\) e \\(F\\) ). Casos clássicos de falta de independência são aqueles em que as observações são denominadas como pseudoréplicas (Hurlbert 1984). Após a publicação clássica de Hurlbert, muito tem sido dito sobre pseudoreplicação. Em experimentos de campo, a falta de independência ocorre geralmente como resultados da proximidade espacial entre as réplicas ou sobre séries temporais.\n\n\n6.3 Variável \\(X\\) é medida sem erros\nVeja que a parcela residual do modelo de regressão se refere à distância vertical de \\(Y_i\\), para um dados valor de \\(X\\). Isto implica que os níveis de \\(X\\) são previamente definidos. Quando existe variabilidade aleatória tanto em \\(Y\\) quanto em \\(X\\), o modelo correto para a estimativa dos parâmetros da regressão é conhecido como Modelo II de regressão. Este pressuposto é frequêntemente ignorado em delineamentos de regressão, sobretudo em estudos observacionais, o que não parece ser particularmente problemático.\n\n\n6.4 Distribuição normal dos resíduos\nAssim como no pressuposto de independência, assumir que os resíduos têm uma distribuição normal permite o uso da distribuição \\(F\\) pra o teste de hipótese e da distribuiçãio \\(t\\) para o cálculo do intervalo de confiança. Uma distribuição de erros diferente da distribuição normal terá influência sobre o cálculo da amplitude do intervalo de confiança.\n\n\n6.5 Variância residual constante\nCaso, a variância \\(\\sigma\\) não seja constante ao longo da reta de regressão, o cálculo do intervalo de confiança e o resultado do teste de hipóteses são afetados. Uma vez diagnosticada uma variância não-constante existem modelos de regressão que podem ser apicados para incorporar este efeito em suas estimativas (Zuur et al. 2009)."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_simples.html#diagnósticos-da-regressão",
    "href": "conteudo/regressao_linear/regressao_linear_simples.html#diagnósticos-da-regressão",
    "title": "Regressão linear simples",
    "section": "7 Diagnósticos da regressão",
    "text": "7 Diagnósticos da regressão\nO diagnóstivo da regressão é composto por observações e testes que ajudam a decidirmos se a regressão linear foi um bom modelo para ajustar a um conjunto de dados particular. Um bom modelo neste contexto significa um modelo que atendeu aos pressuostos descritos acima. Esta verificação passa pela observação de padrões nos resíduos da regressão, ou seja, pela observação da parcela estocástica do modelo.\n\n7.1 Gráfico de resíduos\nO primeiro diagnóstico da regressão é conhecido como gráfico de resíduos, que consiste em um gráfico de dispersão entre os resíduos e o valor ajustado \\(\\hat{Y}\\). Abaixo estão os gráficos de resíduos que surge quando ajustamos uma reta a dados que apresentam uma relação linear, uma função potência, uma função assintótica e uma relação linear porém comm variância heterogênea.\n\n\n\n\n\n\n\n\n\nNas primeiras duas figuras, em que a relação é linear, vemos um padrão crescente de \\(Y\\) como função de \\(X\\) (figura da esquerda), em que os pontos estão aleatóriamente acima e abaixo da reta de regressão. Este padrão se reflete em um gráfico de resíduos (figura da direita) em que os pontos ficam aleatóriamente acima e abaixo de zero expressando resíduos positivos e negativos respectivamente. Em uma situação em que os pontos estivessem perfeitamente sobre a reta, os resíduos seriam todos iguais a zero e o gráfico de resíduos mostraria todos os pontos alinhados horizontalmente em zero.\nQuando a relação é potência e tentamos ajustar uma reta sobre, vemos que inicialmente os resíduos sao positivos, o seja, estão acima da reta. Os resíduos se tornam negativos no centro da nuvem de pontos e novamente positivos ao final do gráfico. Este padrão é mais evidente no gráfico de resíduos, que mostra um componente sistemático dos resíduos como fução do valor ajustado. Ao usar uma regressão linear neste caso, iríamos subestimar consistentemente os valores de Y nos extremos da figura e superestimlá-los no trecho central. Portanto, uma reta de regressão, quando ajustada a um conjunto de dados que expressa um padrão não-linear, não é capaz de isolar adequadamente as parcelas aleatórias e sistemáticas da relação entre \\(Y\\) e \\(X\\). Isto pode ser corrigido aplicando-se uma regressão não-linear aos dados.\nQuando a relação é assintótica, o resultado do ajuste foi inverso ao anterior. De fato, resultados análogos serão observados senpre que tentarmos ajustra uma regressão linear a dados que expressam padrões não-lineares.\nNo último exemplo (variância heterogênea) os pontos tendem a se afastar consistentemente da reta de regressão conforme aumentam os valores de \\(X\\). Isto denota que o pressuposto de variância \\(\\sigma^2\\) constante não é válido nesta relação. Isto pode ser corrigido aplicando-se um modelo de regressão linear com variância heterogênea.\n\n\n7.2 Histograma dos resíduos\nOutro diagnóstico da regressão consiste em fazer um histograma dos gráficos de resíduos. Um histograma, aproximadamente simétrico ao redor de zero o que sugere que o pressuposto de normalidade dos resíduos é válido neste caso. Existem testes formais de normalidade cmo o teste de Kolmogorov Smirnov ou o teste de Shapiro-Wilk."
  },
  {
    "objectID": "conteudo/manipulacao-dados-python/importa-dados-python.html#preparação-inicial",
    "href": "conteudo/manipulacao-dados-python/importa-dados-python.html#preparação-inicial",
    "title": "Importando data frames a partir de arquivos CSV",
    "section": "1 🛠️ Preparação Inicial",
    "text": "1 🛠️ Preparação Inicial\nVamos importar as bibliotecas necessárias:\n\nfrom google.colab import drive  # Permite que o Google Colab leia seu Google Drive\nimport pandas as pd  # Análise e manipulação de dados"
  },
  {
    "objectID": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-1-montando-o-google-drive-no-colab",
    "href": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-1-montando-o-google-drive-no-colab",
    "title": "Importando data frames a partir de arquivos CSV",
    "section": "2 🔗 Etapa 1: Montando o Google Drive no Colab",
    "text": "2 🔗 Etapa 1: Montando o Google Drive no Colab\nO primeiro passo é montar seu Google Drive, dentro do Google Colab. Isto pode ser feito com o comando drive.mount(). No exemplo abaixo, vamos pedir que as pasats de seu Google Dive sejam incluídas em uma estrutura de pastas dentro do caminho \"/content/drive\".\n\n# Montar o Google Drive (vai solicitar permissão)\ndrive.mount(\"/content/drive\")\n\nApós executar este comando, você verá um link de autorização de acesso ao Google Drive pelo Google Colab. Permita todos os acessos.\n\n2.1 ✅ Verificando os arquivos e pastas\nVocê agora será capaz de navegar pela estrutura de pastas do seu Drive. Por exemplo, você pode verificar se seu arquivo está realmente salvo com o comando !ls.\n\n# Listando arquivos e subpastas na pasta raiz do Drive\n!ls \"/content/drive/MyDrive/MyDrive/Projetos/Dados\""
  },
  {
    "objectID": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-2-localizando-seu-arquivo-csv",
    "href": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-2-localizando-seu-arquivo-csv",
    "title": "Importando data frames a partir de arquivos CSV",
    "section": "3 📁 Etapa 2: Localizando seu Arquivo CSV",
    "text": "3 📁 Etapa 2: Localizando seu Arquivo CSV\nCaso seu arquivo arquivo_exemplo.csv esteja listado na pasta acima, crie um objeto que conterá todo o caminho até o arquivo:\n\ncaminho_arquivo = \"/content/drive/MyDrive/Projetos/Dados/arquivo_exemplo.csv\""
  },
  {
    "objectID": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-3-lendo-o-csv-com-pandas",
    "href": "conteudo/manipulacao-dados-python/importa-dados-python.html#etapa-3-lendo-o-csv-com-pandas",
    "title": "Importando data frames a partir de arquivos CSV",
    "section": "4 📊 Etapa 3: Lendo o CSV com pandas",
    "text": "4 📊 Etapa 3: Lendo o CSV com pandas\nLeia o arquivo com a função read_csv() do pandas e salve-o em um data frame.\n\n# Lendo o CSV\ndf = pd.read_csv(caminho_arquivo)\n\n# Verique se os dados foram importados corretamente\nprint(df)\n\nPronto!! Após verificar se a importação foi feita corretamente, você pode utilizar o data frame no restante do código"
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "",
    "text": "Considere a situação em que temos 8 espécies de peixes nas descritas por 6 traços funcionais (Tabela 1). Traços funcionais são características morfológicas, fisiológicas ou comportamentais mensuráveis que influenciam diretamente o desempenho ecológico das espécies, determinando como elas interagem com o ambiente e utilizam recursos (como habitats, alimento e abrigo). Espera-se, por exemplo, que espécies similares em seus traços funcionais ocupem um espaço de nicho e respondam de forma similar a pressões ambientais. Nosso objetivo será quantificar o grau de similaridade entre os pares de espécies por meio do índice de similaridade por cossenos.\n\n\n\nTabela 1: Traços funcionais entre 8 espécies de peixes de riachos. CI: Índice de compressão; RD: Altura relativa; IVF: Índice de achatamento ventral; RAC: Área relativa da nadadeira caudal; REP: Posição relativa do olho; MO: orientação da boca. Bstr: Bryconamericus stramineus; Bsp: Bryconamericus sp.; Cfasc: Characidium fasciatum; Czeb: Characidium zebra; Cih: Cetopsorhamdia iheringi; Imin: Imparfinis minutus; Hsp: Hisonotus sp.; Hypsp: Hypostomus sp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraço\nBstr\nBsp\nCfasc\nCzeb\nCih\nImin\nHsp\nHypsp\n\n\n\n\nCI\n1.67\n1.71\n1.41\n1.47\n0.97\n0.69\n0.73\n0.66\n\n\nRD\n0.21\n0.26\n0.21\n0.22\n0.17\n0.12\n0.16\n0.19\n\n\nIVF\n0.57\n0.53\n0.51\n0.49\n0.59\n0.55\n0.39\n0.35\n\n\nRAC\n0.14\n0.13\n0.14\n0.11\n0.22\n0.26\n0.19\n0.3\n\n\nREP\n0.7\n0.68\n0.79\n0.8\n0.91\n0.76\n0.71\n0.85\n\n\nMO\n1.26\n1.74\n3.03\n1.98\n2.06\n2.24\n3.14\n3.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBryconamericus stramineus\n\n\n\n\n\n\n\nBryconamericus sp\n\n\n\n\n\n\n\n\n\nCharacidium fasciatum\n\n\n\n\n\n\n\nCharacidium zebra\n\n\n\n\n\n\n\n\n\nCetopsorhamdia iheringi\n\n\n\n\n\n\n\nImparfinis minutus\n\n\n\n\n\n\n\n\n\nHisonotus sp\n\n\n\n\n\n\n\nHypostomus sp\n\n\n\n\n\n\nFigura 1: Espécies da Tabela 1.\n\n\n\nCada espécie está representada em uma coluna, e as linhas correspondem às medidas morfológicas que podem ser associadas aos traços funcionais das espécies. Em notação matricial, podemos representar a Tabela 1 como:\n\\[\\mathbf{T} =\n\\begin{bmatrix}\n1.67 & 1.71 & \\dots & 0.66\\\\\n0.21 & 0.26 & \\dots & 0.19\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n1.26 & 1.74 & \\dots & 3.14\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html#matriz-de-traços-morfológicos",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html#matriz-de-traços-morfológicos",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "",
    "text": "Considere a situação em que temos 8 espécies de peixes nas descritas por 6 traços funcionais (Tabela 1). Traços funcionais são características morfológicas, fisiológicas ou comportamentais mensuráveis que influenciam diretamente o desempenho ecológico das espécies, determinando como elas interagem com o ambiente e utilizam recursos (como habitats, alimento e abrigo). Espera-se, por exemplo, que espécies similares em seus traços funcionais ocupem um espaço de nicho e respondam de forma similar a pressões ambientais. Nosso objetivo será quantificar o grau de similaridade entre os pares de espécies por meio do índice de similaridade por cossenos.\n\n\n\nTabela 1: Traços funcionais entre 8 espécies de peixes de riachos. CI: Índice de compressão; RD: Altura relativa; IVF: Índice de achatamento ventral; RAC: Área relativa da nadadeira caudal; REP: Posição relativa do olho; MO: orientação da boca. Bstr: Bryconamericus stramineus; Bsp: Bryconamericus sp.; Cfasc: Characidium fasciatum; Czeb: Characidium zebra; Cih: Cetopsorhamdia iheringi; Imin: Imparfinis minutus; Hsp: Hisonotus sp.; Hypsp: Hypostomus sp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraço\nBstr\nBsp\nCfasc\nCzeb\nCih\nImin\nHsp\nHypsp\n\n\n\n\nCI\n1.67\n1.71\n1.41\n1.47\n0.97\n0.69\n0.73\n0.66\n\n\nRD\n0.21\n0.26\n0.21\n0.22\n0.17\n0.12\n0.16\n0.19\n\n\nIVF\n0.57\n0.53\n0.51\n0.49\n0.59\n0.55\n0.39\n0.35\n\n\nRAC\n0.14\n0.13\n0.14\n0.11\n0.22\n0.26\n0.19\n0.3\n\n\nREP\n0.7\n0.68\n0.79\n0.8\n0.91\n0.76\n0.71\n0.85\n\n\nMO\n1.26\n1.74\n3.03\n1.98\n2.06\n2.24\n3.14\n3.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBryconamericus stramineus\n\n\n\n\n\n\n\nBryconamericus sp\n\n\n\n\n\n\n\n\n\nCharacidium fasciatum\n\n\n\n\n\n\n\nCharacidium zebra\n\n\n\n\n\n\n\n\n\nCetopsorhamdia iheringi\n\n\n\n\n\n\n\nImparfinis minutus\n\n\n\n\n\n\n\n\n\nHisonotus sp\n\n\n\n\n\n\n\nHypostomus sp\n\n\n\n\n\n\nFigura 1: Espécies da Tabela 1.\n\n\n\nCada espécie está representada em uma coluna, e as linhas correspondem às medidas morfológicas que podem ser associadas aos traços funcionais das espécies. Em notação matricial, podemos representar a Tabela 1 como:\n\\[\\mathbf{T} =\n\\begin{bmatrix}\n1.67 & 1.71 & \\dots & 0.66\\\\\n0.21 & 0.26 & \\dots & 0.19\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n1.26 & 1.74 & \\dots & 3.14\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html#calculando-similaridade-por-cossenos",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html#calculando-similaridade-por-cossenos",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "2 Calculando Similaridade por cossenos",
    "text": "2 Calculando Similaridade por cossenos\nCada espécie na Tabela 1 pode ser vista como um vetor \\(\\vec{v}\\) ou \\(\\vec{u}\\) com 6 entradas, uma para cada traço funcional. Assim, o cosseno do ângulo \\(\\theta\\) entre os vetores pode ser calculado por:\n\\[\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{u}}{\\|\\vec{v}\\| \\|\\vec{u}\\|}\n\\tag{1}\\]\nOnde:\n\n\\(\\vec{v} \\cdot \\vec{u}\\) é o produto escalar entre os vetores \\(\\vec{v}\\) e \\(\\vec{u}\\).\n\\(\\|\\vec{v}\\|\\) e \\(\\|\\vec{u}\\|\\) são as normas (comprimentos) dos vetores.\n\\(\\theta\\) é o ângulo entre os vetores no espaço multidimensional de 6 dimensões.\n\nO valor do \\(\\cos(\\theta)\\) funciona como um índice de similaridade cuja interpretação ecológica é direta:\n\n\\(\\cos(\\theta) \\approx 1\\) (ângulo próximo a 0°), indica espécies com alta similaridade funcional, compartilhando estratégias ecológicas semelhantes;\n\\(\\cos(\\theta) \\approx 0\\) (ângulo próximo a 90°) revela espécies ecologicamente distintas, com traços funcionais divergentes."
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html#exemplo-prático-similaridade-entre-bstr-e-bsp",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html#exemplo-prático-similaridade-entre-bstr-e-bsp",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "3 Exemplo Prático: Similaridade entre Bstr e Bsp",
    "text": "3 Exemplo Prático: Similaridade entre Bstr e Bsp\n\nVetores das espécies:\n\n\\[\n\\vec{v}_{\\text{Bstr}} = \\begin{bmatrix}\n1.67 \\\\ 0.21 \\\\ 0.57 \\\\ 0.14 \\\\ 0.7 \\\\ 1.26\n\\end{bmatrix}, \\quad\n\\vec{u}_{\\text{Bsp}} = \\begin{bmatrix}\n1.71 \\\\ 0.26 \\\\ 0.53 \\\\ 0.13 \\\\ 0.68 \\\\ 1.74\n\\end{bmatrix}\n\\]\n\nProduto Escalar:\n\n\\[\\vec{v} \\cdot \\vec{u} = (1.67 \\times 1.71) + (0.21 \\times 0.26) + \\dots + (1.26 \\times 1.74) = 5.899\\]\n\nNormas dos Vetores: \\[\\|\\vec{v}\\| = \\sqrt{1.67^2 + 0.21^2 + \\dots + 1.26^2} \\approx 2.2924\\] \\[\\|\\vec{u}\\| = \\sqrt{1.71^2 + 0.26^2 + \\dots + 1.74^2} \\approx 2.6037\\]\nCosseno do Ângulo: \\[\\cos(\\theta) = \\frac{5.899}{2.2924 \\times 2.6037} \\approx 0.988\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html#similaridade-por-cossenos-a-partir-de-operações-matriciais",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html#similaridade-por-cossenos-a-partir-de-operações-matriciais",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "4 Similaridade por Cossenos a partir de operações matriciais",
    "text": "4 Similaridade por Cossenos a partir de operações matriciais\nOs passos do item dois podem ser generalizados para todos os pares de espécies utilizando uma série de operações matriciais.\n\nObtenção da Matriz de Produtos Escalares (\\(\\mathbf{E}\\)): \\[\\mathbf{E} = \\mathbf{T}^\\top \\mathbf{T}\\]\n\n\\[\\mathbf{T^\\top} =\n\\begin{bmatrix}\n1.67 & 0.21 & \\dots & 1.26\\\\\n1.71 & 0.26 & \\dots & 1.74\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0.66 & 0.19 & \\dots & 3.14\n\\end{bmatrix}, \\quad\n\\mathbf{E} =\n\\begin{bmatrix}\ne_{11} & e_{12} & \\dots & e_{18}\\\\\ne_{21} & e_{22} & \\dots & e_{28}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ne_{81} & e_{82} & \\dots & e_{88}\n\\end{bmatrix}\n\\]\n\n\n\nObtenção da Matriz \\(\\mathbf{D}\\):\n\nAs normas dos vetores de espécies da matriz \\(\\mathbf{T}\\) podem ser obtidas a partir dos elementos da diagonal da matriz \\(\\mathbf{E}\\), em que:\n\\[\\text{norma}_i = \\sqrt{e_{ii}}\\]\nSabendo disso, obtenha a Matriz \\(\\mathbf{D}\\):\n\\[\\mathbf{D} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{e_{11}}} & 0 & \\dots & 0\\\\\n0 & \\frac{1}{\\sqrt{e_{22}}} & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\dots & \\frac{1}{\\sqrt{e_{88}}}\n\\end{bmatrix}\n\\]\n\n\nMatriz Final de similaridade por cossenos (\\(\\mathbf{C}\\)):\n\n\\[\\mathbf{C} = \\mathbf{D} \\mathbf{E} \\mathbf{D}\n\\tag{2}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/cossine-similarity.html#roteiro-matriz-de-similaridade-no-google-planilhas",
    "href": "conteudo/ecologia-numerica/cossine-similarity.html#roteiro-matriz-de-similaridade-no-google-planilhas",
    "title": "Ecologia Funcional: aplicação da álgebra matricial",
    "section": "5 Roteiro: Matriz de Similaridade no Google Planilhas",
    "text": "5 Roteiro: Matriz de Similaridade no Google Planilhas\n\nAcesse sheets.google.com.\nInsira os dados da tabela \\(\\mathbf{T}\\) (Tabela 1). Se necessário modifique o decimal de ponto (.) para vírgula (,).\nCalcule \\(\\mathbf{T}^\\top\\).\n\nDica - utilize a fórmula:\n=TRANSPOR()\n\nCalcule a matriz \\(\\mathbf{E}\\).\n\nDica - utilize a fórmula:\n=MATRIZ.MULT()\n\nCalcule as normas das colunas da matriz \\(\\mathbf{E}\\) e monte a matriz diagonal \\(\\mathbf{D}\\).\n\nDica: A matriz diagonal \\(\\mathbf{D}\\) terá as mesmas dimensões de \\(\\mathbf{E}\\), mas será preenchida com zeros exceto na diagonal principal. Nela, os valores serão \\(\\frac{1}{\\sqrt{e_{ii}}}\\), onde \\(e_{ii}\\) são os elementos da diagonal principal de \\(\\mathbf{E}\\).\n\nCalcule a matriz \\(\\mathbf{C}\\) conforme a Equação 2.\n\nDica - utilize a fórmula:\n=MATRIZ.MULT()\n\nVerificação: calcule o cosseno de \\(\\theta\\) entre algumas espécies utilizando a Equação 1 e verifique se os resultados coincidem com os observados na matriz de similaridade \\(\\mathbf{C}\\).\nVerificação: Considerando as imagens apresentadas na Figura 1, avalie criticamente se a matriz de similaridade representa de maneira fidedigna a semelhança morfométrica entre as espécies."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-prioris-e-posterioris",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-prioris-e-posterioris",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "O que aprendemos até aqui: prioris e posterioris",
    "text": "O que aprendemos até aqui: prioris e posterioris"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-distribuição-normal-de-probabilidade",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-distribuição-normal-de-probabilidade",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "O que aprendemos até aqui: distribuição Normal de Probabilidade",
    "text": "O que aprendemos até aqui: distribuição Normal de Probabilidade\n\\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2} \\left(\\frac{y - \\mu}{\\sigma} \\right)^2} \\longrightarrow \\quad y \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-o-modelo-de-regressão-linear",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-o-modelo-de-regressão-linear",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "O que aprendemos até aqui: o modelo de Regressão linear",
    "text": "O que aprendemos até aqui: o modelo de Regressão linear\n\n\n\nVariável aleatória resposta\n\n\\[\ny \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\n\\[\n\\mu = \\beta_0 + \\beta_1 x\n\\]\n\nPrioris\n\n\\[\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n\\]\n\\[\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n\\]\n\\[\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-programação-probabilística",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-programação-probabilística",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "O que aprendemos até aqui: Programação Probabilística",
    "text": "O que aprendemos até aqui: Programação Probabilística\n\n\n\nPyMC\n\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=60, sigma=5)\n    calcado = pm.Normal(\"calcado\", mu=2.8, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n    \n    # Definição do modelo\n    mu = beta_0 + beta_1 * X\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, \n                        observed=Y)\n    \n    # Amostra a distribuição posterior\n    resultados = pm.sample()\n\n\nBambi\n\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=60, sigma=5),\n    \"calcado\": bmb.Prior(\"Normal\", mu=2.8, sigma=0.1),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=10)\n}\n\n# Definição do modelo\nmodelo = bmb.Model(\"altura ~ calcado\", df, \n                    priors=custom_priors)\n\n# Amostra a distribuição posterior\nresultados = modelo.fit()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-ajuste-da-posteriori",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#o-que-aprendemos-até-aqui-ajuste-da-posteriori",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "O que aprendemos até aqui: ajuste da posteriori",
    "text": "O que aprendemos até aqui: ajuste da posteriori"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#daqui-para-frente-uma-variedade-de-modelos-e-estruturas",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#daqui-para-frente-uma-variedade-de-modelos-e-estruturas",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Daqui para frente: uma variedade de modelos e estruturas",
    "text": "Daqui para frente: uma variedade de modelos e estruturas\n\nExtenção da Regressão Linear para:\n\nMúltiplos preditores.\nDiferentes tipos de variáveis resposta (GLMs).\nDados com estrutura de agrupamento (Modelos Hierárquicos)."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-linear-múltipla",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-linear-múltipla",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Regressão linear múltipla",
    "text": "Regressão linear múltipla\n\n\n\nVariável aleatória resposta\n\n\\[\ny \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\n\\[\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n\\]\n\nPrioris\n\n\\[\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n\\]\n\\[\n\\beta_j \\sim \\mathcal{N}(\\mu_{\\beta_j}, \\sigma_{\\beta_j}) \\quad \\text{para } j = 1, \\dots, k\n\\]\n\\[\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Programação Probabilística",
    "text": "Programação Probabilística\n\n\n\nPyMC\n\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=60, sigma=5)\n    beta_1 = pm.Normal(\"beta_1\", mu=2.8, sigma=0.1)\n    beta_2 = pm.Normal(\"beta_2\", mu=1.5, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n\n    # Definição do modelo\n    mu = Intercept + beta_1 * X1 + beta_2 * X2\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, \n                       observed=Y)\n\n    # Amostra a distribuição posterior\n    resultados = pm.sample()\n\n\nBambi\n\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=60, sigma=5),\n    \"X1\": bmb.Prior(\"Normal\", mu=2.8, sigma=0.1),\n    \"X2\": bmb.Prior(\"Normal\", mu=1.5, sigma=0.1),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=10)\n}\n\n# Definição do modelo\nmodelo = bmb.Model(\"altura ~ X1 + X2\", df, \n                   priors=custom_priors)\n\n# Amostra a distribuição posterior\nresultados = modelo.fit()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-de-poisson-dados-de-contagem",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-de-poisson-dados-de-contagem",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Regressão de Poisson: dados de contagem",
    "text": "Regressão de Poisson: dados de contagem\n\n\n\\[\ny \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\n\\log(\\lambda) = \\mu = \\beta_0 + \\beta_1 x\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-de-poisson-dados-de-contagem-1",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-de-poisson-dados-de-contagem-1",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Regressão de Poisson: dados de contagem",
    "text": "Regressão de Poisson: dados de contagem\n\n\n\\[\nf(y) = \\frac{e^{-\\lambda} \\lambda^y}{y!} \\longrightarrow \\quad y \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\n\\log(\\lambda) = \\mu\n\\]\n\n\nVariável aleatória resposta\n\n\\[\ny \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\n\\log(\\lambda) = \\mu = \\beta_0 + \\beta_1 x\n\\]\n\nPrioris\n\n\\[\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n\\]\n\\[\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística-1",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística-1",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Programação Probabilística",
    "text": "Programação Probabilística\n\n\n\nPyMC\n\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=0, sigma=5)\n    beta = pm.Normal(\"beta\", mu=0, sigma=2)\n    \n    # Função de ligação log: log(λ) = μ = Intercept + beta * X\n    mu = Intercept + beta * X\n    lambda_ = pm.math.exp(mu)\n    \n    # Modelo de verossimilhança\n    contagem = pm.Poisson(\"contagem\", mu=lambda_, \n                          observed=Y)\n    \n    # Amostragem\n    resultados = pm.sample()\n\n\nBambi\n\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=0, sigma=5),\n    \"x\": bmb.Prior(\"Normal\", mu=0, sigma=2),\n}\n\n# Modelo com função de ligação log (default da família Poisson)\nmodelo = bmb.Model(\"contagem ~ x\", df, \n                   family=\"poisson\", priors=custom_priors)\n\n# Amostragem\nresultados = modelo.fit()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-logística-dados-dicotômicos",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-logística-dados-dicotômicos",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Regressão Logística: dados dicotômicos",
    "text": "Regressão Logística: dados dicotômicos\n\n\n\\[\ny \\sim \\text{Bernoulli}(p)\n\\]\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\mu = \\beta_0 + \\beta_1 x\n\\]\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-logística-dados-dicotômicos-1",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#regressão-logística-dados-dicotômicos-1",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Regressão Logística: dados dicotômicos",
    "text": "Regressão Logística: dados dicotômicos\n\n\n\nDistribuição da variável resposta\n\n\\[\nf(y) = p^y (1 - p)^{1 - y}\n\\]\n\\[\ny \\sim \\text{Bernoulli}(p)\n\\]\n\nFunção de ligação\n\n\\[\n\\text{logit}(p) = \\mu = \\beta_0 + \\beta_1 x\n\\]\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x\n\\]\n\n\\[\n\\frac{p}{1 - p} = e^{\\beta_0 + \\beta_1 x}\n\\]\n\\[\np = (1 - p) \\cdot e^{\\beta_0 + \\beta_1 x}\n\\]\n\\[\np = e^{\\beta_0 + \\beta_1 x} - p \\cdot e^{\\beta_0 + \\beta_1 x}\n\\]\n\\[\np \\left(1 + e^{\\beta_0 + \\beta_1 x}\\right) = e^{\\beta_0 + \\beta_1 x}\n\\]\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística-2",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#programação-probabilística-2",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Programação Probabilística",
    "text": "Programação Probabilística\n\n\n\nPyMC\n\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=0, sigma=5)\n    beta = pm.Normal(\"beta\", mu=0, sigma=2)\n    \n    # Preditor linear e função de ligação logit\n    mu = Intercept + beta * X\n    p = pm.math.sigmoid(mu)\n    \n    # Verossimilhança\n    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=Y)\n    \n    # Amostragem\n    resultados = pm.sample()\n\n\nBambi\n\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=0, sigma=5),\n    \"x\": bmb.Prior(\"Normal\", mu=0, sigma=2),\n}\n\n# Modelo logístico (ligação logit é padrão para bernoulli)\nmodelo = bmb.Model(\"y ~ x\", df, \n                   family=\"bernoulli\", priors=custom_priors)\n\n# Amostragem\nresultados = modelo.fit()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#modelo-hierárquico-normal-com-intercepto-e-inclinação-variaveis",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#modelo-hierárquico-normal-com-intercepto-e-inclinação-variaveis",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Modelo Hierárquico Normal com Intercepto e Inclinação Variáveis",
    "text": "Modelo Hierárquico Normal com Intercepto e Inclinação Variáveis\n\n\n\\[\ny_{ij} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma^2)\n\\]\n\\[\n\\mu_{ij} = \\beta_{0j} + \\beta_{1j} x_{ij}\n\\]\n\\[\n\\beta_{0j} \\sim \\mathcal{N}(\\gamma_0, \\tau_0^2) \\\\\n\\beta_{1j} \\sim \\mathcal{N}(\\gamma_1, \\tau_1^2)\n\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#variação-entre-grupos-coeficientes-do-modelo-hierárquico",
    "href": "conteudo/modelos_regressao_bayes/regressao-glm-hierarquico-apresentacao.html#variação-entre-grupos-coeficientes-do-modelo-hierárquico",
    "title": "Explorando Modelos de Regressão Bayesiana",
    "section": "Variação entre grupos: coeficientes do modelo hierárquico",
    "text": "Variação entre grupos: coeficientes do modelo hierárquico\n\n\nCoeficientes específicos por grupo:\n\\[\n\\beta_{0j} \\sim \\mathcal{N}(\\gamma_0, \\tau_0^2) \\\\\n\\beta_{1j} \\sim \\mathcal{N}(\\gamma_1, \\tau_1^2)\n\\]\nVisualizando a dispersão dos parâmetros em relação às médias populacionais \\(\\gamma_0, \\gamma_1\\)."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "",
    "text": "A modelagem bayesiana constitui um processo sistemático e iterativo para integrar dados e conhecimento prévio, visando a compreensão aprofundada de um fenômeno. O fluxo de trabalho bayesiano é um ciclo contínuo que envolve:\nDescobertas ou problemas identificados em etapas posteriores (como diagnóstico ou validação) frequentemente levam à revisão e ao refinamento de decisões tomadas em etapas anteriores (como a especificação do modelo ou a escolha das priors). Para uma dsicussão detalhada do fluxo de trabalho Bayesiano, veja o texto Bayesian workflow.\nPara demonstrar este fluxo de trabalho de forma prática, utilizaremos a biblioteca Bambi (BAyesian Model-Building Interface), uma interface de alto nível construída sobre o PyMC que simplifica a implementação de modelos bayesianos comuns em Python. A biblioteca Bambi utiliza uma sintaxe baseada em fórmulas, semelhante àquela encontrada em pacotes R como lme4 ou brms, permitindo que nos concentremos mais nas etapas analíticas do fluxo de trabalho do que nos detalhes computacionais subjacentes.\nNosso objetivo será percorrer estas etapas utilizando o conjunto de dados altura_adultos_subset.csv, que descreve a relação entre altura de indivíduos e número do calçado. Ao fazer isso, esperamos que você reflita criticamente sobre como a avaliação sistemática e o refinamento contínuo do modelo, facilitados por ferramentas como Bambi/PyMC, são essenciais para extrair conhecimento científico robusto sobre os processos subjacentes aos dados observados.\nNosso objetivo será percorrer cada uma das etapas do fluxo de trabalho bayesiano utilizando o conjunto de dados altura_adultos_subset.csv, que descreve a relação entre a altura de indivíduos e o número do calçado."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#preparação-do-ambiente",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#preparação-do-ambiente",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "1 Preparação do ambiente",
    "text": "1 Preparação do ambiente\n\n# Importação das principais bibliotecas necessárias para análise de dados, visualização, modelagem bayesiana e diagnóstico.\nimport arviz as az\nimport bambi as bmb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport random"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#importação-e-visualização-dos-dados",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#importação-e-visualização-dos-dados",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "2 Importação e visualização dos dados",
    "text": "2 Importação e visualização dos dados\nVamos visualizar a relação entre o número do calçado e a altura. Esta etapa é importante para termos uma ideia preliminar do padrão nos dados a fim de julgarmos qual modelo adequado para descrever esta relação.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/altura_adultos_subset.csv')\n\n\nsns.regplot(data=df, x='calcado', y='altura', ci=None, scatter=True, fit_reg=False)\n\n\n\n\n\n\n\n\nOs dados observados, sugere que um modelo linear é razoável se buscamos prever a altura de uma passoal adultra como função do número do calçado o que justifica a implementação de um modelo de regressão linear simples."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#especificação-e-ajuste-do-modelo",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#especificação-e-ajuste-do-modelo",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "3 Especificação e ajuste do modelo",
    "text": "3 Especificação e ajuste do modelo\nEspecificamos um modelo de regressão linear bayesiano em que a altura é modelada como uma função do número do calçado. A biblioteca Bambi adota uma sintaxe onde expressamos a relação entre as variáveis na forma y ~ x. Essa notação indica que a variável resposta (y) é explicada linearmente pela variável preditora (x), o que corresponde, ao modelo:\n\\[\ny = \\beta_0 + \\beta_1 x.\n\\]\nNesse caso, o Bambi interpreta a fórmula altura ~ calcado como uma especificação de que a altura dos indivíduos depende linearmente do número do calçado, com coeficientes a serem estimados a partir dos dados.\n\nmod = bmb.Model(\"altura ~ calcado\", df)\nmod\n\n       Formula: altura ~ calcado\n        Family: gaussian\n          Link: mu = identity\n  Observations: 5\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 172.0, sigma: 300.3331)\n            calcado ~ Normal(mu: 0.0, sigma: 7.3612)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 11.8659)\n\n\n\n\n\n\n\n\nEstrutura do modelo em Bambi\n\n\n\nAo inspecionair o objeto mod, podemos verificar um resumo da estrutura do modelo especificado:\n\nFormula: altura ~ calcado: Descreve a fórmula estatística especificada no modelo.\nFamily: gaussian: Indica a família da distribuição de probabilidade que Bambi assumiu para a variável resposta.\nLink: mu = identity: Especifica a função de ligação que conecta o modelo linear ao parâmetro da família da distribuição.\nObservations: 5: Mostra o número de linhas (observações) no DataFrame que foram usadas para construir o modelo.\nPriors: Distribuições a priori. Se não especificadas pelo usuário, o Bambi atribui priors padrão razoáveis parea o conjuto de dados e o modelo utilizado.\n\ntarget = mu: Indica que o modelo linear está focando em estimar a média (mu) da distribuição Gaussiana.\n\nCommon-level effects: Lista os parâmetros associados aos termos fixos no modelo linear.\nIntercept ~ Normal(…): Define a prior para o intercepto (\\(\\beta_0\\))\ncalcado ~ Normal(…): Define a prior para o coeficiente de regressão associado à variável calcado (\\(\\beta_1\\)).\n\nAuxiliary parameters: Parâmetros não diretamente modelados pelo predictor linear.\n\nsigma ~ HalfStudentT(…): Define a prior para o parâmetro de desvio padrão (\\(\\sigma\\))."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#amostragem-mcmc",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#amostragem-mcmc",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "4 Amostragem MCMC",
    "text": "4 Amostragem MCMC\nRealizamos a amostragem MCMC (Markov Chain Monte Carlo) para obter amostras da distribuição a posteriori dos parâmetros, combinando as informações fornecidas pelos dados com as distribuições a priori.\n\nmod_fit = mod.fit()\nmod_fit\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 104kB\nDimensions:    (chain: 4, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 32B 0 1 2 3\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    sigma      (chain, draw) float64 32kB 11.86 10.15 11.94 ... 4.894 6.69 8.378\n    Intercept  (chain, draw) float64 32kB 87.33 90.64 34.92 ... 84.1 44.14 59.14\n    calcado    (chain, draw) float64 32kB 1.803 1.763 3.224 ... 3.209 2.722\nAttributes:\n    created_at:                  2025-05-30T13:46:23.157991+00:00\n    arviz_version:               0.21.0\n    inference_library:           pymc\n    inference_library_version:   5.22.0\n    sampling_time:               1.2499067783355713\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (3)sigma(chain, draw)float6411.86 10.15 11.94 ... 6.69 8.378array([[11.86477137, 10.15128558, 11.9404382 , ...,  3.76103784,\n         5.95682226,  5.95682226],\n       [14.35906369,  8.93774861,  5.63827284, ...,  5.71426053,\n         5.11316288,  5.11316288],\n       [12.87408476, 14.09568449, 15.66752404, ..., 19.59085219,\n        12.67264739, 16.49480086],\n       [ 7.26725709,  7.35418956,  7.56856572, ...,  4.8935912 ,\n         6.68975232,  8.37829416]], shape=(4, 1000))Intercept(chain, draw)float6487.33 90.64 34.92 ... 44.14 59.14array([[ 87.33131758,  90.64192016,  34.9222604 , ...,  70.57258135,\n         72.68062549,  72.68062549],\n       [ 51.18387516, 107.74872089,  59.0531013 , ...,  90.63732289,\n         57.85395845,  57.85395845],\n       [-53.48917611,  74.98521704, 226.83997395, ..., 240.89626643,\n        156.3910527 , 145.63489119],\n       [ 78.97108538,  60.99587518,   0.97771223, ...,  84.09580471,\n         44.13761519,  59.14098448]], shape=(4, 1000))calcado(chain, draw)float641.803 1.763 3.224 ... 3.209 2.722array([[ 1.80322541,  1.76304107,  3.22371097, ...,  2.51703444,\n         2.41963497,  2.41963497],\n       [ 2.97272307,  1.62177739,  2.9033664 , ...,  1.96525012,\n         2.74284597,  2.74284597],\n       [ 5.23590392,  2.20596835, -1.41932356, ..., -1.66633675,\n         0.52144477,  0.77138747],\n       [ 2.21728192,  2.82372555,  4.10504773, ...,  2.18656311,\n         3.20902371,  2.72152341]], shape=(4, 1000))Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2025-05-30T13:46:23.157991+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0sampling_time :1.2499067783355713tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 496kB\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 32B 0 1 2 3\n  * draw                   (draw) int64 8kB 0 1 2 3 4 5 ... 995 996 997 998 999\nData variables: (12/17)\n    perf_counter_start     (chain, draw) float64 32kB 1.509e+06 ... 1.509e+06\n    process_time_diff      (chain, draw) float64 32kB 0.0003775 ... 0.0003657\n    n_steps                (chain, draw) float64 32kB 7.0 3.0 3.0 ... 3.0 7.0\n    diverging              (chain, draw) bool 4kB False False ... False False\n    max_energy_error       (chain, draw) float64 32kB 0.73 -0.09901 ... -0.2155\n    smallest_eigval        (chain, draw) float64 32kB nan nan nan ... nan nan\n    ...                     ...\n    reached_max_treedepth  (chain, draw) bool 4kB False False ... False False\n    largest_eigval         (chain, draw) float64 32kB nan nan nan ... nan nan\n    energy                 (chain, draw) float64 32kB 33.56 30.79 ... 27.58\n    step_size              (chain, draw) float64 32kB 0.6078 0.6078 ... 0.6198\n    step_size_bar          (chain, draw) float64 32kB 0.6792 0.6792 ... 0.702\n    tree_depth             (chain, draw) int64 32kB 3 2 2 3 3 2 ... 3 3 2 2 2 3\nAttributes:\n    created_at:                  2025-05-30T13:46:23.170103+00:00\n    arviz_version:               0.21.0\n    inference_library:           pymc\n    inference_library_version:   5.22.0\n    sampling_time:               1.2499067783355713\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999], shape=(1000,))Data variables: (17)perf_counter_start(chain, draw)float641.509e+06 1.509e+06 ... 1.509e+06array([[1509173.29019948, 1509173.2906515 , 1509173.29092577, ...,\n        1509173.64997446, 1509173.65041338, 1509173.65114615],\n       [1509173.28464032, 1509173.28505573, 1509173.28551873, ...,\n        1509173.67987514, 1509173.68030308, 1509173.68054778],\n       [1509173.27894851, 1509173.27938086, 1509173.27979148, ...,\n        1509173.64920008, 1509173.64960877, 1509173.65023571],\n       [1509173.29898873, 1509173.29926685, 1509173.29953312, ...,\n        1509173.7485508 , 1509173.74879253, 1509173.74903801]],\n      shape=(4, 1000))process_time_diff(chain, draw)float640.0003775 0.0002018 ... 0.0003657array([[3.77470e-04, 2.01850e-04, 2.09880e-04, ..., 3.69550e-04,\n        5.44850e-04, 3.07350e-04],\n       [3.24560e-04, 3.87550e-04, 3.75901e-04, ..., 3.49750e-04,\n        1.80850e-04, 9.42900e-05],\n       [3.62370e-04, 3.45830e-04, 3.53860e-04, ..., 3.45410e-04,\n        3.41110e-04, 1.92010e-04],\n       [2.07500e-04, 1.97260e-04, 1.97360e-04, ..., 1.80250e-04,\n        1.78760e-04, 3.65700e-04]], shape=(4, 1000))n_steps(chain, draw)float647.0 3.0 3.0 7.0 ... 3.0 3.0 3.0 7.0array([[7., 3., 3., ..., 7., 7., 3.],\n       [3., 7., 7., ..., 7., 3., 1.],\n       [7., 7., 7., ..., 7., 7., 3.],\n       [3., 3., 3., ..., 3., 3., 7.]], shape=(4, 1000))diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))max_energy_error(chain, draw)float640.73 -0.09901 ... 1.027 -0.2155array([[ 0.72997728, -0.09900739, -0.10906044, ..., -0.41916173,\n        -0.20922749,  0.24970342],\n       [ 1.57433219,  1.27389191, -0.31758128, ..., -0.08568415,\n        -0.04667045,  6.61530849],\n       [ 1.26372731, -0.48685816,  0.91414297, ...,  3.20537544,\n        -0.66024269,  0.18920641],\n       [ 0.72978671,  0.20058716,  0.46076393, ..., -0.45564808,\n         1.02653708, -0.21552247]], shape=(4, 1000))smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 1000))lp(chain, draw)float64-30.39 -29.9 ... -26.9 -26.92array([[-30.39347385, -29.89663576, -28.71491483, ..., -27.22877211,\n        -26.28045812, -26.28045812],\n       [-28.76168152, -27.4697495 , -28.14214924, ..., -26.8752654 ,\n        -26.93829466, -26.93829466],\n       [-32.5764908 , -29.35688635, -31.91369125, ..., -32.27447206,\n        -29.72670046, -30.09658165],\n       [-26.94412726, -27.05992823, -28.89853798, ..., -26.64767046,\n        -26.90277585, -26.92166239]], shape=(4, 1000))energy_error(chain, draw)float64-0.1167 0.08795 ... 0.3881 -0.03299array([[-0.1166558 ,  0.08794639, -0.10083383, ..., -0.23949834,\n        -0.09624587,  0.        ],\n       [ 1.23915864, -0.61760088, -0.31758128, ..., -0.08568415,\n        -0.04471395,  0.        ],\n       [ 0.34403876, -0.25885535,  0.07224033, ...,  3.07896529,\n        -0.01031046,  0.05682008],\n       [ 0.32582708,  0.02102798,  0.12287965, ..., -0.45564808,\n         0.38808395, -0.0329913 ]], shape=(4, 1000))acceptance_rate(chain, draw)float640.7891 0.9719 1.0 ... 0.4746 0.967array([[0.78911608, 0.97193666, 1.        , ..., 1.        , 0.99117253,\n        0.82625091],\n       [0.29672456, 0.88417763, 0.89100958, ..., 1.        , 1.        ,\n        0.0013397 ],\n       [0.55408423, 0.9881366 , 0.84314093, ..., 0.48419232, 0.99875024,\n        0.89295982],\n       [0.61062199, 0.93235666, 0.75226543, ..., 0.87300062, 0.47464928,\n        0.96697727]], shape=(4, 1000))index_in_trajectory(chain, draw)int642 -1 2 -3 -3 1 1 ... -2 -3 2 2 -1 3array([[ 2, -1,  2, ...,  1,  6,  0],\n       [ 3, -3,  7, ...,  2,  1,  0],\n       [ 2,  2,  2, ..., -6, -2,  1],\n       [ 2,  2,  2, ...,  2, -1,  3]], shape=(4, 1000))perf_counter_diff(chain, draw)float640.0003775 0.0002018 ... 0.0003655array([[3.77502991e-04, 2.01751944e-04, 2.09812075e-04, ...,\n        3.69423069e-04, 6.24225009e-04, 3.07342038e-04],\n       [3.24732857e-04, 3.87344044e-04, 3.75723001e-04, ...,\n        3.59812053e-04, 1.80641888e-04, 9.41110775e-05],\n       [3.62223014e-04, 3.45603097e-04, 3.53631796e-04, ...,\n        3.45263164e-04, 3.40893166e-04, 1.91872008e-04],\n       [2.07441160e-04, 1.97152141e-04, 1.97242014e-04, ...,\n        1.80091942e-04, 1.78511953e-04, 3.65494052e-04]], shape=(4, 1000))reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]], shape=(4, 1000))largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], shape=(4, 1000))energy(chain, draw)float6433.56 30.79 30.11 ... 28.16 27.58array([[33.55610327, 30.79463523, 30.10965285, ..., 27.63049618,\n        27.64036433, 27.05930446],\n       [30.90472513, 31.61729771, 29.53056351, ..., 27.66736592,\n        27.41160831, 30.25749755],\n       [33.79373079, 33.37791002, 33.86726122, ..., 34.32955468,\n        32.98327958, 30.44263401],\n       [28.76621741, 27.46913187, 30.16007238, ..., 27.87277549,\n        28.15908774, 27.57624054]], shape=(4, 1000))step_size(chain, draw)float640.6078 0.6078 ... 0.6198 0.6198array([[0.60783499, 0.60783499, 0.60783499, ..., 0.60783499, 0.60783499,\n        0.60783499],\n       [0.64909787, 0.64909787, 0.64909787, ..., 0.64909787, 0.64909787,\n        0.64909787],\n       [0.39508241, 0.39508241, 0.39508241, ..., 0.39508241, 0.39508241,\n        0.39508241],\n       [0.61978711, 0.61978711, 0.61978711, ..., 0.61978711, 0.61978711,\n        0.61978711]], shape=(4, 1000))step_size_bar(chain, draw)float640.6792 0.6792 ... 0.702 0.702array([[0.67918696, 0.67918696, 0.67918696, ..., 0.67918696, 0.67918696,\n        0.67918696],\n       [0.63734302, 0.63734302, 0.63734302, ..., 0.63734302, 0.63734302,\n        0.63734302],\n       [0.6308153 , 0.6308153 , 0.6308153 , ..., 0.6308153 , 0.6308153 ,\n        0.6308153 ],\n       [0.7019917 , 0.7019917 , 0.7019917 , ..., 0.7019917 , 0.7019917 ,\n        0.7019917 ]], shape=(4, 1000))tree_depth(chain, draw)int643 2 2 3 3 2 2 3 ... 2 2 3 3 2 2 2 3array([[3, 2, 2, ..., 3, 3, 2],\n       [2, 3, 3, ..., 3, 2, 1],\n       [3, 3, 3, ..., 3, 3, 2],\n       [2, 2, 2, ..., 2, 2, 3]], shape=(4, 1000))Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2025-05-30T13:46:23.170103+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0sampling_time :1.2499067783355713tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.15.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 80B\nDimensions:  (__obs__: 5)\nCoordinates:\n  * __obs__  (__obs__) int64 40B 0 1 2 3 4\nData variables:\n    altura   (__obs__) float64 40B 178.0 163.0 175.0 155.0 189.0\nAttributes:\n    created_at:                  2025-05-30T13:46:23.174230+00:00\n    arviz_version:               0.21.0\n    inference_library:           pymc\n    inference_library_version:   5.22.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.15.0xarray.DatasetDimensions:__obs__: 5Coordinates: (1)__obs__(__obs__)int640 1 2 3 4array([0, 1, 2, 3, 4])Data variables: (1)altura(__obs__)float64178.0 163.0 175.0 155.0 189.0array([178., 163., 175., 155., 189.])Indexes: (1)__obs__PandasIndexPandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='__obs__'))Attributes: (6)created_at :2025-05-30T13:46:23.174230+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0modeling_interface :bambimodeling_interface_version :0.15.0"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#verificação-das-priors",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#verificação-das-priors",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "5 Verificação das priors",
    "text": "5 Verificação das priors\nNeste exemplo as priors foram atribuídas automaticamente. Veremos como especificá-las manualmente mais adiante. Por enquanto, iremos nos concentrar em visualizar as distribuições a priori para entender as expectativas iniciais do modelo sobre os parâmetros antes de processar os dados.\n\nmod.plot_priors(var_names=['Intercept', 'calcado', 'sigma'], figsize=(9, 4))\n\narray([&lt;Axes: title={'center': 'Intercept'}&gt;,\n       &lt;Axes: title={'center': 'calcado'}&gt;,\n       &lt;Axes: title={'center': 'sigma'}&gt;], dtype=object)"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#diagnósticos-de-convergência",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#diagnósticos-de-convergência",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "6 Diagnósticos de convergência",
    "text": "6 Diagnósticos de convergência\nApós realizar a amostragem MCMC, é fundamental verificar se o processo de amostragem funcionou corretamente. Para isso, utilizamos diagnósticos de convergência, que nos ajudam a avaliar se as cadeias geradas estão representando adequadamente a distribuição a posteriori dos parâmetros.\nUma das ferramentas mais comuns para essa avaliação são os trace plots — gráficos que mostram os valores amostrados para cada parâmetro ao longo das iterações. Idealmente, essas cadeias devem parecer bem misturadas e sem padrões visíveis, o que indica que a amostragem atingiu o chamado estado estacionário, sugerindo que as estimativas são confiáveis.\nTendências, oscilações sistemáticas ou falta de sobreposição entre diferentes cadeias pode ser um sinal de que o algoritmo não convergiu adequadamente, exigindo ajustes no modelo ou no processo de amostragem.\n\n# Gráficos de diagnóstico\nfig, axes = plt.subplots(3, 2, figsize=(8, 8))\n\n# Trace plots\naz.plot_trace(mod_fit, var_names=['Intercept', 'calcado', 'sigma'], axes=axes)\nplt.suptitle('Trace Plots - Convergência das Cadeias MCMC', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#resumo-do-ajuste",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#resumo-do-ajuste",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "7 Resumo do ajuste",
    "text": "7 Resumo do ajuste\n\naz.summary(mod_fit)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n9.054\n4.328\n3.614\n17.059\n0.139\n0.179\n946.0\n1292.0\n1.01\n\n\nIntercept\n67.498\n45.410\n-11.602\n164.973\n0.932\n1.674\n2501.0\n1518.0\n1.00\n\n\ncalcado\n2.571\n1.118\n0.220\n4.542\n0.023\n0.042\n2526.0\n1674.0\n1.00\n\n\n\n\n\n\n\nApós a amostragem MCMC e a verificação da convergência, exploramos um resumo estatístico das distribuições a posteriori dos parâmetros do modelo. Na tabela acima, cada linha corresponde a um parâmetro (sigma, Intercept, calcado). As colunas fornecem estimativas pontuais (MEAN), intervalos de incerteza (SD, HDI) e métricas para verificar a qualidade e a confiabilidade das amostras MCMC (MCSE, ESS, R_HAT).\n\n\n\n\n\n\nTabela resumo em um modelo bayesiano\n\n\n\n\nMEAN: A média das amostras a posteriori para o parâmetro.\nSD: O desvio padrão das amostras a posteriori.\nHDI_3% e HDI_97%: Os limites inferior (3%) e superior (97%) do Intervalo de Credibilidade de Maior Densidade (HDI).\nMCSE_MEAN (Monte Carlo Standard Error of the Mean): O Erro Padrão de Monte Carlo da Média estima a variabilidade da estimativa da média a posteriori devido ao número finito e à correlação entre as amostras MCMC.\nMCSE_SD (Monte Carlo Standard Error of the Standard Deviation): O Erro Padrão de Monte Carlo do Desvio Padrão. Similar ao MCSE_MEAN, mas estima a precisão com que o desvio padrão a posteriori foi estimado a partir das amostras.\nESS_BULK (Effective Sample Size - Bulk): O Tamanho Efetivo da Amostra. Devido à autocorrelação nas cadeias MCMC, o número de amostras efetivamente independentes é geralmente menor que o número total de amostras coletadas.\nESS_TAIL (Effective Sample Size - Tail): O Tamanho Efetivo da Amostra para as características das caudas da distribuição (como quantis extremos).\nR_HAT (Gelman-Rubin statistic): O R-hat é um diagnóstico de convergência que compara a variabilidade dentro de cada cadeia MCMC com a variabilidade entre as diferentes cadeias. Se todas as cadeias convergiram para a mesma distribuição estacionária (a posteriori alvo), o valor de R_HAT deve ser muito próximo de 1 (idealmente &lt;= 1.01 ou &lt;= 1.05 no máximo). Valores significativamente maiores que 1 indicam que as cadeias não convergiram bem."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#verificação-das-posteriores-e-comparação-com-as-priors",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#verificação-das-posteriores-e-comparação-com-as-priors",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "8 Verificação das posteriores e comparação com as priors",
    "text": "8 Verificação das posteriores e comparação com as priors\nA comparação gráfica entre as distribuições a priori e a posteriori dos parâmetros nos ajuda a avaliar o quanto os dados foram informativos, mostrando quais parâmetros foram mais ou menos atualizados em relação às nossas crenças iniciais. Uma pequena mudança da prior para a posteriori indica que os dados trouxeram pouca informação nova sobre aquele parâmetro, enquanto uma grande diferença sugere que os dados foram bastante informativos. No gráfico abaixo, visualizamos essa comparação para os parâmetros do modelo (Intercept, calcado e sigma), onde a linha superior exibe as distribuições a priori (atribuídas automaticamente) e a linha inferior apresenta as distribuições a posteriori resultantes da análise bayesiana.\n\nparam_order = ['Intercept', 'calcado', 'sigma']\n\nfig, axes = plt.subplots(nrows=2, ncols=len(param_order), figsize=(9, 6))\n\nmod.plot_priors(var_names=param_order, ax=axes[0, :])\naz.plot_posterior(mod_fit, var_names=param_order, ax=axes[1, :])\n\naxes[0, 0].set_ylabel('Densidade das Prioris')\naxes[1, 0].set_ylabel('Densidade das Posteriores')\n\nText(0, 0.5, 'Densidade das Posteriores')\n\n\n\n\n\n\n\n\n\n\naz.plot_trace(mod_fit, figsize=(9,10))\n\narray([[&lt;Axes: title={'center': 'sigma'}&gt;,\n        &lt;Axes: title={'center': 'sigma'}&gt;],\n       [&lt;Axes: title={'center': 'Intercept'}&gt;,\n        &lt;Axes: title={'center': 'Intercept'}&gt;],\n       [&lt;Axes: title={'center': 'calcado'}&gt;,\n        &lt;Axes: title={'center': 'calcado'}&gt;]], dtype=object)"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#predição-bayesiana",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#predição-bayesiana",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "9 Predição Bayesiana",
    "text": "9 Predição Bayesiana\nO processo de predição nos permite estimar valores da variável resposta para novas observações não incluídas no conjunto de dados original. Na abordagem bayesiana, a predição vai além de fornecer uma estimativa pontual (isto é, da média), incorporando explicitamente a incerteza associada tanto aos parâmetros do modelo quanto à variabilidade intrínseca do processo gerador dos dados.\nA predição bayesiana utiliza toda a distribuição a posteriori dos parâmetros para gerar a distribuição preditiva a posteriori. Esta distribuição captura duas fontes principais de incerteza:\n\nIncerteza epistêmica: Relacionada ao nosso conhecimento limitado sobre os verdadeiros valores dos parâmetros do modelo (representada pela distribuição a posteriori dos parâmetros).\nIncerteza aleatória: Relacionada à variabilidade natural do processo (representada pelo componente estocástico do modelo, como o termo de erro \\(\\sigma\\)).\n\nA combinação dessas duas fontes de incerteza resulta em intervalos de predição mais amplos que os intervalos de credibilidade da reta média, refletindo de forma mais realista nossa incerteza sobre observações futuras.\n\n9.1 Predição sobre a reta média (Incerteza epistêmica)\nA predição da reta média quantifica nossa incerteza sobre o valor esperado da variável resposta, considerando apenas a incerteza nos parâmetros do modelo. As amostras da posteriori geradas pelo método MCMC nos fornecem várias combinações de parâmetros possíveis ajustadas ao conjunto de dados. Podemos entender estas como retas possíveis para o conjunto observado - algumas combinações dos parâmetros fornecem retas mais prováveis, outras menos.\n\n9.1.1 Visualização das retas possíveis\nPara visualizar essa incerteza epistêmica, vamos primeiro obter amostras da distribuição posterior e construir algumas retas possíveis:\n\n# Definir pontos extremos para construção das retas\nx_vals = [32, 48]\nnovo_x = pd.DataFrame({\"calcado\": x_vals})\nposterior_par = mod.predict(mod_fit, kind=\"response_params\", data=novo_x, inplace=False)\nmu_vals = posterior_par.posterior['mu'].values\nmu_flat = mu_vals.reshape(-1, mu_vals.shape[-1])\n\n\n# Calcular a reta média\ny_mean = (posterior_par.posterior['Intercept'].mean().values + \n          posterior_par.posterior['calcado'].mean().values * x_vals)\n\n\n# Plotar uma amostra de retas possíveis\nn = 100\nindices = np.random.choice(mu_flat.shape[0], size=n, replace=False)\n\nplt.figure(figsize=(9, 6))\nfor i in indices:\n    plt.plot(x_vals, mu_flat[i, :], '#e37d76', alpha=0.1)\n\nplt.plot(x_vals, y_mean, '#162be0', linewidth=2, label='Reta média')\n\n# Adicionar os pontos observados e reta média\nsns.scatterplot(data=df, x='calcado', y='altura', color='green', label='Observações', s = 100)\n\n\nplt.xlabel(\"Calçado\")\nplt.ylabel(\"Altura prevista\")\nplt.title(f\"Amostra de {n} retas da posteriori\")\nplt.legend()\nplt.grid(True)\nplt.ylim(120, 210)\nplt.xlim(32, 48)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.1.2 Intervalo de Credibilidade (IC) para \\(\\mu\\)\nAo invés de representar todas as retas possíveis, podemos criar um envelope contendo as combinações que determinam os intervalos de credibilidade para a reta média, representando nossa incerteza sobre o valor esperado da resposta:\n\n# Criar sequência contínua para visualização\nx_seq = np.linspace(32, 48, 100)\n\n# Extrair amostras dos parâmetros\nintercept = posterior_par.posterior['Intercept'].values.flatten()\nslope = posterior_par.posterior['calcado'].values.flatten()\n\n# Calcular retas para toda a sequência\ny_seq = intercept[:, None] + slope[:, None] * x_seq[None, :]\n\n# Calcular intervalo de credibilidade de 95%\ny_ci = np.percentile(y_seq, [2.5, 97.5], axis=0)\n\n# Reta média para toda a sequência\nintercept_mean = posterior_par.posterior['Intercept'].mean().values\nslope_mean = posterior_par.posterior['calcado'].mean().values\ny_mean_seq = intercept_mean + slope_mean * x_seq\n\n\n# Plotar resultados\nplt.figure(figsize=(9, 6))\n\n# Intervalo de credibilidade (envelope)\nplt.fill_between(x_seq, y_ci[0], y_ci[1], color='#e37d76', alpha=0.2, \n                 label='IC 95% da reta média')\n\n# Pontos observados\nsns.scatterplot(data=df, x='calcado', y='altura', color='green', label='Observações', s = 100)\n\nplt.plot(x_seq, y_mean_seq, '#162be0', linewidth=2, label='Reta média')\n\nplt.xlabel(\"Calçado\")\nplt.ylabel(\"Resposta média predita ($\\mu$)\")\nplt.title(\"Reta média e intervalo de credibilidade (95%)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nO Intervalo de Credibilidade representa nossa incerteza sobre o valor médio esperado da altura para indivíduos com um determinado número de calçado, refletindo exclusivamente a incerteza epistêmica associada aos parâmetros do modelo. Note que a maioria das retas passa próxima ao centro da distribuição de \\(x\\) e \\(y\\), pois, nas proximidades da média de \\(x\\), há maior confiança na estimativa da trajetória da reta. À medida que nos afastamos desse centro, a incerteza aumenta, resultando em maior variabilidade nas regiões mais afastadas de \\(x\\).\n\n\n\n9.2 Predição sobre novos pontos (Incerteza epistêmica + aleatória)\nA predição de novas observações vai além da estimativa da média, incorporando também a variabilidade intrínseca do processo. Enquanto o intervalo de credibilidade nos representa nossa incerteza quanto à reta média, o Intervalo de Predição (IP) nos informa sobre a incerteza associada ao valor que uma nova observação específica pode assumir.\n\n9.2.1 Implementação da predição para novas observações\n\n# Definir valores para predição\ncalcado_pred = np.array([35, 40, 45])\ndados_pred = pd.DataFrame({\"calcado\": calcado_pred})\n\n# Predição da resposta média (μ - apenas incerteza epistêmica)\npred_mu = mod.predict(mod_fit, kind=\"response_params\", data=dados_pred, inplace=False)\n\n# Predição de novas observações (μ + σ - ambas as incertezas)\npred_obs = mod.predict(mod_fit, kind=\"response\", data=dados_pred, inplace=False)\n\nprint(\"Valores preditos para número do calçado:\", calcado_pred)\nprint(\"\\nResumo das predições:\")\nprint(f\"Calçado 35: μ = {pred_mu.posterior['mu'].values[:,:,0].mean():.1f} cm\")\nprint(f\"Calçado 40: μ = {pred_mu.posterior['mu'].values[:,:,1].mean():.1f} cm\") \nprint(f\"Calçado 45: μ = {pred_mu.posterior['mu'].values[:,:,2].mean():.1f} cm\")\n\nValores preditos para número do calçado: [35 40 45]\n\nResumo das predições:\nCalçado 35: μ = 157.5 cm\nCalçado 40: μ = 170.4 cm\nCalçado 45: μ = 183.2 cm\n\n\n\n# Visualização das predições pontuais\n# Extrair amostras aleatórias de novas observações para cada valor de calçado\nn_samples = 80\ncalcado_pred = [35, 40, 45]\ncores = ['#ff6b6b', '#4ecdc4', '#d1c845']\n\nplt.figure(figsize=(8, 5))\n\n# Reta média\nplt.plot(x_seq, y_mean_seq, '#162be0', linewidth=2, label='Reta média (μ)')\n\n# Plotar dados originais\nsns.scatterplot(data=df, x='calcado', y='altura', color='green', s=100, \n                label='Dados observados', zorder=5)\n\n# Plotar uma amostra das predições para novos pontos\nfor i, (calcado_val, cor) in enumerate(zip(calcado_pred, cores)):\n    x_sample = [calcado_val] * n_samples\n    y_sample = random.sample(list(pred_mu.posterior['mu'].values[:,:,i].flatten()), n_samples)\n    plt.scatter(x_sample, y_sample, color=cor, alpha=0.4, s=50, label=f'Predições calçado n. {calcado_val}')\n\nplt.xlabel(\"Número do calçado\")\nplt.ylabel(\"Altura (cm)\")\nplt.title(f\"Reta média e amostra de {n_samples} predições para cada valor de calçado\")\nplt.legend(loc='lower right')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHistograma das novas predições\n\nplt.figure(figsize=(8, 5))\n\nfor i, (calcado_val, cor) in enumerate(zip(calcado_pred, cores)):\n    y_sample = random.sample(\n        list(pred_mu.posterior['mu'].values[:, :, i].flatten()), n_samples\n    )\n    sns.kdeplot(y_sample, color=cor, fill=True, alpha=0.3, linewidth=2,\n                label=f'Calçado n. {calcado_val}')\n\nplt.xlabel(\"Altura prevista\")\nplt.ylabel(\"Densidade\")\nplt.title(\"Distribuições de altura\\ndas novas predições por número de calçado\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nObserve que as médias das predições para novas observações coincidem com a reta média estimada (\\(\\mu\\)), mas a variabilidade em torno dessas médias é maior. Isso ocorre porque essas predições incorporam tanto a incerteza epistêmica, proveniente da estimação dos parâmetros do modelo, e a incerteza aleatória, associada à variabilidade intrínseca do processo (\\(\\sigma\\)).\n\n\n\n\n\n\nInterpretação dos tipos de predição bayesiana\n\n\n\nA abordagem bayesiana fornece uma caracterização completa e hierárquica da incerteza preditiva: primeiro quantificando nossa incerteza sobre a relação média entre as variáveis, e depois expandindo essa análise para incluir a variabilidade natural do fenômeno. Essa distinção é fundamental para decisões informadas, pois diferentes tipos de decisão podem requerer diferentes tipos de intervalo preditivo.\nIntervalo de Credibilidade (IC) para μ: Representa nossa incerteza sobre o valor médio esperado da altura para indivíduos com um determinado número de calçado. Este intervalo reflete apenas a incerteza epistêmica sobre os parâmetros do modelo. É mais estreito porque representa nossa incerteza sobre a linha média da relação.\nIntervalo de Predição (IP) para novas observações: Representa nossa incerteza sobre a altura de um novo indivíduo específico com um determinado número de calçado. Este intervalo é sempre mais amplo pois incorpora tanto a incerteza epistêmica (sobre os parâmetros) quanto a incerteza aleatória (variabilidade natural representada por \\(\\sigma\\)).\nA razão entre as larguras dos intervalos indica o quanto a variabilidade intrínseca do processo contribui para a incerteza total. Valores maiores sugerem que a variabilidade natural dos dados (\\(\\sigma\\)) é a principal fonte de incerteza nas predições, enquanto valores próximos de 1 indicariam que a incerteza sobre os parâmetros é dominante. Essa razão também permite avaliar aspectos do delineamento experimental, pois a incerteza epistêmica diminui à medida que obtemos mais dados, aumentando nossa confiança na posição da reta média. Por outro lado, a incerteza aleatória é inerente ao processo gerador dos dados e não é afetada pelo tamanho da amostra, representando um limite irreducível da precisão nas predições."
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#estrutura-de-código-bambi-vs-pymc",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana-bambi.html#estrutura-de-código-bambi-vs-pymc",
    "title": "Fluxo de Trabalho na Modelagem Bayesiana",
    "section": "10 Estrutura de código: Bambi vs PyMC",
    "text": "10 Estrutura de código: Bambi vs PyMC\nNeste bloco, utilizamos a biblioteca Bambi como alternativa ao PyMC. Segue portanto uma comparação entre as duas abordagens para a implementação de uma regressão linear bayesiana.\n\n\n\n\n\n\nQuando usar cada abordagem\n\n\n\nBambi: estrutura de código\n\n# Bambi\nmodelo = bmb.Model(\"altura ~ calcado\", df, priors=priors)\nresultados = modelo.fit()\n\nQuando usar:\n\nDeseja implementar modelos estatísticos padrão (regressão linear, GLMs, modelos hierárquicos)\nNecessita de rapidez de desenvolvimento e ajustes\n\nPyMC: estrutura de código\n\n# PyMC - requer definição manual de todas as componentes\nwith pm.Model() as modelo:\n    # Priors\n    beta_0 = pm.Normal(\"beta_0\", mu=60, sigma=5)\n    beta_1 = pm.Normal(\"beta_1\", mu=2.8, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n    \n    # Verossimilhança\n    mu = beta_0 + beta_1 * X\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, observed=Y)\n    trace = pm.sample()\n\nQuando usar:\n\nNecessitar controle total sobre a especificação do modelo\nNecessita implementar modelos customizados ou muito complexos\nNecessita de funcionalidades específicas não disponíveis no Bambi"
  },
  {
    "objectID": "conteudo/amostragem/tipos_amostragem.html",
    "href": "conteudo/amostragem/tipos_amostragem.html",
    "title": "Amostrando uma população estatística",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas no capítulo\n\n\n\n\n\nPacotes:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(knitr)\nO objetivo da amostragem é descrever características da população estatística por meio de características da amostra. Por exemplo, em um estudo sobre o tamanho do pescado em uma área de pesca, a população estatística são os comprimentos de todos os peixes que podem ser pescados na região. A população estatística pode ser descrita por parâmetros que representam medidas de centro como o comprimento médio (\\(\\mu\\)), ou por medidas de variação como o desvio padrão (\\(\\sigma\\)), que representam o grau de dispersão das unidades amostrais ao redor da média. Se amostramos n elementos desta população, a média amostral (\\(\\overline{X}\\)) e o desvio padrão amostral (\\(s\\)) dos diâmetros serão os estimadores destas características.\nDependendo da questão envolvida e do conhecimento prévio sobre a população, diferentes métodos de amostragem são apropriados. A teoria da amostragem é a área da ciência que estuda estes métodos. Neste capítulo vamos discutir três tipos de amostragem: aleatória simples, estratificada e sistemática."
  },
  {
    "objectID": "conteudo/amostragem/tipos_amostragem.html#amostragem-aleatória-simples",
    "href": "conteudo/amostragem/tipos_amostragem.html#amostragem-aleatória-simples",
    "title": "Amostrando uma população estatística",
    "section": "1 Amostragem aleatória simples",
    "text": "1 Amostragem aleatória simples\nÉ aquela em que cada elemento da população tem a mesma probabilidade de ser selecionado para compor a amostra. Se a população consiste de \\(1000\\) elementos, cada um terá uma probabilidade de \\(\\frac{1}{1000}\\) de ser escolhido. Isto isenta o pesquisador de tomar qualquer decisão com base em julgamentos pré-concebidos, sobre quais elementos devem ou não compor a amostra.\n\n\nCódigo\nset.seed(1)\npop = c(3, 10, 14, 19, 27, 28, 29, 41, 42, 43)\nN = length(pop)\nAm1 = sort(pop)[1:5]\nset.seed(2)\nAm2 = sample(pop, size = 5, replace = F)\n\n\nSuponha uma população hipotética de somente \\(10\\) elementos:\nPopulação: 3, 10, 14, 19, 27, 28, 29, 41, 42, 43\nEm uma amostra aleatória simples de cinco elementos, qualquer combinação destes \\(10\\) elementos é igualmente provável. Se por puro acaso sortearmos uma amostra aleatória contendo os cinco menores valores da população:\nAmostra 1: 3, 10, 14, 19, 27\nA amostra seria tão aleatória e válida do ponto de vista estatístico quanto qualquer outra como:\nAmostra 2: 27, 28, 42, 3, 43\nIsto significa que uma amostra aleatória pode não ser necessariamente representativa da população. Amostras pequenas por exemplo, têm uma chance maior de selecionar apenas valores extremos, ou seja, os maiores ou menores elementos da população. A média amostral (\\(\\overline{X}\\)) calculada para estas amostras estará distante da média populacional (\\(\\mu\\)). No entanto, a importância central da amostragem aleatória em estatística está no fato de que a aleatoriedade produz, em média, amostras representativas da população. Deste modo, é esperado que na maioria das vezes, uma amostra aleatória gere médias amostrais próximas à média populacional. Por este motivo, é fundamental prezar pela aleatoriedade no processo amostral, pois de outro modo não poderemos garantir que a inferência seja válida com base nas leis de probabilidade.\nO modo mais direto de se obter uma amostra aleatória é por meio de sorteio. Após atribuir números de 1 a \\(N\\) a cada unidade amostral, estas unidades são sorteadas até que seja atingido o tamanho \\(n\\) desejado. Na prática, nem sempre é simples, ou mesmo possível obter uma amostra aleatória nestes moldes. Não seria possível enumerar todos os peixes de uma região para, após um sorteio, tomar as medidas somente dequeles selecionados. Entretanto, se empregarmos métodos de captura em que indivíduos de todos os tamanhos sejam igualmente sujeitos a serem capturados poderíamos no aproximar do que seria uma amostra verdsadeiramente aleatória. Outras dificuldades práticas surgiriam neste estudo como por exemplo: garantir acesso irrestrito à toda a área de ocorrência da espécie, tempo disponível para percorrer a toda região. Questões como estas não desmerecem o requisito básico de se obter uma amostra aleatória, mas devem nos auxiliar a decidir como conciliar a prática experimental com a necessidade de garantirmos uma amostra aleatória."
  },
  {
    "objectID": "conteudo/amostragem/tipos_amostragem.html#amostragem-aleatória-estratificada",
    "href": "conteudo/amostragem/tipos_amostragem.html#amostragem-aleatória-estratificada",
    "title": "Amostrando uma população estatística",
    "section": "2 Amostragem aleatória estratificada",
    "text": "2 Amostragem aleatória estratificada\nSe tivermos algum conhecimento prévio de como a população está estruturada, a amostra aleatória simples, embora não esteja incorreta, pode não ser a estratégia mais eficiente do ponto de vista estatístico. Se for possivel identificar estratos ou subgrupos dentro da população, podemos conduzir uma amostragem aleatória estatificada.\nVoltemos ao exemplo do comprimento do pescado. Suponha que existam duas áreas de ocorrência da espécie. Uma delas sujeita a intensa atividade pesqueira e outra sendo uma área protegida. Poderíamos supor que na área protegida estejam os maiores indivíduos, justamente porque nesta área não há atividade de pesca (que em geral busca indivíduos maiores). Dizemos que os comprimentos em cada uma das duas regiões compõem estratos da população estatística.\nNesta situação, uma amostra puramente aleatória sem considerar a existência dos dois estratos pode fazer com que, puramente ao acaso, um deles se torne mais representados na amostra. Se por exemplo da maioria dos pontos selecionados estiverem na região intensamente pescada, o comprimento médio da amostra (\\(\\overline{X}\\)) tenderá a ficar consistentemente abaixo de \\(\\mu\\). A chance disto ocorrer se torna maior principalmente se o tamanho amostral for pequeno.\nEntretanto, se a seleção dos indivíduos foi feita por meio de sorteio, o simples fato de observarmos este padrão não seria por si só justificativa para refarzermos a amostra. O ponto relevante aqui é que em uma amostra aleatória simples, estes extremos indesejáveis (um estrato mais representado que outro) são mais prováveis de acontecer.\nSe temos conhecimento da existência dos dois estratos portanto, a amostragem aleatória estratificada seria a mais indicada. Neste tipo de amostragem, o esforço amostral é subdividido entre os estratos. O tamanho amostral em cada estrato será o mesmo, ou proporcional ao seu tamanho. Uma vez definirmos os tamanhos amostrais que será aplicado aos estratos, as unidades são selecionadas por meio de uma amostragem aleatória simples em cada um.\nA amostragem aleatória estratificada garante que todos os estratos estejam presentes na amostra conforme sua representatividade na população. Ao fazer isto, as estimativas da amostra tenderão a se concentrar mais próximas ao parâmetro da população. Deste modo, quando os estratos são identificados corretamente, a principal vantagem da amostra aleatória estratificada sobre a amostra aleatória simples está em aumentar a precisão das estimativas. Mais a frente iremos discutir os conceitos de precisão e acurácia e relacioná-los com as estratégias amostrais discutidas aqui."
  },
  {
    "objectID": "conteudo/amostragem/tipos_amostragem.html#amostragem-sistemática",
    "href": "conteudo/amostragem/tipos_amostragem.html#amostragem-sistemática",
    "title": "Amostrando uma população estatística",
    "section": "3 Amostragem sistemática",
    "text": "3 Amostragem sistemática\nEm uma amostragem sistemática o pesquisador escolhe um elemento inicial e toma medidas a cada \\(k\\) ocorrências, seguindo a ordem de observação. No caso do comprimento de pescado, para facilitar a tomada de dados, o pesquisador pode medir o primeiro peixe coletado e, em seguida, medir os peixes em intervalos regulares, por exemplo a cada \\(10\\) observados.\nA escolha da amostragem sistemática ao invés de uma amostragem aleatória simples, deve-se à sua praticidade. Se a característica de interesse das unidades amostrais estiver disposta de forma aleatória ao longo da sequência escolhida, a amostragem aleatória e sistemática irão gerar resultados similares. Na maioria dos casos, é isto que o pesquisador assume (ainda que implicitamente) quando opta por uma amostragem sistemática."
  },
  {
    "objectID": "conteudo/amostragem/tipos_amostragem.html#erro-amostral-acurácia-e-precisão",
    "href": "conteudo/amostragem/tipos_amostragem.html#erro-amostral-acurácia-e-precisão",
    "title": "Amostrando uma população estatística",
    "section": "4 Erro amostral, acurácia e precisão",
    "text": "4 Erro amostral, acurácia e precisão\nComo as estimativas são obtidas de um subconjunto da população (a amostra), é regra que o resultado obtido de uma amostra aleatória particular, não será igual ao verdadeiro valor da população (o parâmetro), embora exista uma grande probabilidade estar próximo.\n\nErro amostral: é a diferença entre uma estimativa em particular e o parâmetro na população e portanto, é inerente à variabilidade do processo de amostragem. Suponha que, puramente ao acaso, a amostra inclua os menores elementos da população. A média amostral (\\(\\overline{X}\\)) estará muito abaixo da média populacional (\\(\\mu\\)) e o erro amostral será grande. Se calcularmos a média (\\(\\overline{X}\\)) de uma amostra particular, o erro amostral será dado por:\n\n\\[E = \\overline{X} - \\mu\\]\nA estatística estuda o comportamento probabilístico dos erros amostrais. Existe também o erro não amostral que decorre de equívocos de amostragem, inexperiência do amostrador, falha de equipamentos, enganos no cômputo dos resultados, etc. A estatística não lida com este tipo de erro.\n\nAcurácia: se refere à proximidade entre o parâmetro e o estimador. Um estimador acurado é, em média, igual ao parâmetro populacional. Diferente do erro amostral, a acurácia não se refere a uma estimativa em particular, mas ao valor esperado do estimador, caso a amostragem fosse repetida um grande número de vezes. Um estimador não-acurado (viciado) resulta em valores consistentemente diferentes do parâmetro, podendo estar acima (viés positivo) ou abaixo (viés negativo) do valor populacional. A média aritmética amostral (\\(\\overline{X}\\)) é um estimador não-viciado da média populacional (\\(\\mu\\)) pois:\n\n\\[\\mu_{\\overline{X}} = \\mu\\]\n\nPrecisão: tem relação com a variabilidade do estimador. Estimadores que geram estimativas similares entre si são mais precisos. Porém, se as estimativas estiverem distantes de sua média, o estimador é dito pouco preciso. Exemplo: Para uma população normalmente distribuída, tanto a média aritmética quanto a mediana são estimadores acurados. Entretanto, a variância da mediana é maior que da média aritmética. Dizemos portanto, que a média aritmética é um estimador mais preciso que a mediana. A precisão de um estimador é medida pelo erro padrão da média.\n\n\\[\\sigma_{\\overline{X}} =\\frac{\\sigma}{\\sqrt{n}}\\]\nA figura abaixo é comnmente utilizada para representar os conceitos de precição e acurácia. O centro do alvo é o valor do parâmetro populacional e os pontos em preto são as estimativas. Estimadores acurados geram, em média, estimativas ao redor do parâmetro populacional (viés \\(= 0\\)). Estimadores não-acurados geram, em média, valores deslocados do parâmetro populacional (viés \\(\\ne 0\\)). Estimadores precisos resultam sempre em estimativas próximas entre si, enquanto estimadores não precisos resultam em estimativas distantes umas das outras.\n\n\n\n\n\n\nFigura 1: Representação dos conceitos de precisão e acurácia.\n\n\n\n\n4.1 Erro amostral\nVoltemos à nossa população fictícia com \\(N = 10\\) elementos:\nPopulação: 3, 10, 14, 19, 27, 28, 29, 41, 42, 43\nPara esta população em particular nós conhecemos a média populacional (\\(\\mu\\) = 25.6), de modo que será possível compará-la com as estimativas amostrais.\n\n\nCódigo\nset.seed(4)\nn = 5\nAm1 = sample(pop, size = n, replace = F)\nsomaAm1 = paste(Am1, collapse = \"+\")\nmp = round(mean(pop),1)\nmAm1 = round(mean(Am1),1)\nE1 = mAm1 - mp\n\n\nVamos tomar uma amostra aleatória de tamanho \\(n = 5\\):\nAmostra 1: \\(41, 14, 42, 29, 19\\)\nPara esta amostra, a média vale: \\(\\overline{X} =\\frac{41+14+42+29+19}{5} = 29\\).\nOs valores \\(\\mu = 25.6\\) e \\(\\overline{X} = 29\\) não são idênticos, pois a amostra contém somente alguns elementos da população. A diferença entre \\(\\mu\\) e \\(\\overline{X}\\) é o chamamos de erro amostral.\nNeste caso, o erro amostral é:\nErro amostral 1: \\(E_1 = 29  -  25.6  =  3.4\\)\nSe tomarmos outra amostra aleatória, teremos outro conjunto de unidades amostrais, e consequentemente, um \\(\\overline{X}\\) e um erro amostral diferentes. Por exemplo:\n\n\nCódigo\nset.seed(3)\nn = 5\nAm2 = sample(pop, size = n, replace = F)\nmAm2 = round(mean(Am2),1)\nE2 = mAm2 - mp\n\n\nAmostra 2: \\(27, 29, 19, 10, 14\\)\nMédia amostral 2: \\(\\overline{X_2} = 19.8\\)\nErro amostral 2: \\(E_2 = 19.8  -  25.6  =  -5.8\\)\n\n\n4.2 Acurácia\n\n\nCódigo\nN = length(pop)\nn = 5\nCT = choose(N,n)\n\n\nAté agora, analisamos duas amostras diferentes da população. Porém, quantas amostras distintas seriam possíveis? Para uma amostragem sem reposição, a teoria combinatória nos diz que são possíveis:\n\\[{{10}\\choose{5}} = \\frac{10!}{(10-5)! \\times 5!} = 252\\]\nformas diferentes de combinarmos \\(N = 10\\) elementos em amostras de tamanho \\(n = 5\\).\n\n\nCódigo\nset.seed(8)\nR = 8\nA15 = replicate(n = R, sample(pop, size = n, replace = F))\ncolnames(A15) = paste(\"A\", 1:ncol(A15), sep = \"\")\nMedias = round(apply(A15, 2, mean),2)\n\n\nInicialmente vamos avaliar a questão com um número menor. Sejam por exemplo, 8 amostras tomadas aleatoriamente, gerando os resultados a seguir:\n\n\nCódigo\nA15 |&gt; \n  as.data.frame() |&gt; \n  add_column('Obs' = rep('', times = nrow(A15)), .before = 'A1') |&gt; \n  rbind(Obs = c('Médias', Medias)) |&gt; \n  gt() |&gt; \n  tab_style(\n    style = list(cell_fill(color = \"lightblue\")),\n    locations = cells_body(\n      rows = Obs == \"Médias\"\n    )\n  ) |&gt; \n  cols_width(\n    everything() ~ px(150)\n  )\n\n\n\n\nTabela 1: Oito amostras de tamanho n = 5 da população estatística.\n\n\n\n\n\n\n\n\n\nObs\nA1\nA2\nA3\nA4\nA5\nA6\nA7\nA8\n\n\n\n\n\n19\n29\n10\n19\n42\n29\n28\n43\n\n\n\n29\n43\n14\n41\n29\n10\n42\n28\n\n\n\n10\n28\n42\n43\n41\n27\n10\n42\n\n\n\n42\n3\n28\n28\n14\n42\n43\n27\n\n\n\n43\n27\n29\n3\n28\n43\n27\n19\n\n\nMédias\n28.6\n26\n24.6\n26.8\n30.8\n30.2\n30\n31.8\n\n\n\n\n\n\n\n\n\n\nCada coluna desta matriz corresponde a uma possível amostra aleatória e suas respectivas médias.\nAlgumas amostras tiveram médias muito distantes de \\(\\mu\\), como: \\(\\overline{X_{A8}} = 31.8\\) ou \\(\\overline{X_{A3}} = 24.6\\). Esta variação é natural do processo amostral. Os métodos de amostragem e de inferência estatística tratam justamente de como interpretar e como lidar com esta variação. Para entender melhor este processo, vamos obter todas as 252 combinações possíveis de amostras com \\(n = 5\\) e, em seguida, extrair suas respectivas médias.\nOs resultados das 252 médias possíveis podem ser vistos a seguir ordenados da menor para a maior média possível:\n\n\nCódigo\nAllcomb = combn(x = pop, m = 5)\nM_Allcomb = apply(Allcomb,2,mean)\nM_Allcomb_round = round(M_Allcomb,1)\nknitr::kable(matrix(M_Allcomb_round,nc = 14, byrow = T))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.6\n14.8\n15.0\n17.4\n17.6\n17.8\n16.4\n16.6\n19.0\n19.2\n19.4\n16.8\n19.2\n19.4\n\n\n19.6\n19.4\n19.6\n19.8\n22.0\n22.2\n22.4\n17.4\n17.6\n20.0\n20.2\n20.4\n17.8\n20.2\n\n\n20.4\n20.6\n20.4\n20.6\n20.8\n23.0\n23.2\n23.4\n19.4\n21.8\n22.0\n22.2\n22.0\n22.2\n\n\n22.4\n24.6\n24.8\n25.0\n22.2\n22.4\n22.6\n24.8\n25.0\n25.2\n25.0\n25.2\n25.4\n27.8\n\n\n18.2\n18.4\n20.8\n21.0\n21.2\n18.6\n21.0\n21.2\n21.4\n21.2\n21.4\n21.6\n23.8\n24.0\n\n\n24.2\n20.2\n22.6\n22.8\n23.0\n22.8\n23.0\n23.2\n25.4\n25.6\n25.8\n23.0\n23.2\n23.4\n\n\n25.6\n25.8\n26.0\n25.8\n26.0\n26.2\n28.6\n21.2\n23.6\n23.8\n24.0\n23.8\n24.0\n24.2\n\n\n26.4\n26.6\n26.8\n24.0\n24.2\n24.4\n26.6\n26.8\n27.0\n26.8\n27.0\n27.2\n29.6\n25.6\n\n\n25.8\n26.0\n28.2\n28.4\n28.6\n28.4\n28.6\n28.8\n31.2\n28.6\n28.8\n29.0\n31.4\n31.6\n\n\n19.6\n19.8\n22.2\n22.4\n22.6\n20.0\n22.4\n22.6\n22.8\n22.6\n22.8\n23.0\n25.2\n25.4\n\n\n25.6\n21.6\n24.0\n24.2\n24.4\n24.2\n24.4\n24.6\n26.8\n27.0\n27.2\n24.4\n24.6\n24.8\n\n\n27.0\n27.2\n27.4\n27.2\n27.4\n27.6\n30.0\n22.6\n25.0\n25.2\n25.4\n25.2\n25.4\n25.6\n\n\n27.8\n28.0\n28.2\n25.4\n25.6\n25.8\n28.0\n28.2\n28.4\n28.2\n28.4\n28.6\n31.0\n27.0\n\n\n27.2\n27.4\n29.6\n29.8\n30.0\n29.8\n30.0\n30.2\n32.6\n30.0\n30.2\n30.4\n32.8\n33.0\n\n\n23.4\n25.8\n26.0\n26.2\n26.0\n26.2\n26.4\n28.6\n28.8\n29.0\n26.2\n26.4\n26.6\n28.8\n\n\n29.0\n29.2\n29.0\n29.2\n29.4\n31.8\n27.8\n28.0\n28.2\n30.4\n30.6\n30.8\n30.6\n30.8\n\n\n31.0\n33.4\n30.8\n31.0\n31.2\n33.6\n33.8\n28.8\n29.0\n29.2\n31.4\n31.6\n31.8\n31.6\n\n\n31.8\n32.0\n34.4\n31.8\n32.0\n32.2\n34.6\n34.8\n33.4\n33.6\n33.8\n36.2\n36.4\n36.6\n\n\n\n\n\nA menor e maior médias possíveis são 14.6 e 36.6 respectivamente. Estes valores são os mais distantes do parâmetro populacional (\\(\\mu = 25.6\\)) e ocorrem puramente ao acaso quanto são amostrados os 5 menores (3, 10, 14, 19, 27) ou os 5 maiores (43, 42, 41, 29, 28) elementos da população estatística. Estes casos extremos são raros. Em nosso exemplo, valores superiores a 33.8 ou inferiores a 17.4 são muito improváveis.\nPodemos avaliar graficamente a distribuição das médias amostrais através de um histograma. A grande maioria das médias amostrais concentra-se na porção intermediária do gráfico entre estes limites. Por exemplo, somente 3.2% das observações estão acima de 33.8. Da mesma forma, somente 3.2% das observações estão abaixo de 17.4\n\n\nCódigo\nM_Allcomb_df = data.frame(M = as.numeric(M_Allcomb))\n\ngp5 &lt;- ggplot(M_Allcomb_df, aes(x = M)) +\n  geom_histogram(fill = 'brown3', color = 'black', bins = 10) +\n  scale_x_continuous(breaks = seq(0, 50, by = 5)) +\n  coord_cartesian(xlim = c(10, 40)) +\n  labs(x = \"Médias\",\n       y = \"Frequência\") +\n  theme_classic()\n\ngp5\n\n\n\n\n\n\n\n\nFigura 2: Histograma das 252 médias amostrais obtidas a partis de amostras de tamanho n = 5.\n\n\n\n\n\nSe calcularmos a média das médias (\\(\\mu_{\\overline{X}}\\)), ou seja, somarmos todos estes valores e dividirmos por 252, o resultado será 25.6, que é exatamente o valor da média populacional \\(\\mu\\). Isto têm uma implicação central em inferência estatística. Significa que a média amostral \\(\\overline{X}\\) é um estimador acurado (= não-viciado), pois tende a estimar corretamente o valor da média populacional \\(\\mu\\). Ou seja, o histograma acima está centrado ao redor de \\(\\mu\\), o que significa que em média uma amostra particular tem maior probabilidade de expressar um \\(\\overline{X}\\) próximo ao valor populacional.\n\n\n4.3 Precisão: o erro padrão da média (\\(\\sigma_{\\overline{X}}\\))\n\n\nCódigo\nn2 = 7\nAllcomb7 = combn(x = pop, m = n2)\nM_Allcomb7 = apply(Allcomb7,2,mean)\nM_Allcomb7_round = round(M_Allcomb7,1)\nCT2 = choose(N,n2)\n\n\nSuponha agora que tomemos ao acaso amostras com \\(n = 7\\) desta mesma população. Existem ao todo:\n\\[{{10}\\choose{7}} = \\frac{10!}{(10-7)! \\times 7!} = 120\\]\namostras diferentes de tamanho \\(n = 7\\) que podem ser retiradas de uma população de tamanho \\(n = 10\\). Se tomarmos estas 120 amostras e calcularmos suas respectivas médias amostrais, teremos os resultados abaixo:\n\n\nCódigo\nknitr::kable(matrix(M_Allcomb7_round,nc = 12, byrow = T))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.6\n20.3\n20.4\n20.6\n20.4\n20.6\n20.7\n22.3\n22.4\n22.6\n20.6\n20.7\n\n\n20.9\n22.4\n22.6\n22.7\n22.6\n22.7\n22.9\n24.6\n21.7\n21.9\n22.0\n23.6\n\n\n23.7\n23.9\n23.7\n23.9\n24.0\n25.7\n23.9\n24.0\n24.1\n25.9\n26.0\n22.4\n\n\n22.6\n22.7\n24.3\n24.4\n24.6\n24.4\n24.6\n24.7\n26.4\n24.6\n24.7\n24.9\n\n\n26.6\n26.7\n25.7\n25.9\n26.0\n27.7\n27.9\n28.0\n23.0\n23.1\n23.3\n24.9\n\n\n25.0\n25.1\n25.0\n25.1\n25.3\n27.0\n25.1\n25.3\n25.4\n27.1\n27.3\n26.3\n\n\n26.4\n26.6\n28.3\n28.4\n28.6\n27.0\n27.1\n27.3\n29.0\n29.1\n29.3\n30.4\n\n\n24.0\n24.1\n24.3\n25.9\n26.0\n26.1\n26.0\n26.1\n26.3\n28.0\n26.1\n26.3\n\n\n26.4\n28.1\n28.3\n27.3\n27.4\n27.6\n29.3\n29.4\n29.6\n28.0\n28.1\n28.3\n\n\n30.0\n30.1\n30.3\n31.4\n28.6\n28.7\n28.9\n30.6\n30.7\n30.9\n32.0\n32.7\n\n\n\n\n\n\n\nCódigo\nM_Allcomb7_df = data.frame(M = as.numeric(M_Allcomb7))\n\ngp7 &lt;- ggplot(M_Allcomb7_df, aes(x = M)) +\n  geom_histogram(fill = 'brown3', color = 'black', bins = 10) +\n  scale_x_continuous(breaks = seq(0, 50, by = 5)) +\n  coord_cartesian(xlim = c(10, 40)) +\n  labs(x = \"Média\",\n       y = \"Frequência\") +\n  theme_classic()\n\ngp7\n\n\n\n\n\n\n\n\nFigura 3: Histograma das 120 médias amostrais obtidas a partis de amostras de tamanho n = 7\n\n\n\n\n\n\n\nCódigo\nmu_pop = mean(pop)\nN = length(pop)\nvar_pop = mean((pop - mu_pop)^2)\nsigma_pop = sqrt(var_pop)\n\nep5 = sigma_pop/sqrt(n)\nep7 = sigma_pop/sqrt(n2)\n\n\nSe compararmos os histogramas com \\(n = 5\\) e \\(n = 7\\) (Figura 2 e Figura 3), veremos que os dois resultam em estimadores acurados, pois \\(\\mu_{\\overline{X}} = \\mu\\). No entando, o intervalo de variação é menor para amostras de tamanho \\(n = 7\\). Para esta figura, os valores estão mais concentrados ao redor da média. Portanto, à medida que aumenta o tamanho amostral, diminui a dispersão das médias amostrais ao redor de \\(\\mu\\). Assim, para amostras grandes torna-se mais improvável obter uma média amostral distante da média populacional. Dizemos então que conforme aumenta o tamanho amostral, conseguimos estimativas mais precisas.\nA precisão de um estimador pode ser medida pelo Erro padrão da média (\\(\\sigma_{\\overline{X}}\\)) que pode ser calculado por:\n\\[\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nO erro padrão da média é o desvio padrão de todas as médias amostrais que poderiam ser obtidas de uma amostra com tamanho \\(n\\). Para nosso exemplo com \\(n = 5\\), \\(\\sigma_{\\overline{X}}\\) = 5.93, enquanto para \\(n = 7\\), \\(\\sigma_{\\overline{X}}\\) = 5.01. Dizemos que o último exemplo fornece estimativas mais precisas.\n\n\n\n\n\n\nErro padrão amostral\n\n\n\nNa prática científica não conhecemos o desvio padrão populacional \\(\\sigma\\) e, consequentemente, não temos obter o erro padrão populacional \\(\\sigma_{\\overline{X}}\\). No entanto, dado que temos uma amostra particular, podemos estimá-lo a partir do desvio padrão amostral \\(\\sigma_{\\overline{X}}\\) pela expressão:\n\\[s_{\\overline{X}} = \\frac{s}{\\sqrt{n}}\\]\nem que \\(s_{\\overline{X}}\\) é denominado de erro padrão amostral\n\n\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "manipulacao-dados-python.html",
    "href": "manipulacao-dados-python.html",
    "title": "Manipulação de Dados em Python",
    "section": "",
    "text": "Importando data frames a partir de arquivos CSV\n\n\nAprenda como montar o Google Drive no Colab e importar dados de arquivos CSV usando pandas\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "amostragem.html",
    "href": "amostragem.html",
    "title": "Amostragem",
    "section": "",
    "text": "Descrevendo populações e amostras\n\n\nDescreve populações e amostras, abordando a distinção entre parâmetros populacionais e estimadores amostrais.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmostrando uma população estatística\n\n\nMétodos de amostragem aleatória simples, estratificada e sistemática, destacando o erro amostral e a acurácia das estimativas.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "teste_hipoteses.html",
    "href": "teste_hipoteses.html",
    "title": "Teste de Hipóteses",
    "section": "",
    "text": "Introdução ao teste de hipóteses\n\n\nApresentação do teste de hipóteses, definições de hipóteses nula e alternativa, erros do tipo I/II e valor de p.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparando variâncias\n\n\nMétodos de comparação de variâncias, incluindo o teste F e o teste de Levene.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparando médias: teste t de Student\n\n\nTeste t de Student para comparação de médias, abrangendo uma amostra, grupos independentes e medidas pareadas.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "intro_bayes.html",
    "href": "intro_bayes.html",
    "title": "Introdução à Inferência Bayesiana",
    "section": "",
    "text": "Contando possibilidades\n\n\nIntrodução à contagem de possibilidades na abordagem bayesiana. Baseado em Statistical Rethinking (McElreath 2018).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe contagens a probabilidades\n\n\nTransição de contagens para probabilidades sob uma perspectiva bayesiana. Baseado em Statistical Rethinking (McElreath 2018).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruindo um modelo bayesiano\n\n\nConstrução de um modelo bayesiano, enfatizando a formulação de distribuições a priori e posterior. Baseado em Statistical Rethinking (McElreath 2018).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferência Bayesiana Binomial\n\n\nIntrodução ao conceito de aproximação por grid na abordagem bayesiana.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferência Bayesiana Binomial com PyMC\n\n\nIntrodução à modelagem probabilística na abordagem bayesiana.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelo Normal Bayesiano\n\n\nIntrodução à modelagem Bayesiana de dados contínuos, incluindo escolha das priores e checagens preditivas.\n\n\n\n\n\n\n\n\nNenhum item correspondente\nReferências\n\nMcElreath, Richard. 2018. Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman; Hall/CRC."
  },
  {
    "objectID": "regressao_linear.html",
    "href": "regressao_linear.html",
    "title": "Modelos de Regressão",
    "section": "",
    "text": "Método dos Mínimos Quadrados na Regressão Linear Simples\n\n\nMétodo dos mínimos quadrados na Regressão linear simples por meio da representação vetorial e matricial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMétodo dos Mínimos Quadrados na Regressão Linear Simples\n\n\nTutorial prático para implementar o método dos mínimos quadrados em Python, aplicando os conceitos de álgebra linear e estatística básica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMétodo dos Mínimos Quadrados na Regressão Polinomial\n\n\nTutorial prático para implementar o método dos mínimos quadrados em Python para modelos polinomiais, aplicando os conceitos de álgebra linear e estatística básica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressão linear simples\n\n\nIntrodução à regressão linear simples, incluindo ANOVA da regressão, coeficiente de determinação e diagnósticos básicos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressão linear múltipla\n\n\nRegressão linear múltipla, discutindo seleção de variáveis, pressupostos e diagnósticos de adequação do modelo.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "inferencia_estatistica.html",
    "href": "inferencia_estatistica.html",
    "title": "Inferência Estatística",
    "section": "",
    "text": "Distribuição das médias amostrais\n\n\nExposição do teorema central do limite, enfatizando a distribuição das médias amostrais e sua importância em inferência estatística.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimando a média populacional\n\n\nConstrução e interpretação de intervalos de confiança para estimar a média populacional, incluindo a distribuição t de Student.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "manipulacao-dados-R.html",
    "href": "manipulacao-dados-R.html",
    "title": "Manipulação de Dados em R",
    "section": "",
    "text": "Os pacotes em tidyverse\n\n\n\nCiência de dados\n\nR\n\nTidyverse\n\n\n\nPacotes do tidyverse para importação, organização, transformação e visualização de dados em R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportando/Exportando dados\n\n\n\nCiência de dados\n\nR\n\nTidyverse\n\nImportação e exportação de dados\n\n\n\nTécnicas de importação e exportação de dados com o pacote readr do Tidyverse, para diversos formatos de arquivo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperadores pipe\n\n\n\nCiência de dados\n\nR\n\nTidyverse\n\nTransformação de dados\n\n\n\nUso de operadores pipe para encadear funções e simplificar fluxos de dados em R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformação de Dados\n\n\n\nCiência de dados\n\nR\n\nTidyverse\n\nTransformação de dados\n\n\n\nManipulação e transformação de dados com as funções principais do Tidyverse, incluindo dplyr e tidyr.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGráficos em camadas\n\n\n\nCiência de dados\n\nR\n\nTidyverse\n\nVisualização gráfica\n\n\n\nCriação de gráficos em camadas com ggplot2, incluindo histogramas, boxplots e gráficos de dispersão.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html",
    "href": "conteudo/amostragem/pop_amostra.html",
    "title": "Descrevendo populações e amostras",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas no capítulo\n\n\n\n\n\nPacotes:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(knitr)"
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#população-amostra-e-unidade-amostral",
    "href": "conteudo/amostragem/pop_amostra.html#população-amostra-e-unidade-amostral",
    "title": "Descrevendo populações e amostras",
    "section": "1 População, amostra e unidade amostral",
    "text": "1 População, amostra e unidade amostral\nUma população estatística são todos os elementos sobre os quais queremos tirar conclusões. Refere-se ao conjunto de medidas que podem ser mensuradas como resultado de um experimento. As medidas que compõem a população estatística podem ser pesos, temperaturas, velocidades, tempos de reação, entre outras, a depender das características de um estudo particular. Uma população estatística pode ser finita ou infinita. Quando é finita, o número de elementos é dado por \\(N\\). O termo população não pode ser confundido com seu uso do dia-a-dia, quando refere-se a conjuntos de pessoas ou de organismos, nem mesmo com os elementos físicos nos quais as variáveis foram mensuradas.\nA abrangência da população estatística depende do contexto e do escopo da pergunta que se pretende responder.\n\nExemplo 1: Suponha um estudo para descrever o comprimento do lambari Deuterodon iguape em riachos do litoral de São Paulo. A população estatística não são os peixes em si, mas o comprimento de cada indivíduo que habita os riachos destas bacias. Dado o escopo do estudo (bacias do litoral de São Paulo), a população estatística abrange somente comprimentos dos organismos existem nesta região.\nExemplo 2: Suponha agora que desejamos estudar a diversidade de espécies de peixes em bacias costeiras do litoral de São Paulo. Neste caso, a população estatística seria constituida de um índice de diversidade calculado para cada uma das bacias costeiras do litoral. Fica claro que, neste caso, população estatística não se refere a população biológica, mas sim a variável que foi mensurada a partir do conjunto de espécies que habitam cada bacia.\n\nNestes dois exemplos é inviável obter informações de todos os elementos que compõem a população estaística. No caso dos comprimentos, não temos como capturar todos os animais presentes em uma bacia hidrográfica, mas ainda que tivéssemos seria inviável medir todos, pois existem provavelmente alguns milhares de peixes somente em um pequeno trecho de riacho. Já o número de Bacias costeiras no litoral do Estado de São Paulo é bem menor, porém ainda seria inviável mensurar a diversidade de espécies em todas elas.\nUm censo ocorre nos raros exemplos em que é possível mensurar todos os elementos da população estatística. Entretanto, a prática científica lida com a maioria dos casos em que mensuramos um subconjunto da população estatística, definido como uma amostra. O tamanho da amostra é denominado de \\(n\\).\nFinalmente, unidade amostral é definida como um único elemento da população estatística. A unidade amostral é uma determinada observação da variável de interesse. No exemplo dos lambaris, unidade amostral é o comprimento mensurado em um indivíduo da espécie de interesse, enquanto no exemplo das bacias costeiras, as unidades amostrais são cada um dos valores de diversidade calculados para cada bacia costeira.\n\n\n\n\n\n\nDEFINIÇÕES\n\n\n\nPopulação estatística: todos os elementos que podem compor uma amostra. Podem ser medidas como comprimentos, temperaturas, velocidades, etc.\nUnidade amostral: um único elemento da população.\nCenso: o levantamento de todos os elementos da população.\nAmostra: um subconjunto extraído da população.\nTamanho populacional (N): o número de elementos da população.\nTamanho amostral (n): o número de elementos da amostra."
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#distribuição-de-frequências-na-população-estatística",
    "href": "conteudo/amostragem/pop_amostra.html#distribuição-de-frequências-na-população-estatística",
    "title": "Descrevendo populações e amostras",
    "section": "2 Distribuição de frequências na população estatística",
    "text": "2 Distribuição de frequências na população estatística\nOs valores em uma população estatística não são idênticos, de modo que poderíamos descrevê-los por meio de uma distribuição de frequência, em que algumas faixas de valores são mais frequentes que outras. Os comprimentos de Deuterodon iguape por exemplo devem variar de alguns milímetros (pós-larva) a cerca de 20 cm (adulto), em que nem todos os comprimentos são igualmente representados. Certamente, existem mais lambaris pequenos e médios do que lambaris grandes. De fato, animais muito grandes são os mais raros, de modo que se tivéssemos informação da população estatística, veríamos que faixas de valores muito elevados se tornariam cada vez menos frequentes.\nSe fosse possível observar todos os elementos da população estatística, saberíamos exatamente qual o formato de sua distribuição de frequências. Suponha por exemplo, a altura de adultos acima de 18 anos. Seria razoável supor que a maioria das alturas consiste de valores intermediários ao redor de, por exemplo, 170 centímetros. É razoável supor também que a frequência de pessoas muito altas ou muito baixas vai diminuindo gradativamente, de modo que é muito raro encontrarmos adultos muito altos (ex. acima de \\(200\\) centímetros) ou muito baixos (ex. menores que \\(150\\) centímetros). A figura abaixo, descreve uma distribuição de frequência de uma população fictícia de exatamente \\(N = 1000\\) alturas.\n\n\nCódigo\nmu = 170\nsd = 10\nN = 1000\nset.seed(1)\nadultos = data.frame(\n  CP = round(rnorm(n = N, mean = mu, sd = sd),2)\n  )\n\nplt_pop = ggplot(adultos, aes(x = CP)) +\n  geom_histogram(fill = 'dodgerblue4', \n                 color = 'black', bins = 20) +\n   labs(x = \"Alturas em centímetros\",\n        y = \"Frequência\") +\n  scale_x_continuous(breaks = seq(130, 220, by = 10)) +\n  scale_y_continuous(breaks = seq(0, 200, by = 20)) +\n  coord_cartesian(ylim = c(0, 200), xlim = c(130, 220)) +\n  theme_classic(base_size = 15)\n\n\n\n\nCódigo\nplt_pop +\n  annotate(geom = 'text', x = 130, y = 175, \n           label = deparse(bquote('N' == .(N))), parse = TRUE, hjust = 0, size = 7) +\n  annotate(geom = 'text', x = 130, y = 155, \n           label = deparse(bquote(mu == .(mu) ~ 'm')), parse = TRUE, hjust = 0, size = 7)\n\n\n\n\n\n\n\n\nFigura 1: Distribuição de uma população estatística representando as alturas (em centímetros) de adultos acima de 18 anos.\n\n\n\n\n\nVemos que existem mais valores entre \\(160\\) e \\(180\\) e poucas observações extremas. Por exemplo, das \\(1000\\) observações, apenas 27 mais extremas que \\(190\\) cm, o que é condizente com nossa expectativa para a distribuição de frequências das alturas de indivíduos adultos."
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#distribuição-de-probabilidade-da-população-estatística",
    "href": "conteudo/amostragem/pop_amostra.html#distribuição-de-probabilidade-da-população-estatística",
    "title": "Descrevendo populações e amostras",
    "section": "3 Distribuição de probabilidade da população estatística",
    "text": "3 Distribuição de probabilidade da população estatística\nNa prática, como não temos acesso a toda a população estatística, não temos como visualizar toda a sua distribuição de frequência. Dizemos portanto, que conhecemos a população estatística quando conhecemos a função de probabilidades associada a variável que está sendo mensurada. No exemplo da Figura 1, diríamos que a variável altura segue uma distribuição normal de probabilidades. Quando descrevemos uma distribuição de probabilidades, precisamos caracterizá-la por meio de certas quantidades, ou parâmetros da distribuição. Na distribuição normal, os parâmetros de interesse são a média \\(\\mu\\) e o desvio padrão \\(\\sigma\\). No exemplo das alturas, \\(\\mu = 170\\) e \\(\\sigma = 10\\)."
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#distribuições-de-frequências-na-amostra",
    "href": "conteudo/amostragem/pop_amostra.html#distribuições-de-frequências-na-amostra",
    "title": "Descrevendo populações e amostras",
    "section": "4 Distribuições de frequências na amostra",
    "text": "4 Distribuições de frequências na amostra\n\n\nCódigo\nn = 50\nset.seed(2)\nselecao = sample(N, size = n)\nAm1 = sort(adultos$CP[selecao], decreasing = FALSE)\n\n\nAinda que não tenhamos acesso a toda população estatística, gostaríamos de ter informações sobre a variável de interesse. Utilizamos o processo de amostragem para obter estas informações.\nSuponha, uma amostra de \\(n = 50\\) adultos. Se organizarmos esta amostra em valores crescentes teríamos:\n140.03, 151.5, 152.13, 152.91, 154.07, 158.14, 158.32, 159.59, 159.69, 160.14, 160.42, 160.46, 161.48, 161.89, 162.05, 163.11, 163.59, 163.77, 165.36, 166.11, 166.38, 166.69, 166.76, 167.03, 168.55, 169.72, 170.56, 172.17, 172.94, 173.8, 173.92, 174.34, 174.5, 175.24, 175.76, 175.95, 176.16, 177.13, 177.63, 177.66, 178.03, 178.48, 180.96, 180.97, 183.94, 184.17, 187.54, 187.64, 188.87, 191.69\nOs valores estão entre \\(140.03\\) e \\(191.69\\), o que certamente não é igual aos valores máximos e mínimos da população. Vamos representar esta amostra por meio de um histograma.\n\n\nCódigo\namostra_df = data.frame(CP = adultos$CP[selecao]) \nggplot(amostra_df, aes(x = CP)) +\n  geom_histogram(fill = 'dodgerblue4', color = 'black', bins = 15) +\n   labs(x = \"Alturas em centímetros\",\n        y = \"Frequência\") +\n  scale_x_continuous(breaks = seq(130, 220, by = 10)) +\n  scale_y_continuous(breaks = seq(0, 10, by = 1)) +\n  coord_cartesian(xlim = c(130, 220), ylim = c(0, 10)) +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nFigura 2: Amostra de tamanho n = 50 da população estatística de alturas.\n\n\n\n\n\nAinda que a distribuição da amostra não seja igual à da população estatística, podemos perceber que há uma concentração de valores justamente entre \\(160\\) cm e \\(180\\) cm, assim como na população estatística.\nA diferença entre a distribuição da população e a distribuição da amostra é esperada e ocorre porque estamos observando um subconjunto particular de elementos. Deste modo, sempre que amostrarmos uma população estatística, teremos uma amostra ligeiramente diferente.\nVamos verificar por exemplo, as distribuições de frequência de seis amostras possíves de tamanho \\(n = 50\\) desta mesma população.\n\n\nCódigo\nplot_list = list()\nfor (i in 1:6){\n  df = slice_sample(adultos, n = n)\n  p = ggplot(df, aes(x = CP)) +\n    geom_histogram(fill = 'dodgerblue4', color = 'black', bins = 10) +\n    labs(x = \"Alturas em centímetros\",\n         y = \"Frequência\") +\n  scale_x_continuous(breaks = seq(130, 220, by = 20)) +\n  scale_y_continuous(breaks = seq(0, 16, by = 2)) +\n  coord_cartesian(xlim = c(130, 220), ylim = c(0, 16)) +\n    theme_classic()\n  plot_list[[i]] = p\n}\n\n(plot_list[[1]] + plot_list[[2]] + plot_list[[3]]) /\n  (plot_list[[4]] + plot_list[[5]] + plot_list[[6]])\n\n\n\n\n\n\n\n\nFigura 3: Seis diferentes amostra de tamanho n = 50 da população estatística de alturas.\n\n\n\n\n\nCada amostra resulta em distribuições diferentes, mas em todas a frequência de observações na faixa intermediária é maior. O processo de amostragem nos forneceu portanto amostras representativas da população estatística, isto é, amostras em que a distribuição de frequências se aproximou da distribuição de frequências da população.\nNeste exemplo fictício, como conhecemos a população estatística é fácil verificar que as amostras foram representativas. Na prática científica não conhecemos a população estatística e, consequentemente, não temos como saber se a nossa amostra em particular foi ou não representativa.\nDevemos portanto conduzir o processo de tal forma que a teoria da amostragem nos garanta que a amostra resultante de um determinado experimento seja, em média, representativa da população. O modo mais simples de garantir este comportamento é realizarmos uma amostra aleatória dos elementos da população estatística."
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#parâmetros-e-estimadores",
    "href": "conteudo/amostragem/pop_amostra.html#parâmetros-e-estimadores",
    "title": "Descrevendo populações e amostras",
    "section": "5 Parâmetros e estimadores",
    "text": "5 Parâmetros e estimadores\nA população estatística tem determinadas quantias de interesse que definimos como parâmetros da população. Se fosse possível medir as alturas dos \\(N = 1000\\) adultos, poderíamos calcular a média da população. Seja uma variável \\(X\\) composta por \\(X_1, X_2, X_3, \\cdots , X_N,\\), a média da população estatística é denominada de \\(\\mu\\) e definida por:\n\\[\\mu=\\frac{X_1+X_2+X_3+\\cdots+X_N}{N}=\\frac{\\sum_{i=1}^N{X_i}}{N}\\] Como não temos acesso a toda a população não podemos obter \\(\\mu\\), mas podemos estimá-lo por meio de uma amostra. Neste caso, seja uma amostra de tamanho \\(n\\) composta por \\(X_1, X_2, X_3, \\cdots, X_n\\), a média da amosta é denominada de \\(\\overline{X}\\) e definida por:\n\\[\\overline{X}=\\frac{X_1+X_2+X_3+\\cdots+X_n}{n}=\\frac{\\sum_{i=1}^n{X_i}}{n}\\]\nDizemos que \\(\\overline{X}\\) é um estimador não viciado de \\(\\mu\\).\nComo os valores da população estatística não são idênticos, podemos obter uma medida de dispersão como a variância populacional (\\(\\sigma^2\\)) definida por:\n\\[\\sigma^2=\\frac{\\sum_{i=1}^N{(X_i - \\mu)^2}}{N}\\]\nNovamente, como não temos acesso a todos os \\(N\\) elementos, podemos apenas calcular a variância amostral (\\(s^2\\)) definida por:\n\\[s^2=\\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1}\\]\nNote que na expressão acima, substituimos \\(\\mu\\) por \\(\\overline{X}\\) pois estamos nos referindo à variância da amostra. No denominador fizemos a divisão por \\(n-1\\) não por \\(N\\). Estas mudanças são necessárias para que \\(s^2\\) seja um estimador não-viciado de \\(\\sigma^2\\).\nDenominamos de parâmetro ao descritor obtido a partir da mensuração de todos os elementos da população estatística e de estimador (ou estatística), a quantia obtida a partir da amostra. Os parâmetros são comumente representados por letras gregas. O símbolo \\(\\mu\\) e \\(\\sigma^2\\) representam, respectivamente, a média e variância populacionais, enquanto \\(\\overline{X}\\) e \\(s^2\\) são a média e variância amostrais.\n\n\n\n\n\n\nDEFINIÇÕES\n\n\n\nParâmetro: a medida que descreve uma característica da população estatística. Ex.: a média (\\(\\mu\\)) ou a variância (\\(\\sigma^2\\)) populacional.\nEstimador ou Estatística: Uma medida que descreve uma característica da amostra. Ex.: a média amostral (\\(\\overline{X}\\)) ou a variância amostral (\\(s^2\\)). Os estimadores támbem podem ser representados por letras gregas com o símbolo \\(\\hat{}\\). A variância amostral, por exemplo, pode ser representada por \\(\\hat{\\sigma}^2\\).\nEstimativa: é o valor numérico assumido pelo estimador. Ex. o valor numérico calculado para a média ou variância de uma amostra em particular.\n\n\n\n5.1 Verificando as propriedades de \\(\\overline{X}\\) e \\(s^2\\)\n\n\nCódigo\nNsmall = 5\nnsmall = 2\nset.seed(5)\npop_small = sample(20, size = Nsmall, replace = F)\nmu = mean(pop_small)\nsigma2 = sum((pop_small - mu)^2)/Nsmall\n\nm = matrix(NA, ncol = Nsmall, nrow = Nsmall)\nrownames(m) = colnames(m) = as.character(pop_small)\nfor (i in 1:Nsmall){\n  for (j in 1:Nsmall){\n    m[i,j] = paste('(', pop_small[i], ' ; ', pop_small[j], ')', sep = '')\n  }\n}\n\n\nPor meio de um exmplo, vamos verificar empiricamente que \\(\\overline{X}\\) e \\(s^2\\) são estimadores não viciados de \\(\\mu\\) e \\(\\sigma^2\\) respectivamente. Suponha uma população de somente \\(5\\) elementos.\n\\[2 - 11 - 15 - 19 - 9\\]\n1. Calculando \\(\\mu\\) e \\(\\sigma^2\\).\nComo conhecemos a população vamos obter:\n\\[\\mu = \\frac{\\sum_{i=1}^N{X_i}}{N} = \\frac{2 + 11 + 15 + 19 + 9}{5}= \\frac{5}{5} = 11.2\\]\ne\n\\[\\sigma^2=\\frac{\\sum_{i=1}^N{(X_i - \\mu)^2}}{N} = \\frac{164.8}{5} = 32.96\\]\n2. Amostrando a população estatística\nA Tabela 1 mostra todas as \\(25\\) amostras com reposição de tamanho \\(n = 2\\) que podem ser obtidas desta população.\nA tabela abaixo mostra todas as \\(25\\) amostras com reposição de tamanho \\(n = 2\\) que podem ser obtidas desta população.\n\n\nCódigo\nknitr::kable(m)\n\n\n\n\nTabela 1: Todas as amostras possíveis de tamanho n = 2 da população estatística com N = 5\n\n\n\n\n\n\n\n2\n11\n15\n19\n9\n\n\n\n\n2\n(2 ; 2)\n(2 ; 11)\n(2 ; 15)\n(2 ; 19)\n(2 ; 9)\n\n\n11\n(11 ; 2)\n(11 ; 11)\n(11 ; 15)\n(11 ; 19)\n(11 ; 9)\n\n\n15\n(15 ; 2)\n(15 ; 11)\n(15 ; 15)\n(15 ; 19)\n(15 ; 9)\n\n\n19\n(19 ; 2)\n(19 ; 11)\n(19 ; 15)\n(19 ; 19)\n(19 ; 9)\n\n\n9\n(9 ; 2)\n(9 ; 11)\n(9 ; 15)\n(9 ; 19)\n(9 ; 9)\n\n\n\n\n\n\n\n\n3. Calculando \\(\\overline{X}\\) e \\(s^2\\).\nEm seguida, organizamos as amostras em uma outra tabela, de modo que possamos calcular, para cada uma, os estimadores \\(\\overline{X}\\) e \\(s^2\\)\n\n\nCódigo\ntab_df = data.frame(expand_grid(pop_small, pop_small))\ncolnames(tab_df) = c('X1', 'X2')\ntab_df = tab_df |&gt; \n  rowwise() |&gt; \n  mutate(Xm = mean(c(X1,X2)),\n         s2 = var(c(X1,X2)))\nEx = mean(tab_df$Xm)\nEs2 = mean(tab_df$s2)\n\n\n\n\nCódigo\ntab_df |&gt; \n  knitr::kable(col.names = c('$x_1$', '$x_2$', '$\\\\overline{X}$', '$s^2$'))\n\n\n\n\nTabela 2: Média e variância amostrais para todas as amostras possíveis de tamanho n = 2 da população estatística com N = 5\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(\\overline{X}\\)\n\\(s^2\\)\n\n\n\n\n2\n2\n2.0\n0.0\n\n\n2\n11\n6.5\n40.5\n\n\n2\n15\n8.5\n84.5\n\n\n2\n19\n10.5\n144.5\n\n\n2\n9\n5.5\n24.5\n\n\n11\n2\n6.5\n40.5\n\n\n11\n11\n11.0\n0.0\n\n\n11\n15\n13.0\n8.0\n\n\n11\n19\n15.0\n32.0\n\n\n11\n9\n10.0\n2.0\n\n\n15\n2\n8.5\n84.5\n\n\n15\n11\n13.0\n8.0\n\n\n15\n15\n15.0\n0.0\n\n\n15\n19\n17.0\n8.0\n\n\n15\n9\n12.0\n18.0\n\n\n19\n2\n10.5\n144.5\n\n\n19\n11\n15.0\n32.0\n\n\n19\n15\n17.0\n8.0\n\n\n19\n19\n19.0\n0.0\n\n\n19\n9\n14.0\n50.0\n\n\n9\n2\n5.5\n24.5\n\n\n9\n11\n10.0\n2.0\n\n\n9\n15\n12.0\n18.0\n\n\n9\n19\n14.0\n50.0\n\n\n9\n9\n9.0\n0.0\n\n\n\n\n\n\n\n\nVemos que a média amostral pode variar entre 2, quando amostramos o menor valor da população duas vezes (isto é, o número 2) e 19, quando a amostra contém o maior valor da população (isto é, 19). Vemos ainda que existe uma maior concentração de valores entre \\(8\\) e \\(14\\) e que a distribuição das médias é aproximadamente simétrica.\nPara a variância amostral \\(s^2\\) também verificamos uma grande diferença entre as amostras particulares, com resultados que varia entre 0 e 144.5, além de uma distribuição altamente assimétrica.\n\n\nCódigo\nplt_Xm = ggplot(tab_df, aes(x = Xm)) +\n  geom_histogram(fill = 'dodgerblue4', color = 'black', bins = 10) +\n   labs(x = bquote('Distribuição de ' ~ bar(X)),\n        y = \"Frequência\") +\n  scale_x_continuous(breaks = seq(0, 20, by = 2)) +\n  theme_classic(base_size = 15)\n\nplt_s2 = ggplot(tab_df, aes(x = s2)) +\n  geom_histogram(fill = 'red4', color = 'black', bins = 12) +\n   labs(x = bquote('Distribuição de ' ~ s^2),\n        y = \"Frequência\") +\n  scale_x_continuous(breaks = seq(0, 200, by = 20)) +\n  theme_classic(base_size = 15)\n\n\n\n\nCódigo\nplt_Xm | plt_s2\n\n\n\n\n\n\n\n\nFigura 4: Distribuição das médias e variâncias amostrais com n = 2\n\n\n\n\n\n3. Verificando os valores esperados de \\(\\overline{X}\\) e \\(s^2\\).\nPara verificar empiricamente os valores esperados de \\(\\overline{X}\\) e \\(s^2\\) podemos encontrar sua médias. Vemos que:\n\\[\\overline{\\overline{X}} = \\frac{\\sum_{i=1}^{25}{280}}{25} = 11.2 = \\mu\\]\ne que\n\\[\\overline{s^2} = \\frac{\\sum_{i=1}^{25}{824}}{25} = 32.96 = \\sigma^2\\]\nEstes resultados mostram que, em média, espera-se que as estimativas de \\(\\overline{X}\\) e \\(s^2\\) coincidem exatamente com os parâmetros \\(\\mu\\) e \\(\\sigma^2\\). Quando isto ocorre dizemos que o estimador é não viciado.\n\n\n\n\n\n\nAmostragem com e sem reposição\n\n\n\nA discussão acima é válida para a amostragem de uma população infinita ou para a amostragem com reposição de uma população finita de tamanho \\(N\\). Se a amostragem for feita sem reposição de uma população finita, \\(\\overline{X}\\) continua sendo o estimador não viciado de \\(\\mu\\), porém o estimador não viciado da variância fica:\n\\(s^2 = \\left( \\frac{N-1}{N} \\right) \\left( \\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1} \\right)\\)\nNa prática, raramente conduzimos uma amostragem com reposição. No entanto, ou a população é infinita como nos casos de estudos experimentais, ou a população é finita porém muito grande, como na maioria dos estudos observacionais. Neste segundo caso, para populações finitas com \\(N\\) grande, o termo \\(\\left( \\frac{N-1}{N}  \\right)\\sim 1\\)."
  },
  {
    "objectID": "conteudo/amostragem/pop_amostra.html#amostragem-e-inferência",
    "href": "conteudo/amostragem/pop_amostra.html#amostragem-e-inferência",
    "title": "Descrevendo populações e amostras",
    "section": "6 Amostragem e inferência",
    "text": "6 Amostragem e inferência\nO problema central que começamos e discutir neste capítulo e com o qual iremos lidar em estatística é que:\n\nEstamos interessados nas características da população estatística, porém só temos informação sobre a amostra.\nA estimativa obtida a partir de uma amostra particular é sujeita à variação decorrente do processo de amostragem.\n\nConsidere por exemplo, as diferentes amostras que podem ser obtidas a partir da população estatística de alturas para um \\(n = 50\\) amostras:\n\n\nCódigo\nplot_list = list()\nfor (i in 1:6){\n  df = slice_sample(adultos, n = n)\n  x_bar = mean(df$CP)\n  s2_bar = var(df$CP)\n  p = ggplot(df, aes(x = CP)) +\n    geom_histogram(fill = 'dodgerblue4', color = 'black', bins = 10) +\n    labs(x = \"Alturas em centímetros\",\n         y = \"Frequência\") +\n    scale_x_continuous(breaks = seq(130, 220, by = 20)) +\n    scale_y_continuous(breaks = seq(0, 16, by = 2)) +\n    coord_cartesian(xlim = c(130, 220), ylim = c(0, 16)) +\n    annotate(geom = 'text', x = 130, y = 15, label = deparse(bquote('n' == .(n))), parse = TRUE, hjust = 0, size = 3) +\n    annotate(geom = 'text', x = 130, y = 14, label = deparse(bquote(bar(X) == .(round(x_bar,2)))), parse = TRUE, hjust = 0, size = 3) +\n    annotate(geom = 'text', x = 130, y = 13, label = deparse(bquote(s^2 == .(round(s2_bar,2)))), parse = TRUE, hjust = 0, size = 3) +\n    theme_classic()\n  plot_list[[i]] = p\n}\n\n(plot_list[[1]] + plot_list[[2]] + plot_list[[3]]) /\n  (plot_list[[4]] + plot_list[[5]] + plot_list[[6]])\n\n\n\n\n\n\n\n\nFigura 5: Seis diferentes amostras de tamanho n = 50 da população de alturas.\n\n\n\n\n\nVemos que a cada nova amostra, \\(\\overline{X}\\) e \\(s^2\\) são numericamente diferentes e não coicidem com os parâmetros da população estatística (\\(\\mu = 11.2\\), \\(\\sigma^2 = 100\\)). Vemos entretanto, que mesmo sendo diferentes, estão ao redor dos parâmetros populacionais. Se pudermos conhecer algumas propriedades destes estimadores, seremos capazes de estabelecer limites de confiança sobre as conclusões que podemos tirar as respeito da população estatística.\nNeste sentido, o processo de amostragem e inferência consiste em:\n\nObter uma amostra representativa da população estatística;\nCalcular estimativas a partir das características da amostra (ex. \\(\\overline{X}\\) e \\(s^2\\));\nAssumir distribuições de probabilidade apropriadas para os estimadores;\nUtilizar para estas distribuições para calcular intervalos de confiança ou testar hipóteses estatísticas.\n\nEste processo pode ser resumido na figura abaixo e será discutido nos próximos capítulos.\n\n\n\n\n\n\nFigura 6: Processo de amostragem e inferência estatística.\n\n\n\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana.html",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana.html",
    "title": "Regressão Linear Bayesiana",
    "section": "",
    "text": "Na inferência bayesiana, atualizamos nossas crenças sobre os parâmetros de um modelo combinando o conhecimento prévio (expresso pela distribuição a priori) com a informação contida nos dados observados (expressa pela verossimilhança) para obter a distribuição a posteriori. A inferência bayesiana fornece uma distribuição completa de probabilidade para os parâmetros, refletindo explicitamente a incerteza sobre seus valores.\nNo modelo de regressão linear bayesiano, assumimos que a variável resposta \\(y\\) é uma variável aleatória com distribuição Normal, cuja média depende linearmente de uma variável preditora \\(x\\), ou seja, \\(\\mu = \\beta_0 + \\beta_1 x\\), e com desvio padrão \\(\\sigma\\).\n\\[\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma)\n\\tag{1}\\]\nOnde:\nA especificação completa do modelo bayesiano requer a definição das distribuições a priori para os parâmetros \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma\\). Podemos assumir, por exemplo, distribuições normais para os coeficientes da regressão e uma distribuição Lognormal para o desvio padrão, garantindo que \\(\\sigma\\) assuma apenas valores positivos. As distribuições a priori são então:\n\\[\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n\\tag{2}\\]\n\\[\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n\\tag{3}\\]\n\\[\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n\\tag{4}\\]"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana.html#atividate-prática",
    "href": "conteudo/modelos_regressao_bayes/regressao-linear-bayesiana.html#atividate-prática",
    "title": "Regressão Linear Bayesiana",
    "section": "1 Atividate prática",
    "text": "1 Atividate prática\n\n\n\n\n\n\nObjetivos de Aprendizagem\n\n\n\n\nCompreender os fundamentos da regressão linear sob a abordagem bayesiana.\nSimular dados utilizando a biblioteca SciPy.\nAplicar conhecimento prévio para especificar distribuições a priori informativas para os parâmetros do modelo.\nImplementar um modelo de regressão linear bayesiana com PyMC, realizar a checagem preditiva a priori e ajustá-lo a dados reais para obter as distribuições a posteriori dos parâmetros.\nInterpretar e validar os resultados da inferência bayesiana.\n\n\n\nNesta atividade, aplicaremos a inferência bayesiana para modelar a relação entre duas variáveis contínuas: a altura de indivíduos e o número do calçado que utilizam. O conjunto de dados de altura (cm) e número do calçado está disponível no link: altura_adultos.csv.\nIntuitivamente, esperamos que haja uma relação positiva: pessoas com pés maiores tendem a ser mais altas. Para quantificar essa relação, utilizaremos um modelo de regressão linear. O co\n\n# Configuração inicial e importação de bibliotecas\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, lognorm\nimport pandas as pd\nimport seaborn as sns\nimport arviz as az\n\n\n\n1.1 Escolhendo as prioris\nDesenvolver uma boa intuição sobre os parâmetros é essencial para construir modelos coerentes com o conhecimento prévio. Esta atividade tem como finalidade apoiar a definição informada das distribuições a priori no modelo bayesiano, de modo que reflitam o que sabemos (ou assumimos saber) sobre os parâmetros antes de observar os dados. O objetivo é explorar diferentes valores para os parâmetros da regressão linear e identificar combinações que representem, de forma realista, a relação esperada entre essas duas variáveis, e que possam ser utilizadas como prioris no modelo.\n1. Escolha valores para os parâmetros \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma\\) (Equação 1).\n\n\\(\\beta_0\\) (intercepto): representa altura esperada quando o número do calçado é 0. Embora esse valor não tenha significado físico direto, ele influencia a posição da reta ajustada.\n\\(\\beta_1\\) (inclinação da reta): representa a variação média na altura para cada número a mais de calçado.\n\\(\\sigma\\) (desvio padrão): representa a variação natural nas alturas entre pessoas com o mesmo número de calçado.\n\n\n# Parâmetros para simulação\nbeta_0 =       # ESCOLHA a altura base (quando o número do calçado é zero)\nbeta_1 =      # ESCOLHA a taxa média de aumento na altura para cada número a mais de calçado\nsigma =         # ESCOLHA a variação individual na altura (desvio padrão dos erros)\n\n2. Crie uma sequência de valores para \\(x\\) abrangendo limites coerentes com número do calçado para indivíduos adultos e utilize a função norm.rvs da biblioteca SciPy para gerar dados simulados de altura com base no número do calçado.\n\n# Simule o número do calçado (ex.: valores inteiros de 33 a 48, com 100 repetições por número)\nx_sim = np.repeat(np.arange(33, 49), 100)\n\n# Gere as alturas simuladas com erro normal\nmu = beta_0 + beta_1 * x_sim\ny_sim = norm.rvs(loc=mu, scale=sigma, size=len(x_sim))\n\n3. Utilize o matplotlib para visualizar os dados simulados. O gráfico de dispersão mostrará a altura em função do número do calçado. Isso ajudará a avaliar se a simulação é coerente com sua expectativa sobre essa relação.\n\n# Use o matplotlib para plotar o resultado da simulação, isto é, altura_sim em função de x_sim\nplt.figure(figsize=(9, 6))\nplt.scatter(x_sim, y_sim, color='steelblue', alpha=0.6, label=\"Alturas simuladas\")\nplt.xlabel(\"Número do calçado\")\nplt.ylabel(\"Altura (cm)\")\nplt.title(\"Relação simulada entre número do calçado e altura\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n4. Ajuste os valores de \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma\\) e repita a simulação até obter uma distribuição de pontos que represente adequadamente sua espectativa sobre a relação entre as variáveis.\n\n\n\n1.2 Implementando distribuições a priori no PyMC\nAgora que você já explorou os efeitos dos parâmetros \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma\\), o próximo passo é formalizar esse conhecimento na distribuições a priori (Equações 2, 3 e 4). Para isso, vamos utilizar a biblioteca de programação probabilística PyMC.\n1. Defina as distribuições a priori\nUtilize os valores escolhidos anteriormente como os centros das distribuições a priori, isto é, utilize beta_0, beta_1 e sigma respectivamente para representar \\(\\mu_{\\beta_0}\\) (Equação 2), \\(\\mu_{\\beta_1}\\) (Equação 3) e \\(\\mu_{\\log \\sigma}\\) (Equação 4). A implementação em PyMC tem por objetivo facilitar a escolha de valores razoáveis para \\(\\sigma_{\\beta_0}\\) (Equação 2), \\(\\sigma_{\\beta_1}\\) (Equação 3) e \\(\\sigma_{\\log \\sigma}\\) (Equação 4) compatíveis com seu grau de incerteza sobre estes parâmetros.\n\n# Geração de valores simulados para a variável preditora (calcado)\ncalcado_sim = np.arange(33, 49)\n\n# ESCOLHA altura base (quando o número do calçado é zero)\nmu_beta_0 =  # Média\nsd_beta_0 =  # Desvio padrão\n\n# ESCOLHA a taxa média de aumento na altura para cada número a mais de calçad\nmu_beta_1 =   # Média\nsd_beta_1 =   # Desvio padrão\n\n# ESCOLHA a variação individual na altura (desvio padrão dos erros)\nmu_lsigma =   # Média\nsd_lsigma =   # Desvio padrão\n\n\nn_samples = 1000\nwith pm.Model() as modelo_regressao_linear:\n\n    # Prioris\n    beta_0 = pm.Normal(\"beta_0\", mu=mu_beta_0, sigma=sd_beta_0) \n    beta_1 = pm.Normal(\"beta_1\", mu=mu_beta_1, sigma=sd_beta_1)\n    sigma = pm.Lognormal(\"sigma\", mu=np.log(mu_lsigma), sigma=sd_lsigma)\n\n    # Verossimilhança\n    mu = beta_0 + beta_1 * calcado_sim\n    altura_sim = pm.Normal(\"altura_sim\", mu=mu, sigma=sigma, shape=len(calcado_sim))\n\n    # Amostragem da distribuição preditiva a priori\n    prior_predictive_samples = pm.sample_prior_predictive(samples=n_samples)\n\n2. Checagem preditiva a priori: Extraia as distribuições a priori dos parâmetros\n\n# Extração das distribuições a priori dos parâmetros\nbeta_0_prior = prior_predictive_samples.prior[\"beta_0\"].values.flatten()\nbeta_1_prior = prior_predictive_samples.prior[\"beta_1\"].values.flatten()\nsigma_prior = prior_predictive_samples.prior[\"sigma\"].values.flatten()\n\n# Extração da distribuição preditiva de y\naltura_sim_prior = prior_predictive_samples.prior[\"altura_sim\"].values.flatten()\n\n# Repita calcado_sim para alinhar com os n_samples valores de altura_sim_prior\ncalcado_sim_rep = np.tile(calcado_sim, n_samples)\n\n3. Verifique os histogramas das distribuições a priori e a distribuição preditiva com os dados simulados\n\n\n\n# Plot dos histogramas e do gráfico de dispersão\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\n# Histograma do beta_0\naxes[0, 0].hist(beta_0_prior, bins=30, color='lightcoral', edgecolor='black')\naxes[0, 0].set_title(\"Intercepto: β₀\")\naxes[0, 0].set_xlabel(\"β₀\")\naxes[0, 0].set_ylabel(\"Frequência\")\n\n# Histograma do beta_1\naxes[0, 1].hist(beta_1_prior, bins=30, color='cornflowerblue', edgecolor='black')\naxes[0, 1].set_title(\"Inclinação: β₁\")\naxes[0, 1].set_xlabel(\"β₁\")\naxes[0, 1].set_ylabel(\"Frequência\")\n\n# Histograma de sigma\naxes[1, 0].hist(sigma_prior, bins=30, color='mediumseagreen', edgecolor='black')\naxes[1, 0].set_title(\"Desvio padrão: σ\")\naxes[1, 0].set_xlabel(\"σ\")\naxes[1, 0].set_ylabel(\"Frequência\")\n\n# Gráfico de dispersão dos dados simulados anteriormente\naxes[1, 1].scatter(calcado_sim_rep, altura_sim_prior, color='steelblue', alpha=0.6, label=\"Alturas simuladas\")\naxes[1, 1].set_title(\"Relação a priori predita\")\naxes[1, 1].set_xlabel(\"Número do calçado\")\naxes[1, 1].set_ylabel(\"Altura (cm)\")\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\nFigura 1\n\n\n\n4. Ajuste os valores dos parâmetros e repita a implementação do modelo até obter uma distribuição de pontos que represente adequadamente sua espectativa sobre a relação entre as variáveis.\n\n\n1.3 Ajustando o modelo a dados reais\n1. Importe os dados altura_adultos.csv\n\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/altura_adultos.csv')\ndf\n\nOs dados contém informações sobre altura (cm), número do calcado e ano de adultos.\n2. Faça um gráfico de dispersão entre altura (\\(y\\)) e calcado (\\(x\\)).\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='calcado', y='altura', \n                alpha=0.7,           # transparência dos pontos\n                s=60,                # tamanho dos pontos\n                color='firebrick')   # cor dos pontos\n\nplt.title('Relação entre Número do Calçado e Altura', fontsize=14, fontweight='bold')\nplt.xlabel('Número do Calçado', fontsize=12)\nplt.ylabel('Altura (cm)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n3. Implemente os dados no PyMC para estimar as distribuições posteriores\nDADOS DE ENTRADA\n\n# ENTRE com os parâmetros das prioris\nmu_beta_0 = \nsd_beta_0 = \n\nmu_beta_1 = \nsd_beta_1 = \n\nmu_lsigma = \nsd_lsigma = \n\n# Dados observados\nX = df['calcado']\nY = df['altura']\n\nIMPLEMENTAÇÃO EM PYMC\n\nwith pm.Model() as modelo_regressao_linear:\n    \n    # Priori\n    beta_0 = pm.Normal(\"beta_0\", mu=mu_beta_0, sigma=sd_beta_0)\n    beta_1 = pm.Normal(\"beta_1\", mu=mu_beta_1, sigma=sd_beta_1)\n    sigma = pm.Lognormal(\"sigma\", mu=np.log(mu_lsigma), sigma=sd_lsigma)\n\n    # Verossimilhança\n    mu = beta_0 + beta_1 * calcado_sim # Equação da reta (modelo preditivo)\n    altura_obs = pm.Normal(\"altura_obs\", mu=beta_0 + beta_1 * X, \n                           sigma=sigma, observed = Y)\n    \n    # Amostragem MCMC para estimar a posterior e da distribuição preditiva posterior\n    trace = pm.sample(draws=1000, tune=1000, chains=4, target_accept=0.95)\n    posterior_predictive_samples = pm.sample_posterior_predictive(trace)\n\n4. Resultados do ajuste\n4.1. Resumo dos parâmetros posteriores\n\naz.summary(trace)\n\n\n4.2. Gráficos de diagnóstico\n\nfig, axes = plt.subplots(3, 2, figsize=(8, 6))\n\n# Trace plots\naz.plot_trace(trace, var_names=['beta_0', 'beta_1', 'sigma'], axes=axes)\nplt.suptitle('Trace Plots - Convergência das Cadeias MCMC')\nplt.tight_layout()\nplt.show()\n\n\n4.3. Distribuições posteriores\n\naz.plot_posterior(trace, var_names=['beta_0', 'beta_1', 'sigma'], \n                 hdi_prob=0.95, figsize=(8, 4))\nplt.suptitle('Distribuições Posteriores dos Parâmetros')\nplt.show()\n\n\n4.4. Ajuste do modelo (dados observados vs predições)\nPredições\n\n# Intervalo de credibilidade das predições\ncalcado_range = np.linspace(X.min(), X.max(), 100)\nposterior_beta_0 = trace.posterior['beta_0'].values.flatten()\nposterior_beta_1 = trace.posterior['beta_1'].values.flatten()\n\n# Calculando intervalos de credibilidade para a linha de regressão\npredictions = []\nfor x in calcado_range:\n    pred = posterior_beta_0 + posterior_beta_1 * x\n    predictions.append(pred)\n\npredictions = np.array(predictions)\npred_mean = np.mean(predictions, axis=1)\npred_lower = np.percentile(predictions, 2.5, axis=1)\npred_upper = np.percentile(predictions, 97.5, axis=1)\n\n Gráfico de valores preditos\n\nplt.figure(figsize=(8, 6))\n\nplt.scatter(X, Y, alpha=0.6, label='Dados Observados', color = 'firebrick')\nplt.plot(calcado_range, pred_mean, color = 'darkgreen', label='Regressão (Média Posterior)', \n        linewidth=2, linestyle=\"--\")\nplt.fill_between(calcado_range, pred_lower, pred_upper, \n                alpha=0.2, color='darkgreen', label='IC 95% (Posterior)')\nplt.xlabel('Número do Calçado')\nplt.ylabel('Altura (cm)')\nplt.title('Ajuste do Modelo de Regressão Linear Bayesiana')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "",
    "text": "Vamos utilizar modelos estatísticos para analisar a complexidade tecnológica tradicional em ilhas da Oceania (Kline e Boyd 2010). Nosso objetivo é compreender como o tamanho populacional influenciou o número de ferramentas disponíveis em cada sociedade.\nCompararemos três estratégias de modelagem, todas implementadas com PyMC:"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html#analisando-o-conjunto-de-dados",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html#analisando-o-conjunto-de-dados",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "1 Analisando o conjunto de dados",
    "text": "1 Analisando o conjunto de dados\nImporte os dados kline.csv.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport bambi as bmb\nimport arviz as az\nimport xarray as xr\n\n\nkline = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/kline.csv')\nkline\n\n\n\n\n\n\n\n\nculture\npopulation\ncontact\ntotal_tools\nmean_TU\nlat\nlon\n\n\n\n\n0\nMalekula\n1100\nlow\n13\n3.2\n-16.3\n167.5\n\n\n1\nTikopia\n1500\nlow\n22\n4.7\n-12.3\n168.8\n\n\n2\nSanta Cruz\n3600\nlow\n24\n4.0\n-10.7\n166.0\n\n\n3\nYap\n4791\nhigh\n43\n5.0\n9.5\n138.1\n\n\n4\nLau Fiji\n7400\nhigh\n33\n5.0\n-17.7\n178.1\n\n\n5\nTrobriand\n8000\nhigh\n19\n4.0\n-8.7\n150.9\n\n\n6\nChuuk\n9200\nhigh\n40\n3.8\n7.4\n151.6\n\n\n7\nManus\n13000\nlow\n28\n6.6\n-2.1\n146.9\n\n\n8\nTonga\n17500\nhigh\n55\n5.4\n-21.2\n-175.2\n\n\n9\nHawaii\n275000\nlow\n71\n6.6\n19.9\n-155.6\n\n\n\n\n\n\n\nVisualize a relação entre tamanho populacional (P) e número total de ferramentas (T).\n\n# Definir tema com fonte maior\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=kline, x='population', y='total_tools', \n                s=100, alpha=0.7, color='#1f77b4')\n\n# Personalização dos rótulos e tema\nplt.xlabel('Tamanho populacional', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nsns.set_theme(style=\"whitegrid\")\n\n\n\n\n\n\n\nFigura 1: Relação entre o número total de ferramentas e o tamanho populacional em ilhas na Oceania"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-1-regressão-linear-com-transformação-logarítmica",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-1-regressão-linear-com-transformação-logarítmica",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "2 Estratégia 1: Regressão Linear com Transformação Logarítmica",
    "text": "2 Estratégia 1: Regressão Linear com Transformação Logarítmica\nA relação na Figura 1 é claramente não linear e pode ser descrita por:\n\\[T = \\beta_0P^{\\beta_1}\\]\nUma alternativa simples neste caso é utilizar uma transformação logarítmica para linearizar a expressão:\n\\[\\log(T) = \\log(\\beta_0P^{\\beta_1}) \\Rightarrow \\log(T) = \\log(\\beta_0) + \\log(P^{\\beta_1}) \\Rightarrow\\]\n\\[\\log(T) = B_0 + \\beta_1 \\log(P)\n\\tag{1}\\]\nem que \\(B_0 = \\log(\\beta_0)\\)\nVisualizando na escala logarítmica:\n\nkline['log_tools'] = np.log(kline['total_tools'])\nkline['log_pop'] = np.log(kline['population'])\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=kline, x='log_pop', y='log_tools', \n                s=100, alpha=0.7, color='#1f77b4')\n\n# Personalização dos rótulos e tema\nplt.xlabel('log do Tamanho populacional', fontsize=14)\nplt.ylabel('log do Número total de ferramentas', fontsize=14)\n\n\n\n\n\n\nText(0, 0.5, 'log do Número total de ferramentas')\n\n\n(a) Relação entre o logarítmo do número total de ferramentas e o logarítmo do tamanho populacional\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigura 2\n\n\n\n\nConsiderando que a relação na Figura 2 é aproximadamente linear, vamos ajustar o modelo de regressão linear descrito na Equação 1.\nNeste modelo, estamos assumindo que \\(\\log(T)\\) é uma variável aleatória normnalmente distribuída:\n\\[\\log(T) \\sim \\mathcal{N}(\\mu,\\,\\sigma)\\]\n\\[\\mu = B_0 + \\beta_1 \\log(P)\\]\n\n2.1 Implementação\n\nmlinear = bmb.Model(\"log_tools ~ log_pop\", data=kline)\ntrace_linear = mlinear.fit()\n\n\n\n\n\n\n\n\n# Resumo dos parâmetros\naz.summary(trace_linear)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.367\n0.105\n0.208\n0.557\n0.002\n0.003\n2459.0\n2370.0\n1.0\n\n\nIntercept\n0.989\n0.752\n-0.449\n2.410\n0.015\n0.021\n2836.0\n1925.0\n1.0\n\n\nlog_pop\n0.272\n0.083\n0.115\n0.434\n0.002\n0.002\n2798.0\n1931.0\n1.0\n\n\n\n\n\n\n\n\n\n2.2 Gerando predições do modelo linear\n\npred_linear = mlinear.predict(trace_linear, kind = 'response', data = kline, inplace=False)\npred_linear_draws = pred_linear.posterior_predictive.log_tools\npred_linear_mean = pred_linear_draws.mean(dim=['chain', 'draw'])\n\n\n\n2.3 Visualização do modelo linear\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=kline, x='log_pop', y='log_tools', \n                s=100, alpha=0.7, color='#1f77b4')\n\nplt.plot(kline['log_pop'],pred_linear_mean.values, color='red', linewidth=2, label='Predição média')\naz.plot_hdi(\n    kline['log_pop'],\n    pred_linear.posterior_predictive.log_tools,\n    hdi_prob=0.95, # Intervalo de 95%\n    color='#f3ae1a',\n    fill_kwargs={'alpha': 0.3, 'label': 'Intervalo de Credibilidade (95%)'}\n)\n\nplt.xlabel('log(População)', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nplt.title('Modelo de linear: log_tools ~ log_pop', fontsize=16)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\n\n\n\n\n\n\n\n2.3.1 Visualização do modelo linear na escala original\nComo a escala original é um modelo não linear, teremos que gerar os valores preditos para mais pontos a partir dos parâmetros estimaos\n\nlog_pop = np.linspace(min(kline['log_pop']), np.max(kline['log_pop']), num=1000)\nnew_x =  xr.DataArray(\n    log_pop,\n    dims=['obs'],\n    coords={'obs': range(len(log_pop))},\n    name='log_pop'\n)\n\nmlinear_pars = mlinear.predict(trace_linear, kind = 'response_params', data = kline, inplace=False)\n\nB0 = mlinear_pars.posterior['Intercept']\nb1 = mlinear_pars.posterior['log_pop']\nnew_pred_linear_mean = B0.values.mean() + b1.values.mean() * new_x\n\nnew_pred_linear = np.exp(B0 + b1 * new_x)\nic_linear = az.hdi(new_pred_linear, hdi_prob=0.95)\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=kline, x='population', y='total_tools', \n                s=100, alpha=0.7, color='#1f77b4')\n\nplt.plot(np.exp(new_x),np.exp(new_pred_linear_mean))\n\nplt.fill_between(np.exp(new_x),\n                 ic_linear.sel(hdi='lower')['x'],\n                 ic_linear.sel(hdi='higher')['x'],\n                 alpha=0.3, color='#f3ae1a', \n                 label='HDI 95%')"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-2-regressão-de-poisson-glm",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-2-regressão-de-poisson-glm",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "3 Estratégia 2: Regressão de Poisson (GLM)",
    "text": "3 Estratégia 2: Regressão de Poisson (GLM)\nEmbora a transformação logarítmica torne linear a porção determinística do modelo, ela não resolve adequadamente a natureza discreta da variável de resposta. No caso do número de ferramentas (total_tools), estamos lidando com dados de contagem — valores inteiros não negativos. Uma abordagem mais apropriada é utilizar uma regressão de Poisson, que modela diretamente a distribuição da variável como uma variável aleatória de contagem.\nVamos assumir que o que realmente influencia a diversidade tecnológica não é o tamanho absoluto da população, mas sim sua ordem de grandeza (Kline e Boyd 2010). Espera-se, portanto, uma associação positiva entre o número de ferramentas e o logaritmo do tamanho populacional.\nEste modelo generativo pode ser descrito como:\n\\[T_i \\sim \\text{Poisson}(\\lambda_i)\\]\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\cdot \\log(P_i)\\]\nOnde: * \\(T_i\\) é o número de ferramentas na sociedade i * \\(P_i\\) é o tamanho populacional * A função de ligação logarítmica garante que \\(\\lambda_i &gt; 0\\)\n\n3.1 Implementação\n\nmpoisson = bmb.Model(\"total_tools ~ log_pop\", \n                     data=kline, \n                     family=\"poisson\")\n\n\ntrace_poisson = mpoisson.fit(draws=2000, tune=1000)\n\n\n\n\n\n\n\n\n# Resumo dos parâmetros\naz.summary(trace_poisson)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n1.333\n0.307\n0.757\n1.905\n0.004\n0.003\n6893.0\n5122.0\n1.0\n\n\nlog_pop\n0.239\n0.032\n0.181\n0.299\n0.000\n0.000\n6870.0\n5334.0\n1.0\n\n\n\n\n\n\n\n\n\n3.2 Gerando predições do modelo Poisson\n\n# Predições para os dados originais\npred_poisson = mpoisson.predict(trace_poisson, kind='response', data=kline, inplace=False)\npred_poisson_draws = pred_poisson.posterior_predictive.total_tools\npred_poisson_mean = pred_poisson_draws.mean(dim=['chain', 'draw'])\n\n\n# Criar dados para predição suave\nlog_pop_new = np.linspace(min(kline['log_pop']), max(kline['log_pop']), num=100)\nnew_data = pd.DataFrame({'log_pop': log_pop_new})\n\n# Gerar predições\nnew_pred_poisson = mpoisson.predict(trace_poisson, kind='response', data=new_data, inplace=False)\nnew_pred_poisson_mean = new_pred_poisson.posterior_predictive.total_tools.mean(dim=['chain', 'draw'])\n\n\n\n3.3 Visualização do modelo Poisson\n\nplt.figure(figsize=(8, 6))\n\n# Pontos originais\nsns.scatterplot(data=kline, x='log_pop', y='total_tools', \n                s=100, alpha=0.7, color='#1f77b4', label='Dados observados')\n\n# Linha de predição média\nplt.plot(log_pop_new, new_pred_poisson_mean.values, \n         color='red', linewidth=2, label='Predição média')\n\n# Intervalo de credibilidade\naz.plot_hdi(\n    log_pop_new,\n    new_pred_poisson.posterior_predictive.total_tools,\n    hdi_prob=0.95,\n    color='#f3ae1a',\n    fill_kwargs={'alpha': 0.3, 'label': 'HDI 95%'}\n)\n\nplt.xlabel('log(População)', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nplt.title('Modelo de Poisson: total_tools ~ log_pop', fontsize=16)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\n\n\n\n\n\n\n\n\n3.4 Visualização na escala original (população)\n\nplt.figure(figsize=(12, 8))\n\nsns.scatterplot(data=kline, x='population', y='total_tools', \n                s=100, alpha=0.7, color='black', label='Dados observados')\n\nplt.plot(np.exp(log_pop_new), new_pred_poisson_mean.values, \n         color='blue', linewidth=2, linestyle='--', label='Modelo Poisson')\naz.plot_hdi(\n    np.exp(log_pop_new),\n    new_pred_poisson.posterior_predictive.total_tools,\n    hdi_prob=0.95,\n    color='#1f77b4',\n    fill_kwargs={'alpha': 0.3, 'label': 'HDI 95% Poisson'},\n)\n\nplt.xlabel('População', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nplt.title('Comparação de Modelos', fontsize=16)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n3.5 Comparação com modelo linear\n\nplt.figure(figsize=(12, 8))\n\nsns.scatterplot(data=kline, x='population', y='total_tools', \n                s=100, alpha=0.7, color='black', label='Dados observados')\n\n# Modelo Linear com transformação log (escala original)\nplt.plot(np.exp(new_x), np.exp(new_pred_linear_mean), \n         color='red', linewidth=2, label='Modelo Linear')\nplt.fill_between(np.exp(new_x),\n                 ic_linear.sel(hdi='lower')['x'],\n                 ic_linear.sel(hdi='higher')['x'],\n                 alpha=0.3, color='#f3ae1a', \n                 label='HDI 95% Linear')\n\n# Modelo Poisson (GLM)\nplt.plot(np.exp(log_pop_new), new_pred_poisson_mean.values, \n         color='blue', linewidth=2, linestyle='--', label='Modelo Poisson')\naz.plot_hdi(\n    np.exp(log_pop_new),\n    new_pred_poisson.posterior_predictive.total_tools,\n    hdi_prob=0.95,\n    color='#1f77b4',\n    fill_kwargs={'alpha': 0.3, 'label': 'HDI 95% Poisson'},\n)\n\nplt.xlabel('População', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nplt.title('Comparação de Modelos', fontsize=16)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-3-modelo-científico-mecanicista",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html#estratégia-3-modelo-científico-mecanicista",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "4 Estratégia 3: Modelo Científico (Mecanicista)",
    "text": "4 Estratégia 3: Modelo Científico (Mecanicista)\nPartindo de uma descrição teórica da dinâmica de inovação tecnológica, podemos expressar a mudança esperada no número de ferramentas ao longo do tempo como:\n\\[\\Delta T = \\alpha P^\\beta - \\gamma T\\]\nOnde: * \\(P\\) é o tamanho da população * \\(T\\) é o número de ferramentas * \\(\\alpha\\), \\(\\beta\\) e \\(\\gamma\\) são parâmetros a serem estimados * \\(\\alpha P^\\beta\\) representa a taxa de inovação (dependente da população) * \\(\\gamma T\\) representa a taxa de perda de ferramentas\nAssumindo que o sistema está em equilíbrio (\\(\\Delta T = 0\\)), podemos resolver para \\(T\\):\n\n\n\n\n\n\nSituação de equilíbrio\n\n\n\nComeçamos com: \\[0 = \\alpha P^\\beta - \\gamma T\\]\nIsolando o termo \\(T\\): \\[\\gamma T = \\alpha P^\\beta\\]\nDividindo ambos os lados por \\(\\gamma\\): \\[T = \\frac{\\alpha P^\\beta}{\\gamma}\n\\tag{2}\\]\n\n\nIncorporando a Equação 2 a um modelo de Poisson:\n\\[T_i \\sim \\text{Poisson}(\\lambda_i)\\]\n\\[\\lambda_i = \\frac{\\alpha P_i^{\\beta}}{\\gamma}\\]\n\n4.1 Implementação em PyMC\nPara facilitar a implementação, vamos reparametrizar usando:\n\\[\\log(\\lambda_i) = \\log(\\alpha) + \\beta \\cdot \\log(P_i) - \\log(\\gamma)\\]\nou equivalentemente:\n\\[\\log(\\lambda_i) = \\theta_\\alpha + \\beta \\cdot \\log(P_i) + \\theta_\\gamma\\]\nonde \\(\\theta_\\alpha = \\log(\\alpha)\\) e \\(\\theta_\\gamma = -\\log(\\gamma)\\).\n\nlog_pop = np.log(kline['population'])\ntools = kline['total_tools'].values\n\nwith pm.Model() as scientific_model:\n    # Priors\n    theta_alpha = pm.Normal(\"theta_alpha\", mu=0, sigma=2)\n    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n    theta_gamma = pm.Normal(\"theta_gamma\", mu=0, sigma=1)\n\n    # Esperança em escala log\n    log_lambda = theta_alpha + beta * log_pop + theta_gamma\n    lambda_ = pm.math.exp(log_lambda)\n\n    # Likelihood\n    T_obs = pm.Poisson(\"total_tools\", mu=lambda_, observed=tools)\n\n    # Amostragem\n    trace_scientific = pm.sample(2000, tune=1000, target_accept=0.95)\n    pm.compute_log_likelihood(trace_scientific)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naz.summary(trace_scientific, var_names=[\"theta_alpha\", \"beta\", \"theta_gamma\"], hdi_prob=0.89)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5.5%\nhdi_94.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ntheta_alpha\n1.037\n0.941\n-0.567\n2.440\n0.020\n0.015\n2175.0\n2261.0\n1.0\n\n\nbeta\n0.241\n0.031\n0.192\n0.290\n0.001\n0.000\n2905.0\n2778.0\n1.0\n\n\ntheta_gamma\n0.276\n0.920\n-1.114\n1.813\n0.019\n0.016\n2256.0\n2236.0\n1.0\n\n\n\n\n\n\n\n\n\n4.2 Gerando predições do modelo científico\n\nposterior = trace_scientific.posterior\n\n# Recuperar parâmetros transformados\nalpha_samples = np.exp(posterior['theta_alpha'])\nbeta_samples = posterior['beta']\ngamma_samples = np.exp(-posterior['theta_gamma'])\n\n# Grid de população para predições\npop_pred = np.linspace(kline['population'].min(), kline['population'].max(), 200)\n\n# Calcular predições usando a fórmula científica\npred_samples = []\nfor i in range(len(alpha_samples.chain)):\n    for j in range(len(alpha_samples.draw)):\n        alpha_val = alpha_samples.isel(chain=i, draw=j).values\n        beta_val = beta_samples.isel(chain=i, draw=j).values\n        gamma_val = gamma_samples.isel(chain=i, draw=j).values\n        \n        pred = (alpha_val * (pop_pred ** beta_val)) / gamma_val\n        pred_samples.append(pred)\n\npred_samples = np.array(pred_samples)\n\n# Calcular estatísticas\nmean_scientific = pred_samples.mean(axis=0)\nhdi_scientific = az.hdi(pred_samples, hdi_prob=0.89)"
  },
  {
    "objectID": "conteudo/modelos_regressao_bayes/regressao-poisson.html#comparação-dos-três-modelos",
    "href": "conteudo/modelos_regressao_bayes/regressao-poisson.html#comparação-dos-três-modelos",
    "title": "Modelos Estatísticos e Modelos Científicos",
    "section": "5 Comparação dos Três Modelos",
    "text": "5 Comparação dos Três Modelos\nVamos comparar as predições dos três modelos em um único gráfico:\n\nplt.figure(figsize=(8, 6))\n\n# Dados observados (plotados apenas uma vez)\nsns.scatterplot(data=kline, x='population', y='total_tools', \n                s=100, alpha=0.7, color='black', label='Dados observados')\n\n# 1. Modelo Linear com transformação log (escala original)\nplt.plot(np.exp(new_x), np.exp(new_pred_linear_mean), \n         color='red', linewidth=2, label='Modelo Linear')\nplt.fill_between(np.exp(new_x),\n                 ic_linear.sel(hdi='lower')['x'],\n                 ic_linear.sel(hdi='higher')['x'],\n                 alpha=0.3, color='#f3ae1a', \n                 label='IC 95% Linear')\n\n# 2. Modelo Poisson (GLM)\nplt.plot(np.exp(log_pop_new), new_pred_poisson_mean.values, \n         color='blue', linewidth=2, linestyle='--', label='Modelo Poisson')\naz.plot_hdi(\n    np.exp(log_pop_new),\n    new_pred_poisson.posterior_predictive.total_tools,\n    hdi_prob=0.95,\n    color='#1f77b4',\n    fill_kwargs={'alpha': 0.3, 'label': 'IC 95% Poisson'},\n)\n\n# 3. Modelo Científico (Mecanicista)\n# Modelo Científico\nplt.plot(pop_pred, mean_scientific, color='#2ca02c', linewidth=3, \n         label='Modelo Mecanicista')\nplt.fill_between(pop_pred, hdi_scientific[:, 0], hdi_scientific[:, 1],\n                 color='#2ca02c', alpha=0.2, label = 'IC Mecanicista) 95%')\n\n# Configurações do gráfico\nplt.xlabel('População', fontsize=14)\nplt.ylabel('Número total de ferramentas', fontsize=14)\nplt.title('Comparação dos Três Modelos', fontsize=16)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html",
    "title": "Estrutura e tipos de dados",
    "section": "",
    "text": "Neste tópico exploramos os conceitos fundamentais de estrutura e tipos de dados, focando na organização de unidades amostrais e descritores em tabelas de dados. Também discutiremos diferentes tipos de variáveis, suas transformações e como lidar com valores ausentes (NA) em tabelas de dados. Para ilustrar esses conceitos, usaremos a tabela penguins_raw do pacote palmerpenguins em R, fornecendo tanto explicações teóricas quanto exemplos práticos de código em R."
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html#palmer-penguins-dataset",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html#palmer-penguins-dataset",
    "title": "Estrutura e tipos de dados",
    "section": "1 Palmer Penguins dataset",
    "text": "1 Palmer Penguins dataset\nA tabela penguins_raw inclui observações de nidificação, dados de morfometria e tamanho dos pinguins e medidas de isótopos de amostras de sangue de pinguins adultos das espécies Adélie (Pygoscelis adeliae), Chinstrap (Pygoscelis antarctica) e Gentoo (Pygoscelis papua).\n\ndata(penguins_raw)\npenguins_raw |&gt; \n  head() |&gt;\n  gt()\n\n\n\nTabela 1: Primeiras linhas da tabela penguins_raw.\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n2007-11-11\n39.1\n18.7\n181\n3750\nMALE\nNA\nNA\nNot enough blood for isotopes.\n\n\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n2007-11-11\n39.5\n17.4\n186\n3800\nFEMALE\n8.94956\n-24.69454\nNA\n\n\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n2007-11-16\n40.3\n18.0\n195\n3250\nFEMALE\n8.36821\n-25.33302\nNA\n\n\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n2007-11-16\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nAdult not sampled.\n\n\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n2007-11-16\n36.7\n19.3\n193\n3450\nFEMALE\n8.76651\n-25.32426\nNA\n\n\nPAL0708\n6\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n2007-11-16\n39.3\n20.6\n190\n3650\nMALE\n8.66496\n-25.29805\nNA\n\n\n\n\n\n\n\n\n\n\nA tabela penguins_raw contém 344 linhas e 17 colunas, cada uma representando diferentes aspectos dos dados coletados sobre os pinguins. Cada linha representa uma unidade amostral (UA) e cada coluna representa uma variável (VAR) que descreve um atributo específico da unidade amostral (Tabela 2).\n\n\n\nTabela 2: Descrição dos atributos da tabela penguins_raw.\n\n\n\n\n\n\n\n\n\n\nVariável\nTipo\nDescrição\n\n\n\n\nstudyName\nCategórica\nExpedição de amostragem de onde os dados foram coletados, gerados, etc.\n\n\nSample Number\nQuantitativa Discreta\nUm número inteiro indicando a sequência contínua de numeração para cada amostra\n\n\nSpecies\nCategórica\nUma string de caracteres indicando a espécie de pinguim\n\n\nRegion\nCategórica\nUma string de caracteres indicando a região da grade de amostragem Palmer LTER\n\n\nIsland\nCategórica\nUma string de caracteres indicando a ilha perto da Estação Palmer onde as amostras foram coletadas\n\n\nStage\nCategórica\nUma string de caracteres indicando o estágio reprodutivo no momento da amostragem\n\n\nIndividual ID\nCategórica\nUma string de caracteres indicando o ID único para cada indivíduo no conjunto de dados\n\n\nClutch Completion\nCategórica\nUma string de caracteres indicando se o ninho estudado foi observado com uma ninhada completa, ou seja, 2 ovos\n\n\nDate Egg\nCategórica Ordinal\nUma data indicando a data em que o ninho estudado foi observado com 1 ovo (amostrado)\n\n\nCulmen Length\nQuantitativa Contínua\nUm número indicando o comprimento da crista dorsal do bico de um pássaro (milímetros)\n\n\nCulmen Depth\nQuantitativa Contínua\nUm número indicando a profundidade da crista dorsal do bico de um pássaro (milímetros)\n\n\nFlipper Length\nQuantitativa Discreta\nUm número inteiro indicando o comprimento da nadadeira do pinguim (milímetros)\n\n\nBody Mass\nQuantitativa Discreta\nUm número inteiro indicando a massa corporal do pinguim (gramas)\n\n\nSex\nCategórica\nUma string de caracteres indicando o sexo do animal\n\n\nDelta 15 N\nQuantitativa Contínua\nUm número indicando a medida da razão dos isótopos estáveis 15N:14N\n\n\nDelta 13 C\nQuantitativa Contínua\nUm número indicando a medida da razão dos isótopos estáveis 13C:12C\n\n\nComments\nCategórica\nUma string de caracteres com texto fornecendo informações adicionais relevantes para os dados"
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html#unidades-amostrais-e-descritores-formato-geral",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html#unidades-amostrais-e-descritores-formato-geral",
    "title": "Estrutura e tipos de dados",
    "section": "2 Unidades amostrais e descritores: formato geral",
    "text": "2 Unidades amostrais e descritores: formato geral\nA tabela Tabela 1 está organizada no formato em que cada linha representa uma unidade amostral (UA) e cada coluna representa uma variável (VA). As variáveis são os descritores ou atributos que descrevem as características de cada unidade amostral.\n\n\n\n\nTabela 3: Estrutura geral de uma base de dados. As linhas representam as unidades amostrais (ou observações) e as colunas representam as variáveis (ou atributos).\n\n\n\n\n\n\n\n\n\nID\nVA 1\nVA 2\nVA 3\nVA 4\nVA 5\nVA 6\nVA 7\n\n\n\n\nUA 1\n\n\n\n\n\n\n\n\n\nUA 2\n\n\n\n\n\n\n\n\n\nUA 3\n\n\n\n\n\n\n\n\n\nUA 4\n\n\n\n\n\n\n\n\n\nUA 5\n\n\n\n\n\n\n\n\n\nUA 6\n\n\n\n\n\n\n\n\n\nUA 7\n\n\n\n\n\n\n\n\n\nUA 8\n\n\n\n\n\n\n\n\n\nUA 9\n\n\n\n\n\n\n\n\n\nUA 10"
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html#dados-ausentes",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html#dados-ausentes",
    "title": "Estrutura e tipos de dados",
    "section": "3 Dados ausentes",
    "text": "3 Dados ausentes\nValores não preenchidos são comuns em conjuntos de dados. Na tabela penguins_raw, diversas colunas apresentam dados ausentes, indicados como NA. A seguir, são apresentadas algumas estratégias para lidar com esses dados faltantes:\n\nRemover valores faltantes: Exclua linhas com dados ausentes usando a função drop_na().\n\n\npenguins_limpo &lt;- drop_na(penguins_raw)\n\nnrow(penguins_limpo)\n\n[1] 34\n\n\nRestaram apenas 344 linhas na tabela, o que indica a necessidade de avaliar cuidadosamente quais colunas terão seus valores NA removidos. Para isso, é útil verificar a quantidade de dados ausentes em cada coluna. Observando a tabela Tabela 1, nota-se que a maioria dos dados ausentes está na variável Comments. Como essa coluna não será incluída nas análises, os valores ausentes nela não precisam ser removidos. Podemos, portanto, excluir as linhas que contêm NA em outras colunas, preservando apenas a coluna Comments.\n\npenguins_limpo &lt;- penguins_raw |&gt; \n  drop_na(-Comments)\n\nnrow(penguins_limpo)\n\n[1] 324\n\n\nAgora, restaram 344 linhas na tabela. A remoção de linhas deve ser feita com cautela, avaliando caso a caso. Como alternativa, pode-se considerar a imputação de valores para as células ausentes, o que pode permitir a preservação de mais dados para análise.\n\nInserir valores faltantes: Preencha valores faltantes usando métodos estatísticos, como substituição pela média.\n\n\npenguins_lpch &lt;- penguins_raw |&gt; \n  mutate(`Culmen Length (mm)` = if_else(\n    is.na(`Culmen Length (mm)`),\n    mean(`Culmen Length (mm)`, na.rm = TRUE),\n    `Culmen Length (mm)`\n  ))\n\nNeste caso, os valores ausentes foram substituídos pela média aritmética da variável Culmen Length (mm).\n\n\n\n\n\n\nTécnicas Avançadas de Imputação\n\n\n\nUma alternativa à substituição pela média simples é a imputação múltipla, que pode utilizar agrupamentos mais detalhados (por exemplo, por espécie e ilha) e considerar a associação com outras variáveis da tabela. Outra opção é empregar métodos mais sofisticados, como o k-Nearest Neighbors (kNN)."
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html#tipos-de-dados",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html#tipos-de-dados",
    "title": "Estrutura e tipos de dados",
    "section": "4 Tipos de dados",
    "text": "4 Tipos de dados\nUma tabela de dados pode ser composta por variáveis quantitativas ou qualitativas.\n\nVariáveis qualitativas\nSão variáveis não-numéricas como categorias ou rótulos. Dentre as variáveis qualitativas temos aquelas do tipo categóricas não-ordenadas e do tipo categóricas ordenadas.\nVariável categórica não-ordenada: a variável Island classifica cada penguim de acordo com a ilha em que foi registrado. Os níveis da variável Island são: Torgersen, Biscoe, Dream. A variável é do tipo categórica não-ordenada, pois os níveis não possuem qualquer relação de ordenação natural entre si.\n\n\nVariáveis quantitativas\nSão variáveis numéricas que também podem ser sub-divididas em dois grupos: discretas e contínuas.\n\nVariáveis quantitativas discretas: envolvem quantias enumeráveis. Na tabela penguins_raw não há nenhum exemplo deste tipo de variável, mas exemplos podem ser a contagem de barcos que saem para pescar em um determinado dia, o número de peixes de um cardume o número de ovos no ninho de ave.\nVariáveis quantitativas contínuas: envolvem quantias não-enumeráveis como a vazão em \\(m^3/seg\\) que verte de uma cachoeira, o volume de chuva em um determinado dia, altura da maré ou a velocidade do vento. O limite de precisão que utilizamos para representá-las depende basicamente da capacidade de mensuração dos aparelhos disponíveis. Na tabela penguins_raw existem diversos exemplos deste tipo de variável como\n\nEm nosso exemplo, temos diversas variáveis deste tipo como Culmen Length, Culmen Depth, Flipper Length, Body Mass, Delta 15 N e Delta 13 C.\n\n\n\n\n\n\nTransformando variáveis\n\n\n\nSempre é possível transformar variáveis quantitativas em qualitativas. Se temos uma variável medindo o comprimento de peixes desembarcados em centímetros (variável quantitativa), é possível expressá-la de forma categórica em peixes grandes e peixes pequenos (variável qualitativa). Por outro lado, se tivermos somente a informação de que um peixe é grande ou pequeno, não podemos recuperar as quantias numéricas originais. Ao transformar uma variável de quantitativa em qualitativa, algumas propriedades são perdidas."
  },
  {
    "objectID": "conteudo/estrutura_dados/estrutura_tipo.html#níveis-de-mensuração",
    "href": "conteudo/estrutura_dados/estrutura_tipo.html#níveis-de-mensuração",
    "title": "Estrutura e tipos de dados",
    "section": "5 Níveis de mensuração",
    "text": "5 Níveis de mensuração\nPodemos organizar uma variável a partir de seu nível de mensuração (Figura 1), dado em: nominal, ordinal, intervalar e razão.\nNível nominal: é característico de variáveis que possuem níveis não ordenaveis. Ex. cor, grupo taxonômico, nomes de cidades, etc.\n\nNível ordinal: é aquele em que os níveis podem ser ordenados, embora não seja possível quantificar as diferenças entre dois níveis. Ex. i - Ordem de chegada de maratonistas em uma competição (\\(1^o\\),\\(2^o\\),\\(3^o\\),\\(\\cdots\\)). ii - Condição de saneamento das cidades (ótimo, bom, ruim, péssimo). iii - Condição de saneamento das praias da baixada santista (próprio, imprórpio). No nível ordinal podemos ordenar os elementos porém não podemos quantificar as diferenças entre eles.\nNível intervalar: é aquele em que além ser possível ordenar, é possível quantificar as diferenças entre duas observações. No entanto, não há um ponto inicial natural, ou seja, um ponto zero que indique ausência da quantia. Ex. i – Temperatura: \\(0^oC\\) não indica ausência de temperatura, assim como \\(10^oC\\) não é duas vezes mais quente que \\(5^oC\\). Essas características são somente uma convenção relacionada à escala de mensuração da temperatura. ii - Ano do calendário: o ano zero é uma convenção do calendário, não significa ausência de tempo.\nNível de razão: é como o intervalar, porém existe um ponto zero natural. Peso igual a \\(0\\) kg indica ausência de peso e dez quilogramas é duas vezes mais pesado que \\(5\\) kg. O mesmo vale para comprimento, distância, velocidade, número de ovos.\n\n\n\n\n\n\n\n\nFigura 1: Tipos de variáveis e níveis de mensuração.\n\n\n\n\nA depender do nível de mensuração, algumas operações matemáticas podem ou não fazer sentido. Por exemplo, se uma espécie tem \\(N_A = 100\\) indivíduos na região A e \\(N_B = 200\\) na região B, a segunda região é duas vezes mais populosa pois \\(\\frac{N_B}{N_A} = 2\\). Por outro lado, se a temperatura na região A é de \\(T_A = 10^oC\\) enquanto na B é de \\(T_B = 20^oC\\) não faz sentido fazer \\(\\frac{T_B}{T_A} = 2\\) e dizer que B seja duas vezes mais quente que A. Ainda que matematicamente a operação seja possível nos dois exemplos, no último sua interpretação física não tem sentido.\n\n\n\n\n\n\nTipos de dados vs níveis de mensuração\n\n\n\nExiste uma relação entre tipo de dados e nível de mensuração. Os níveis nominal e ordinal de mensuração se referem a variáveis qualitativas não-ordenadas e qualitativas ordenadas respectivamente. Já os níveis intervalar e razão se referem a variáveis quantitativas, podendo ser discretas ou contínuas."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#conteúdo-da-aula",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#conteúdo-da-aula",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Conteúdo da aula",
    "text": "Conteúdo da aula\n\n\nDefinição de matriz\nAdição de matrizes\nMultiplicação por um escalar\nMultiplicação de matrizes\nTransposta de uma matriz\nÁlgebra de matrizes\nInversa de uma matriz\nInversa de uma matriz pelo método de Gauss-Jordan\nMatrizes elementares\nCadeias de Markov para Recifes de Coral"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#definição-de-matriz",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#definição-de-matriz",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Definição de matriz",
    "text": "Definição de matriz\n\\[A_{22} = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix};\nB_{33} = \\begin{bmatrix}\n5 & 6 & 7 \\\\\n8 & 9 & 10 \\\\\n11 & 12 & 13\n\\end{bmatrix}; C_{34} = \\begin{bmatrix}\n14 & 15 & 16 & 17 \\\\\n18 & 19 & 20 & 21 \\\\\n22 & 23 & 24 & 25\n\\end{bmatrix}\\]\n\nEstrutura geral: Para uma matriz \\(m \\times n\\), \\(A = [a_{ij}]\\)\n\\[A_{mn} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#adição-de-matrizes",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#adição-de-matrizes",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Adição de matrizes",
    "text": "Adição de matrizes\n\n\n\n\\(A = \\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9\n\\end{bmatrix}; B = \\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\\)\n\n\\(A + B = \\begin{bmatrix}\n1 + 9 & 4 + 8 & 7 + 7 \\\\\n2 + 6 & 5 + 5 & 8 + 4 \\\\\n3 + 3 & 6 + 2 & 9 + 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n10 & 12 & 14 \\\\\n8 & 10 & 12 \\\\\n6 & 8 & 10\n\\end{bmatrix}\\)\n\n\n\nEstrutura Geral: Para duas matrizes \\(m \\times n\\), \\(A = [a_{ij}]\\) e \\(B = [b_{ij}]\\):\n\\[A + B = [a_{ij} + b_{ij}] = \\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-por-um-escalar",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-por-um-escalar",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Multiplicação por um escalar",
    "text": "Multiplicação por um escalar\n\n\n\nSeja \\(A\\) uma matriz \\(3 \\times 3\\):\n\\(A = \\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9\n\\end{bmatrix}\\)\n\nA multiplicação de \\(A\\) por um escalar \\(c = 3\\):\n\\(cA = 3 \\times \\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9\n\\end{bmatrix}\n= \\begin{bmatrix}\n3 \\times 1 & 3 \\times 4 & 3 \\times 7 \\\\\n3 \\times 2 & 3 \\times 5 & 3 \\times 8 \\\\\n3 \\times 3 & 3 \\times 6 & 3 \\times 9\n\\end{bmatrix}\n= \\begin{bmatrix}\n3 & 12 & 21 \\\\\n6 & 15 & 24 \\\\\n9 & 18 & 27\n\\end{bmatrix}\\)\n\n\n\nEstrutura Geral: Para uma matriz \\(m \\times n\\), \\(A = [a_{ij}]\\) e um escalar \\(c\\):\n\\[cA = [c \\times a_{ij}] = \\begin{bmatrix}\nc \\times a_{11} & c \\times a_{12} & \\cdots & c \\times a_{1n} \\\\\nc\\times a_{21} & c \\times a_{22} & \\cdots & c \\times a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc \\times a_{m1} & c \\times a_{m2} & \\cdots & c \\times a_{mn}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-de-matrizes",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-de-matrizes",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Multiplicação de matrizes",
    "text": "Multiplicação de matrizes\n\n\n\nSeja \\(A\\) uma matriz \\(2 \\times 3\\):\n\\(A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\\)\ne \\(B\\) uma matriz \\(3 \\times 2\\):\n\\(B = \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\\)\n\nA multiplicação de \\(A\\) por \\(B\\):\n\\(AB = \\begin{bmatrix}\n1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11 & 1 \\cdot 8 + 2 \\cdot 10 + 3 \\cdot 12 \\\\\n4 \\cdot 7 + 5 \\cdot 9 + 6 \\cdot 11 & 4 \\cdot 8 + 5 \\cdot 10 + 6 \\cdot 12\n\\end{bmatrix}\n= \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\\)\n\n\n\nEstrutura Geral: Para uma matriz \\(m \\times n\\), \\(A = [a_{ij}]\\), e uma matriz \\(n \\times p\\), \\(B = [b_{ij}]\\):\n\\[AB = [c_{ij}] = \\begin{bmatrix}\n\\sum_{k=1}^{n} a_{1k} b_{k1} & \\sum_{k=1}^{n} a_{1k} b_{k2} & \\cdots & \\sum_{k=1}^{n} a_{1k} b_{kp} \\\\\n\\sum_{k=1}^{n} a_{2k} b_{k1} & \\sum_{k=1}^{n} a_{2k} b_{k2} & \\cdots & \\sum_{k=1}^{n} a_{2k} b_{kp} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sum_{k=1}^{n} a_{mk} b_{k1} & \\sum_{k=1}^{n} a_{mk} b_{k2} & \\cdots & \\sum_{k=1}^{n} a_{mk} b_{kp}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-de-matrizes-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#multiplicação-de-matrizes-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Multiplicação de matrizes",
    "text": "Multiplicação de matrizes\nPropriedades Algébricas da Multiplicação de Matrizes\n\nSejam \\(A\\), \\(B\\) e \\(C\\) matrizes (cujas ordens possibilitem que as operações indicadas sejam realizadas) e seja \\(k\\) um escalar. Então:\n\n\n\n\n\n\n\n\n\nPropriedade\nDescrição\n\n\n\n\n1\n\\(A(BC) = (AB)C\\)\nAssociatividade\n\n\n2\n\\(A(B + C) = AB + AC\\)\nDistributiva à esquerda\n\n\n3\n\\((A + B)C = AC + BC\\)\nDistributiva à direita\n\n\n4\n\\(k(AB) = (kA)B = A(kB)\\)\n\n\n\n5\n\\(I_m A = A = A I_n\\) se \\(A\\) for \\(m \\times n\\)\nIdentidade da multiplicação"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#transposta-de-uma-matriz",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#transposta-de-uma-matriz",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Transposta de uma matriz",
    "text": "Transposta de uma matriz\n\n\n\nSeja \\(A\\) uma matriz \\(2 \\times 3\\):\n\\(A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\\)\n\nA transposta de \\(A\\) é dada por \\(A^T = [a_{ij}]^T = [a_{ji}]\\):\n\\(A^T = \\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}\\)\n\n\n\nEstrutura Geral: Para uma matriz \\(m \\times n\\), \\(A = [a_{ij}]\\):\n\n\n\\(A = [a_{ij}] = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\\)\n\n\\(A^T = [a_{ji}] = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{n1} \\\\\na_{12} & a_{22} & \\cdots & a_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1m} & a_{2m} & \\cdots & a_{nm}\n\\end{bmatrix}\\)"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#transposta-de-uma-matriz-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#transposta-de-uma-matriz-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Transposta de uma matriz",
    "text": "Transposta de uma matriz\nPropriedades Algébricas da Transposta de Matrizes\n\nSejam \\(A\\) e \\(B\\) matrizes (cujas ordens são tais que as operações indicadas podem ser realizadas) e seja \\(k\\) um escalar. Então:\n\n\n\n\nPropriedade\n\n\n\n\n1\n\\((A^T)^T = A\\)\n\n\n2\n\\((A + B)^T = A^T + B^T\\)\n\n\n3\n\\((kA)^T = k(A^T)\\)\n\n\n4\n\\((AB)^T = B^T A^T\\)\n\n\n5\n\\((A^r)^T = (A^T)^r\\) para todos os inteiros \\(r\\) não negativos"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#álgebra-de-matrizes",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#álgebra-de-matrizes",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Álgebra de matrizes",
    "text": "Álgebra de matrizes\nPropriedades Algébricas da Adição de Matrizes e da Multiplicação por Escalar\n\nSejam \\(A\\), \\(B\\) e \\(C\\) matrizes de mesma ordem, e \\(c\\) e \\(d\\) escalares. Então:\n\n\n\n\nPropriedade\nDescrição\n\n\n\n\n1\n\\(A + B = B + A\\)\nComutatividade\n\n\n2\n\\((A + B) + C = A + (B + C)\\)\nAssociatividade\n\n\n3\n\\(A + O = A\\)\n\n\n\n4\n\\(A + (-A) = O\\)\n\n\n\n5\n\\(c(A + B) = cA + cB\\)\nDistributividade\n\n\n6\n\\((c + d)A = cA + dA\\)\nDistributividade\n\n\n7\n\\(c(dA) = (cd)A\\)\n\n\n\n8\n\\(1A = A\\)"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#combinações-lineares-em-matrizes",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#combinações-lineares-em-matrizes",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Combinações lineares em matrizes",
    "text": "Combinações lineares em matrizes\n\nEscrevendo a matriz \\(B = \\begin{bmatrix} 1 & 4 \\\\ 2 & 1 \\end{bmatrix}\\) como combinação linear de \\(A_1 = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\), \\(A_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) e \\(A_3 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1  \\end{bmatrix}\\)\ntemos\n\\[c_1A_1 + c_2A_2 + c_3A_3 = B\\]\n\\[c_1\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 0\n\\end{bmatrix} +\nc_2\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} +\nc_3\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 4 \\\\\n2 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\nA combinção linear pode ser resolvida pelo sistema:\n\\[\n\\begin{cases}\nc_2 + c_3 = 1 \\\\\nc_1 + c_3 = 4 \\\\\n-c_1 + c_3 = 2 \\\\\nc_2 + c_3 = 1\n\\end{cases}\n\\]\n\nQue tem solução:\n\\[c_1 = 1\\] \\[c_2 = -2\\] \\[c_3 = 3\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz",
    "text": "Inversa de uma matriz\n\n\n\nSeja \\(A = \\begin{bmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{bmatrix}\\) e \\(A^{-1} = \\begin{bmatrix}\n\\frac{4}{5} & -\\frac{3}{5} \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix}\\)\n\nVerificamos que \\(A^{-1}\\) é inversa de \\(A\\) pois:\n\\(AA^{-1} = \\begin{bmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{4}{5} & -\\frac{3}{5} \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = I\\)\ne\n\\(A^{-1}A = \\begin{bmatrix}\n\\frac{4}{5} & -\\frac{3}{5} \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix}\n\\begin{bmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = I\\)\n\n\n\n\n\n\n\n\n\nDefinição\n\n\nSe \\(A\\) é uma matriz \\(n \\times n\\), uma inversa de \\(A\\) é uma matriz \\(n \\times n\\) \\(A^{-1}\\) que satisfaz:\n\\[AA^{-1} = I\\] e \\[A^{-1}A = I\\]\nsendo \\(I = I_n\\) a matriz identidade \\(n \\times n\\). Se existir uma matriz \\(A^{-1}\\) assim, diremos que \\(A\\) é invertível."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz",
    "text": "Inversa de uma matriz\nEm cada exemplo, verifique se a matriz \\(B\\) é inversa de \\(A\\)\n\n\n\n\n\n\\(A = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\\) e \\(B = \\begin{bmatrix}\n-2 & 1 \\\\\n1.5 & -0.5\n\\end{bmatrix}\\)\n\n\n\n\\(A = \\begin{bmatrix}\n2 & 5 \\\\\n1 & 3\n\\end{bmatrix}\\) e \\(B = \\begin{bmatrix}\n3 & -5 \\\\\n-1 & 1\n\\end{bmatrix}\\)\n\n\n\n\n\n\n\\(A = \\begin{bmatrix}\n2 & 1 & 1 \\\\\n1 & 3 & 2 \\\\\n1 & 0 & 0\n\\end{bmatrix}\\) e \\(B = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n-2 & 1 & 3 \\\\\n3 & -1 & -5\n\\end{bmatrix}\\)\n\n\n\n\\(A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 4 \\\\\n5 & 6 & 0\n\\end{bmatrix}\\) e \\(B = \\begin{bmatrix}\n4 & 2 & -2 \\\\\n-1 & 3 & 5 \\\\\n0 & 5 & 1\n\\end{bmatrix}\\)"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-2",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-2",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz",
    "text": "Inversa de uma matriz\n\n\n\nVerifique que \\(A = \\begin{bmatrix}\n2 & 5 \\\\\n1 & 3\n\\end{bmatrix}\\) invertível e pode ser escrita por:\n\\(\\begin{bmatrix}\n2 & 5 \\\\\n1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nw & x \\\\\ny & z\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\\)\n\nQue resulta no sistema de equações:\n\\(\\begin{cases}\n2w + 5y = 1 \\\\\n2x + 5z = 0 \\\\\nw + 3y = 0 \\\\\nx + 3z = 1\n\\end{cases}\\)\n\n\n\n\nQue pode ser resolvido por:\n\\(\\left[ \\begin{array}{cccc|c}\n2 & 0 & 5 & 0 & 1\\\\\n0 & 2 & 0 & 5 & 0\\\\\n1 & 0 & 3 & 0 & 0\\\\\n0 & 1 & 0 & 3 & 1\n\\end{array} \\right]\\) \\(\\begin{array}{c}\nL_1 \\leftrightarrow L_3\\\\\nL_2 \\leftrightarrow L_4\\\\\n\\\\\n\\\\\n\\end{array}\\) \\(\\left[ \\begin{array}{cccc|c}\n1 & 0 & 3 & 0 & 0\\\\\n0 & 1 & 0 & 3 & 1\\\\\n2 & 0 & 5 & 0 & 1\\\\\n0 & 2 & 0 & 5 & 0\n\\end{array} \\right]\\) \\(\\begin{array}{c}\n\\\\\n\\\\\nL_3 - 2L_1 \\\\\nL_4 - 2L_2 \\\\\n\\end{array}\\) \\(\\left[ \\begin{array}{cccc|c}\n1 & 0 & 3 & 0 & 0\\\\\n0 & 1 & 0 & 3 & 1\\\\\n0 & 0 & -1 & 0 & 1\\\\\n0 & 0 & 0 & -1 & -2\n\\end{array} \\right]\\) \\(\\begin{array}{c}\nL_1 + 3L_3 \\\\\nL_2 + 3L_4 \\\\\n-L_3 \\\\\n-L_4 \\\\\n\\end{array}\\)\n\\(\\left[ \\begin{array}{cccc|c}\n1 & 0 & 0 & 0 & 3\\\\\n0 & 1 & 0 & 0 & -5\\\\\n0 & 0 & 1 & 0 & -1\\\\\n0 & 0 & 0 & 1 & 2\n\\end{array} \\right]\\) \\(S = \\left[\\begin{array}{c}\n3 \\\\\n-5 \\\\\n-1 \\\\\n2 \\\\\n\\end{array} \\right]\\) Portanto: \\(A^{-1} = \\begin{bmatrix}\n3 & -5 \\\\\n-1 & 2\n\\end{bmatrix}\\)"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-3",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-3",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz",
    "text": "Inversa de uma matriz\n\n\n\nVerifique que \\(B = \\begin{bmatrix}\n1 & 2 \\\\\n2 & 4\n\\end{bmatrix}\\) não é invertível e portanto não pode ser escrita por:\n\\(\\begin{bmatrix}\n1 & 2 \\\\\n2 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\nw & x \\\\\ny & z\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\\)\n\nO sistema de equações lineares fica:\n\\(\\begin{cases}\nw + 2y = 1 \\\\\nx + 2z = 0 \\\\\n2w + 4y = 0 \\\\\n2x + 4z = 1\n\\end{cases}\\)\n\n\n\n\nQue pode ser representado por:\n\\(\\left[ \\begin{array}{cccc|c}\n1 & 0 & 2 & 0 & 1\\\\\n0 & 1 & 0 & 2 & 0\\\\\n2 & 0 & 4 & 0 & 0\\\\\n0 & 2 & 0 & 4 & 1\n\\end{array} \\right]\\) \\(\\begin{array}{c}\n\\\\\n\\\\\nL_3 - 2L_1 \\\\\nL_4 - 2L_2 \\\\\n\\end{array}\\) \\(\\left[ \\begin{array}{cccc|c}\n1 & 0 & 2 & 0 & 1\\\\\n0 & 1 & 0 & 2 & 0\\\\\n0 & 0 & 0 & 0 & -2\\\\\n0 & 0 & 0 & 0 & -1\n\\end{array} \\right]\\)\n\nA matriz na forma escalonada mostra que o sistema não tem solução e portanto a matriz \\(B\\) não é invertível."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-4",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-4",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz",
    "text": "Inversa de uma matriz\n\n\n\n\n\n\n\nTeorema\n\n\nSe \\(A\\) é uma matriz \\(n \\times n\\) invertível, o sistema de equações lineares dado por \\(A\\vec{x} = \\vec{b}\\) tem uma única solução \\(\\vec{x} = A^{-1}\\vec{b}\\) para cada \\(\\vec{b}\\) em \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\n\n\n\nPropriedades\n\n\n\nSe \\(A\\) é uma matriz invertível, então \\(A^{-1}\\) é invertível e \\((A^{-1})^{-1} = A\\).\nSe \\(A\\) é uma matriz invertível e \\(c\\) é um escalar não nulo, então \\(cA\\) é uma matriz invertível e \\((cA)^{-1} = \\frac{1}{c}A^{-1}\\).\nSe \\(A\\) e \\(B\\) são matrizes invertíveis de mesma ordem, então \\(AB\\) é invertível e \\((AB)^{-1} = B^{-1}A^{-1}\\).\nSe \\(A\\) é uma matriz invertível, então \\(A^T\\) é invertível e \\((A^T)^{-1} = (A^{-1})^T\\).\nSe \\(A\\) é uma matriz invertível, então, para todo inteiro não negativo \\(n\\), a matriz \\(A^n\\) é invertível e \\((A^n)^{-1} = (A^{-1})^n\\)."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-pelo-método-de-gauss-jordan",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-pelo-método-de-gauss-jordan",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz pelo método de Gauss-Jordan",
    "text": "Inversa de uma matriz pelo método de Gauss-Jordan\n\nPara encontrar a inversa de uma matriz \\(A\\) usando o método de Gauss-Jordan, seguimos os seguintes passos:\n\nFormação da Matriz Aumentada:\n\n\nDada uma matriz \\(A\\) de ordem \\(n \\times n\\), formamos a matriz aumentada \\([A \\mid I]\\), onde \\(I\\) é a matriz identidade de ordem \\(n \\times n\\).\n\n\nAplicação de Operações Elementares:\n\n\nAplicamos operações elementares sobre as linhas da matriz aumentada \\([A \\mid I]\\) para transformar a parte esquerda (a matriz \\(A\\)) na matriz identidade \\(I\\).\n\n\nObtenção da Inversa:\n\n\nQuando a parte esquerda da matriz aumentada se transforma em \\(I\\), a parte direita será a matriz inversa \\(A^{-1}\\). Ou seja, \\([I \\mid A^{-1}]\\)."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-pelo-método-de-gauss-jordan-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#inversa-de-uma-matriz-pelo-método-de-gauss-jordan-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Inversa de uma matriz pelo método de Gauss-Jordan",
    "text": "Inversa de uma matriz pelo método de Gauss-Jordan\n\nExemplo Prático\n\n\nConsidere a matriz \\(A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 3 & 2 \\\\ 1 & 0 & 0 \\end{bmatrix}\\).\n\nQue tem a matriz aumentada \\([A \\mid I]\\):\n\\[\\left[\\begin{array}{ccc|ccc}\n   2 & 1 & 1 & 1 & 0 & 0 \\\\\n   1 & 3 & 2 & 0 & 1 & 0 \\\\\n   1 & 0 & 0 & 0 & 0 & 1\n   \\end{array}\\right]\\]\n\n\n\n\nAplique operações elementares para transformar a parte esquerda em \\(I\\):\n\\(\\left[\\begin{array}{ccc|ccc}\n   2 & 1 & 1 & 1 & 0 & 0 \\\\\n   1 & 3 & 2 & 0 & 1 & 0 \\\\\n   1 & 0 & 0 & 0 & 0 & 1\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   L_1 \\leftrightarrow L_3\\\\\n   \\\\\n   \\\\\n   \\end{array}\\) \\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   1 & 3 & 2 & 0 & 1 & 0 \\\\\n   2 & 1 & 1 & 1 & 0 & 0\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   \\\\\n   L_2 - L_1\\\\\n   L_3 - 2L_1\\\\\n   \\end{array}\\) \\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   0 & 3 & 2 & 0 & 1 & -1 \\\\\n   0 & 1 & 1 & 1 & 0 & -2\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   \\\\\n   L_2 \\leftrightarrow L_3\\\\\n   \\\\\n   \\end{array}\\) \\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   0 & 1 & 1 & 1 & 0 & -2 \\\\\n   0 & 3 & 2 & 0 & 1 & -1\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   \\\\\n   \\\\\n   L_3 - 3L_2\\\\\n   \\end{array}\\)\n\\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   0 & 1 & 1 & 1 & 0 & -2 \\\\\n   0 & 0 & -1 & -3 & 1 & 5\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   \\\\\n   \\\\\n   -L_3\\\\\n   \\end{array}\\) \\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   0 & 1 & 1 & 1 & 0 & -2 \\\\\n   0 & 0 & 1 & 3 & -1 & -5\n   \\end{array}\\right]\\) \\(\\begin{array}{c}\n   \\\\\n   L_2 - L_3\\\\\n   \\\\\n   \\end{array}\\) \\(\\left[\\begin{array}{ccc|ccc}\n   1 & 0 & 0 & 0 & 0 & 1 \\\\\n   0 & 1 & 0 & -2 & 1 & 3 \\\\\n   0 & 0 & 1 & 3 & -1 & -5\n   \\end{array}\\right]\\)\n\n\\[A^{-1} = \\begin{bmatrix}\n   0  &  0 &  1 \\\\\n   -2 &  1 &  3 \\\\\n   3  & -1 & -5\n   \\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#matrizes-elementares",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#matrizes-elementares",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Matrizes Elementares",
    "text": "Matrizes Elementares\n\nDefinição:\nMatrizes elementares são aquelas obtidas através de operações elementares realizadas sobre a matriz identidade. Elas desempenham um papel fundamental na solução de sistemas lineares e na obtenção da inversa de uma matriz.\nExemplos de Operações Elementares:\n\nTroca de Linhas: Exemplo: Trocar a linha 1 pela linha 2.\n\\[E = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\]\nMultiplicação de uma Linha por um Escalar: Exemplo: Multiplicar a linha 1 por um escalar \\(k\\).\n\\[E = \\begin{bmatrix} k & 0 \\\\ 0 & 1 \\end{bmatrix}\\]\nAdição de Múltiplos de Linhas: Exemplo: Adicionar a linha 2 multiplicada por um escalar \\(k\\) à linha 1.\n\\[E = \\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-matrizes-elementares-para-calcular-a-inversa",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-matrizes-elementares-para-calcular-a-inversa",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de Matrizes Elementares para Calcular a Inversa",
    "text": "Exemplo de Matrizes Elementares para Calcular a Inversa\n\n\n\nConsidere a matriz \\(A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 3 & 2 \\\\ 1 & 0 & 0 \\end{bmatrix}\\).\n\nQue foi resolvida no exemplo anterior pela sequência de operações elementares:\n\n\\(L_1 \\leftrightarrow L_3\\)\n\\(L_2 \\rightarrow L_2 - L_1\\); \\(L_3 \\rightarrow L_3 - 2L_1\\)\n\\(L_2 \\leftrightarrow L_3\\)\n\\(L_3 \\rightarrow L_3 - 3L_2\\)\n\\(L_3 \\rightarrow -L_3\\)\n\\(L_2 \\rightarrow L_2 - L_3\\)\n\n\n\n\n\n\n\n\nTroca de \\(L_1\\) e \\(L_3\\):\n\\[E_1 = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 0\n\\end{bmatrix}\\]\n\\(L_2 \\rightarrow L_2 - L_1\\); \\(L_3 \\rightarrow L_3 - 2L_1\\):\n\\[E_2 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n-2 & 0 & 1\n\\end{bmatrix}\\]\nTroca de \\(L_2\\) e \\(L_3\\):\n\\[E_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\\]\n\n\n\n\\(L_3 \\rightarrow L_3 - 3L_2\\):\n\\[E_4 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & -3 & 1\n\\end{bmatrix}\\]\nMultiplicação de \\(L_3\\) por \\(-1\\):\n\\[E_5 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & -1\n\\end{bmatrix}\\]\n\\(L_2 \\rightarrow L_2 - L_3\\):\n\\[E_6 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-matrizes-elementares-para-calcular-a-inversa-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-matrizes-elementares-para-calcular-a-inversa-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de Matrizes Elementares para Calcular a Inversa",
    "text": "Exemplo de Matrizes Elementares para Calcular a Inversa\n\nEstabelecidas as matrizes elementares \\(E_1\\) a \\(E_6\\), tem-se a seguinte relação:\n\\[E_6 \\times E_5 \\times E_4 \\times E_3 \\times E_2 \\times E_1 \\times A = I\\]\nE consequentemente:\n\\[E_1^{-1} \\times E_2^{-1} \\times E_3^{-1} \\times E_4^{-1} \\times E_5^{-1} \\times E_6^{-1} = A\\]\n\n\n\n\n\n\n\n\n\nO Teorema Fundamental das Matrizes Invertíveis - versão 1\n\n\nSeja \\(A\\) uma matriz \\(n \\times n\\). As seguintes afirmações são equivalentes:\n\n\\(A\\) é invertível.\n\\(A\\vec{x} = \\vec{b}\\) tem uma única solução para cada \\(\\vec{b}\\) em \\(\\mathbb{R}^n\\).\n\\(A\\vec{x} = 0\\) tem apenas a solução trivial.\nA forma escalonada reduzida de \\(A\\) é \\(I_n\\).\n\\(A\\) é um produto de matrizes elementares."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral",
    "text": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral\n\nProblema\nOs recifes de coral enfrentam várias ameaças ambientais, como branqueamento, acidificação dos oceanos e destruição física. Essas ameaças podem ser modeladas usando uma Cadeia de Markov para entender a probabilidade de um recife estar em um certo estado de saúde ao longo do tempo.\nEstados\nDefinimos três estados possíveis para a saúde de um recife de coral:\n\nS1: Saudável\nS2: Moderadamente Degradado\nS3: Severamente Degradado\n\nMatriz de Transição\nA matriz de transição de estados, \\(P\\), representa as probabilidades de transição entre os estados de saúde de um recife de coral de um período para o outro.\n\\[P = \\begin{bmatrix}\n0.7 & 0.3 & 0.1 \\\\\n0.2 & 0.5 & 0.3 \\\\\n0.1 & 0.2 & 0.6\n\\end{bmatrix}\\]\nCada elemento \\(P_{ij}\\) na matriz representa a probabilidade de transição do estado \\(j\\) na coluna para o estado \\(i\\) na linha. Por exemplo, \\(P_{12} = 0.2\\) indica que há uma probabilidade de 20% de um recife saudável (\\(S1\\)) passar para o estado moderadamente degradado (\\(S2\\)) no próximo período."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-1",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-1",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral",
    "text": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral\n\nEstado inicial\nVamos considerar que inicialmente no tempo \\(t_0\\) 80% dos recifes estão saudáveis, 15% estão moderadamente degradados e 5% estão severamente degradados. Isso pode ser representado pelo vetor de estado inicial:\n\\[\\vec{v_0} = \\begin{bmatrix}\n0.8 \\\\\n0.15 \\\\\n0.05\n\\end{bmatrix}\\]\nEstado em \\(t + 1\\)\nPara determinar o estado dos recifes após um período de tempo \\(t_1\\), multiplicamos o vetor de estado inicial pela matriz de transição:\n\\[\n\\vec{v_1} = P \\times \\vec{v_0} = \\begin{bmatrix}\n0.7 & 0.3 & 0.1 \\\\\n0.2 & 0.5 & 0.3 \\\\\n0.1 & 0.2 & 0.6\n\\end{bmatrix} \\times \\begin{bmatrix}\n0.8 \\\\\n0.15 \\\\\n0.05\n\\end{bmatrix} = \\begin{bmatrix}\n0.61 \\\\\n0.25 \\\\\n0.14\n\\end{bmatrix}\n\\]\nIsso significa que, após um período, 61% dos recifes estarão saudáveis, 25% estarão moderadamente degradados e 14% estarão severamente degradados."
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-2",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-2",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral",
    "text": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral\n\nEstado Estacionário\nO estado estacionário é um vetor de probabilidades que representa a distribuição dos estados de um sistema em equilíbrio, onde as probabilidades de estar em cada estado não mudam com o tempo. Para uma cadeia de Markov, isso ocorre quando o vetor de estado não muda após uma multiplicação pela matriz de transição.\nSe \\(\\vec{v_{ss}}\\) é um vetor de estado estacionário e \\(P\\) é a matriz de transição, então:\n\\[\n\\vec{v_{ss}} = P \\times \\vec{v_{ss}}\n\\]\nPortanto, precisamos resolver o sistema de equações:\n\\[\n\\begin{bmatrix}\n0.7 & 0.3 & 0.1 \\\\\n0.2 & 0.5 & 0.3 \\\\\n0.1 & 0.2 & 0.6\n\\end{bmatrix} \\times \\begin{bmatrix}\n\\pi_1 \\\\\n\\pi_2 \\\\\n\\pi_3\n\\end{bmatrix} = \\begin{bmatrix}\n\\pi_1 \\\\\n\\pi_2 \\\\\n\\pi_3\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-3",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-3",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral",
    "text": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral\n\nResolução do Sistema\n\n\nHá ainda uma condição adicional de que a soma das probabilidades em \\(\\vec{v_t}\\) seja 1:\n\\[\n\\pi_1 + \\pi_2 + \\pi_3 = 1\n\\]\n\nQue gera e sistema de equações:\n\\[\n\\begin{cases}\n0.7\\pi_1 + 0.3\\pi_2 + 0.1\\pi_3 = \\pi_1 \\\\\n0.2\\pi_1 + 0.5\\pi_2 + 0.3\\pi_3 = \\pi_2 \\\\\n0.1\\pi_1 + 0.2\\pi_2 + 0.6\\pi_3 = \\pi_3 \\\\\n\\pi_1 + \\pi_2 + \\pi_3 = 1\n\\end{cases}\n\\]\n\n\n\n\n\n\nO pode ser reorganizado como:\n\\[\n\\begin{cases}\n-3\\pi_1 + 3\\pi_2 + \\pi_3 = 0 \\\\\n2\\pi_1 - 5\\pi_2 + 3\\pi_3 = 0 \\\\\n\\pi_1 + 2\\pi_2 - 4\\pi_3 = 0 \\\\\n\\pi_1 + \\pi_2 + \\pi_3 = 1\n\\end{cases}\n\\]\n\nE tem solução:\n\\[\n\\vec{v_{ss}} = \\begin{bmatrix} \\frac{7}{17} \\\\ \\frac{11}{34} \\\\ \\frac{9}{34} \\end{bmatrix} ~ \\sim \\begin{bmatrix}\n0.4117 \\\\\n0.3235 \\\\\n0.2647\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nResoluçao completa\n\n\nFaça o download da RESOLUÇÃO COMPLETA"
  },
  {
    "objectID": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-4",
    "href": "conteudo/ecologia-numerica/intro-matrizes.html#exemplo-de-aplicação-cadeias-de-markov-para-recifes-de-coral-4",
    "title": "Introdução à Álgebra de Matrizes",
    "section": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral",
    "text": "Exemplo de aplicação: Cadeias de Markov para Recifes de Coral\n\nInterpretação do vetor de Estado Estacionário\nNo longo prazo, a distribuição das probabilidades entre os estados é:\n\n\\(\\pi_1 = \\frac{7}{17} ~ \\sim 0.4117\\)\n\\(\\pi_2 = \\frac{11}{34} ~ \\sim 0.3235\\)\n\\(\\pi_3 = \\frac{9}{34} ~ \\sim 0.2647\\)\n\n\n\n\n\n\n\n\n\nConclusão\n\n\nA existência de um vetor estacionário indica que, independentemente do estado inicial, a cadeia de Markov converge para essa distribuição de probabilidade quando o sistema está em equilíbrio.\nPortanto, mantendo as condições atuais que resultam na matriz de transição vigente, espera-se que, a longo prazo, aproximadamente 41,18% do recife de coral permanecerá em condições Saudáveis, 32,35% estará Moderadamente Degradado e 26,47% será Severamente Degradado."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "",
    "text": "Objetivos\n\n\n\nNeste tutorial, vamos implementar o Método dos Mínimos Quadrados (MMQ) em Python para ajustar um modelo de regressão linear simples.\nObjetivo: Encontrar os coeficientes \\(\\beta_0\\) e \\(\\beta_1\\) da equação \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) que melhor se ajustam aos nossos dados."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#introdução",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#introdução",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "",
    "text": "Objetivos\n\n\n\nNeste tutorial, vamos implementar o Método dos Mínimos Quadrados (MMQ) em Python para ajustar um modelo de regressão linear simples.\nObjetivo: Encontrar os coeficientes \\(\\beta_0\\) e \\(\\beta_1\\) da equação \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) que melhor se ajustam aos nossos dados."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#importando-as-bibliotecas",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#importando-as-bibliotecas",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "2 🛠️ Importando as Bibliotecas",
    "text": "2 🛠️ Importando as Bibliotecas\nPrimeiro, vamos importar as bibliotecas que usaremos:\n\nimport matplotlib.pyplot as plt  # Para criação e manipulação gráfica\nimport numpy as np           # Para operações matemáticas e matriciais\n\n💡 Dica: No Google Colab, essas bibliotecas já vêm instaladas!"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#inserindo-os-dados",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#inserindo-os-dados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "3 📊 Inserindo os Dados",
    "text": "3 📊 Inserindo os Dados\nVamos trabalhar um exemplo simples em que \\(x\\) e \\(y\\) são inseridos como listas em Python:\n\n# Nossos dados de exemplo\nx = [0, 1, 2, 3, 4]  # Variável independente (preditora)\ny = [0, 1, 1, 4, 4]  # Variável dependente (resposta)\n\nCaso deseje visualizar se os objetos x e y foram criados corretamente podemos utilizar a função print().\n\nprint(\"Valores de x:\", x)\nprint(\"Valores de y:\", y)\n\nValores de x: [0, 1, 2, 3, 4]\nValores de y: [0, 1, 1, 4, 4]"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#visualizando-os-dados",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#visualizando-os-dados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "4 📈 Visualizando os Dados",
    "text": "4 📈 Visualizando os Dados\nAntes de ajustar o modelo, vamos visualizar nossos dados em um gráfigo de dispersão utilizando a função scatter() da biblioteca Matplotlib:\n\n# Criando o gráfico de dispersão\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, color = '#0072B2', s=120, label='Dados observados')\n\n# Configurando o layout gráfico\nplt.title('Gráfico de Dispersão dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n📝 Observação: O gráfico sugere uma relação linear entre as variáveis, o que justifica o uso da regressão linear simples, que iremos ajustra por meio do MMQ."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#implementando-o-mmq---passo-a-passo",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#implementando-o-mmq---passo-a-passo",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "5 🧮 Implementando o MMQ - Passo a Passo",
    "text": "5 🧮 Implementando o MMQ - Passo a Passo\n\n5.1 Criando os Vetores Base\nLembre-se da teoria: inicialmente precisamos caracterizar os vetores \\(\\vec{f}_0\\), \\(\\vec{f}_1\\) e \\(\\vec{y}\\):\n\\[\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\]\n\n# Número de observações\nn = len(x)\n\n# Vetor f0: vetor de 1's (para o intercepto β₀)\nf0 = [1] * n  # Cria uma lista com n elementos iguais a 1\n\n# Vetor f1: nossos valores de x (para o coeficiente β₁)\nf1 = x.copy()  # Copia os valores de x para um novo objeto denominado f1\n\nVisualizando os vetores \\(\\vec{f}_0\\) e \\(\\vec{f}_1\\).\n\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (coeficiente):\", f1)\n\nVetor f0 (intercepto): [1, 1, 1, 1, 1]\nVetor f1 (coeficiente): [0, 1, 2, 3, 4]\n\n\n\n\n5.2 Construindo as Matrizes X e Y\nAgora vamos montar as matrizes do sistema:\n\\[X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\]\n\n# Matriz X: combinando f0 e f1 em colunas\nX = np.column_stack((f0, f1))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna \nY = np.array(y).reshape(n, 1)\n\nVisualizando as matrizes \\(X\\) e \\(Y\\).\n\nprint(\"Matriz X:\")\nprint(X)\nprint(\"\\nMatriz Y:\")\nprint(Y)\nprint(f\"\\nDimensões - X: {X.shape}, Y: {Y.shape}\")\n\nMatriz X:\n[[1 0]\n [1 1]\n [1 2]\n [1 3]\n [1 4]]\n\nMatriz Y:\n[[0]\n [1]\n [1]\n [4]\n [4]]\n\nDimensões - X: (5, 2), Y: (5, 1)\n\n\nNote agora que a matriz \\(X\\) tem 5 linhas e 2 colunas, enquanto a matriz \\(Y\\) tem 5 linhas e 1 colunas.\n\n\n5.3 Resolvendo o Sistema Normal\nAgora vamos calcular os coeficientes usando as operações matriciais: \\[B = (X^T X)^{-1} X^T Y\\]\n\n# Calculando X transposta vezes X\nXTX = np.dot(X.T, X)  # X.T é a transposta de X\n# Calculando X transposta vezes Y\nXTY = np.dot(X.T, Y)\n# Calculando a matriz inversa (X^T X)^(-1)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\n# Coeficientes de regressão\nB = np.dot(XTX_inv, XTY)\n\nVisualizando os objetos\n\n# Calculando X transposta vezes X\nprint(\"X^T X:\")\nprint(XTX)\n\nprint(\"\\nX^T Y:\")\nprint(XTY)\n\nprint(\"\\nB:\")\nprint(B)\n\nX^T X:\n[[ 5 10]\n [10 30]]\n\nX^T Y:\n[[10]\n [31]]\n\nB:\n[[-0.2]\n [ 1.1]]\n\n\n\n\n\n\n\n\nNota\n\n\n\n\nA função ´np.dot()´ em Python pode ser substituída pelo símbolo @. Teste os códigos abaixo e verifique que os resultados coincidem:\n\n\nprint(\"Usando np.dot()\")\nprint(np.dot(X.T, X))\n\nUsando np.dot()\n[[ 5 10]\n [10 30]]\n\n\n\nprint(\"Usando '@'\")\nprint(X.T @ X)\n\nUsando '@'\n[[ 5 10]\n [10 30]]\n\n\n\nOs elementos internos de uma matriz podem ser acessados destacando suas posições na linha e coluna. Considere a matrtiz B. O coeficiente \\(\\beta_0\\) pode ser acessado na linha 1 e coluna 1, enquanto \\(\\beta_1\\) pode ser acessado na linha 2 e coluna 1.\n\n\nprint(B[0,0]) # Beta 0\n\n-0.1999999999999993\n\n\n\nprint(B[1,0]) # Beta 1\n\n1.1\n\n\n\n\n\n\n5.4 Obtendo os Valores Ajustados de y\nTendo obtido os coeficientes de regressão, os valores ajustados de y (\\(\\hat{y}\\)) podem ser obtido pela multiplicação matricial:\n\\[F = X B = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix}\\]\nObs.: denominamos \\(F\\) a matriz de valores ajustados de \\(y\\).\n\n# Valores ajustados (preditos)\nF = np.dot(X, B)\n\n\n# Valores ajustados (preditos)\nprint(F)\n\n[[-0.2]\n [ 0.9]\n [ 2. ]\n [ 3.1]\n [ 4.2]]\n\n\n\n\n5.5 Avaliando a Qualidade do Ajuste\n\n5.5.1 Calculando a Soma dos Quadrados dos Resíduos (\\(SQ_{res}\\))\n\\(SQ_{res}\\) pode ser obtida pela multiplicação matricial:\n\\[SQ_{res} = \\boldsymbol{e}^T \\boldsymbol{e}\\]\nEm que \\(\\boldsymbol{e}\\) é a matriz coluna dos resíduos obtida pela diferença entre os valores observados e ajustados de \\(y\\):\n\\[\\boldsymbol{e} = Y - F = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\]\n\n# Resíduos: diferença entre valores observados e ajustados\ne = Y - F\n\n# Soma dos Quadrados dos Resíduos\nSQres = np.dot(e.T, e)[0, 0]\n\n\n\n5.5.2 Calculando a Soma dos Quadrados Totais (\\(SQ_{tot}\\))\n\\(SQ_{tot}\\) pode ser obtida pela multiplicação matricial:\n\\[SQ_{tot} = \\boldsymbol{D}^T \\boldsymbol{D}\\]\nEm que \\(\\boldsymbol{D}\\) é a matriz coluna dos desvios da médis obtida pela diferença entre os valores observados de \\(y\\) e a média de \\(\\overline{y}\\):\n\\[\\boldsymbol{D} = Y - \\overline{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\overline{y} \\\\ \\overline{y} \\\\ \\vdots \\\\ \\overline{y} \\end{bmatrix} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}\\]\n\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = np.dot(D.T, D)[0, 0]\n\n\n\n5.5.3 Calculando o coeficiente de determinação \\(R^2\\):\nO \\(R^2\\) é dado pela expressão:\n\\[R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}\\]\n\n# Coeficiente de Determinação R²\nR2 = 1 - (SQres / SQtot)\n\n\nVisualizando os resultados:\n\nprint(\"📊 Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos Resíduos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de Determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\n\n📊 Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos Resíduos (SQres): 1.9000\nSoma dos Quadrados Total (SQtot): 14.0000\nCoeficiente de Determinação (R²): 0.8643\nPorcentagem da variação explicada: 86.43%\n\n\n📝 Interpretação do \\(R^2\\):\n\nVaria de 0 a 1\nQuanto mais próximo de 1, melhor o ajuste\nRepresenta a proporção da variação em \\(y\\) explicada pelo modelo"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#visualizando-o-resultado-final",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#visualizando-o-resultado-final",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "6 📊 Visualizando o Resultado Final",
    "text": "6 📊 Visualizando o Resultado Final\nVamos plotar os dados originais junto com a reta ajustada:\n\n# Criando o gráfico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nplt.scatter(x, y, \n            color = '#0072B2', s=120,\n            label=f'Dados observados (n={n})')\n\n# Valores ajustados\nplt.scatter(x, F[:,0],\n            color='#000000', marker='*', s=120, \n            label='Valores ajustados')\n\n# Reta ajustada\nplt.plot(x, F[:,0], color='#D55E00', \n         label=fr'Reta de regressão: $\\hat{{y}} = {B[0,0]:.3f} + {B[1,0]:.3f}x$')\n\n# Configurações do gráfico\nplt.title(f'Regressão Linear Simples - MMQ\\nR² = {R2:.4f}', \n          fontsize=14, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#resumo-dos-resultados",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#resumo-dos-resultados",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "7 🎯 Resumo dos Resultados",
    "text": "7 🎯 Resumo dos Resultados\n\nprint(\"=\"*50)\nprint(\"         RESUMO DA REGRESSÃO LINEAR\")\nprint(\"=\"*50)\nprint(f\"Equação ajustada: y = {B[0,0]:.4f} + {B[1,0]:.4f}x\")\nprint(f\"Coeficiente de determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\nprint(\"=\"*50)\n\n==================================================\n         RESUMO DA REGRESSÃO LINEAR\n==================================================\nEquação ajustada: y = -0.2000 + 1.1000x\nCoeficiente de determinação (R²): 0.8643\nPorcentagem da variação explicada: 86.43%\n=================================================="
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#resumo-do-código",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#resumo-do-código",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "8 🧾 Resumo do Código",
    "text": "8 🧾 Resumo do Código\n\nInserção dos Dados\n\n\nx = [0, 1, 2, 3, 4]\ny = [0, 1, 1, 4, 4]\n\n\nDefinição das matrizes do sistema\n\n\nn = len(x)\nf0 = [1] * n\nf1 = x.copy()\n\nX = np.column_stack((f0, f1))\nY = np.array(y).reshape(n, 1)\n\n\nCálculo dos coeficientes\n\n\nXTX = X.T @ X\nXTY = X.T @ Y\nXTX_inv = np.linalg.inv(XTX)\nB = XTX_inv @ XTY\n\n\nQualidade do ajuste\n\n\nF = X @ B\ne = Y - F\nSQres = (e.T @ e)[0, 0]\n\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n\nR2 = 1 - (SQres / SQtot)"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#exercício-prático",
    "href": "conteudo/regressao_linear/mmq_regressao_linear_simples.html#exercício-prático",
    "title": "Método dos Mínimos Quadrados na Regressão Linear Simples",
    "section": "9 🚀 Exercício Prático",
    "text": "9 🚀 Exercício Prático\nImplemente o MMQ com novos dados:\n\n# Experimente com estes dados:\nx_novo = [1, 2, 3, 4, 5, 6]\ny_novo = [2, 4, 5, 4, 5, 7]\n\n# Dica: você pode copiar e adaptar o código acima!"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "",
    "text": "Objetivos\n\n\n\nNeste tutorial, vamos implementar o Método dos Mínimos Quadrados (MMQ) em Python para ajustar um modelo de regressão polinomial de segundo grau.\nObjetivo: Encontrar os coeficientes \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\beta_2\\) da equação \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) que melhor se ajustam aos nossos dados."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#introdução",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#introdução",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "",
    "text": "Objetivos\n\n\n\nNeste tutorial, vamos implementar o Método dos Mínimos Quadrados (MMQ) em Python para ajustar um modelo de regressão polinomial de segundo grau.\nObjetivo: Encontrar os coeficientes \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\beta_2\\) da equação \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) que melhor se ajustam aos nossos dados."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#importando-as-bibliotecas",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#importando-as-bibliotecas",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "2 🛠️ Importando as Bibliotecas",
    "text": "2 🛠️ Importando as Bibliotecas\nVamos começar importando as bibliotecas necessárias:\n\nimport pandas as pd           # Para manipulação de dados\nimport matplotlib.pyplot as plt  # Para criação e manipulação gráfica\nimport seaborn as sns        # Para criação e manipulação gráfica\nimport numpy as np           # Para operações matemáticas e matriciais"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#inserindo-os-dados",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#inserindo-os-dados",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "3 📊 Inserindo os Dados",
    "text": "3 📊 Inserindo os Dados\nVamos trabalhar com dados que apresentam uma relação quadrática. Ao invés de digitarmos os dados diretamente \\(y\\) e \\(x\\) como listas, iremos ler os dados a partir de um arquivo que está disponível no link regressao_polinomial_exemplo. O arquivo esta no formato .csv em que cada coluna é separada por uma vírgula, um tipo de formatação muito comum.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n\nUtilizando a função read_csv() da bilbioteca Pandas, os dados foram importados no formato de data frame, basicamento uma estrutura de dados em linhas e colunas, em que as colunas são denominadas de atributos.\n\nprint(df)\n\n   x   y\n0  0   5\n1  1   2\n2  2  10\n3  3   8\n4  4  15\n5  5  35"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#visualizando-os-dados",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#visualizando-os-dados",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "4 📈 Visualizando os Dados",
    "text": "4 📈 Visualizando os Dados\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n# Criando the gráfico de dispersão\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = df, x = 'x', y = 'y', color = '#0072B2', s=120, label='Dados observados')\n\n# Configurando o gráfico\nplt.title('Gráfico de Dispersão dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n📝 Observação 1: Aparentemente, um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressão linear simples. Nosso objetivo será explorar esse modelo e, ao final, compará-lo com o modelo linear.\n📝 Observação 2: Como importamos os dados diretamente de um arquivo .csv para o objeto df, utilizamos a função scatterplot da biblioteca Seaborn para plotar o gráfico de dispersão entre as variáveis \\(y\\) e \\(x\\)."
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#implementando-o-mmq-polinomial---passo-a-passo",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#implementando-o-mmq-polinomial---passo-a-passo",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "5 🧮 Implementando o MMQ Polinomial - Passo a Passo",
    "text": "5 🧮 Implementando o MMQ Polinomial - Passo a Passo\n\n5.1 Criando os Vetores Base\nPara o modelo polinomial \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\), precisamos dos vetores:\n\\[\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\]\n\n# Número de observações\nn = len(df['x'])\n\n# Vetor f0: vetor de 1's (para o intercepto β₀)\nf0 = [1] * n\n\n# Vetor f1: valores de x (para o coeficiente linear β₁)\nf1 = df['x'].copy()\n\n# Vetor f2: valores de x² (para o coeficiente quadrático β₂)\nf2 = np.array(df['x'])**2  # Eleva cada elemento de x ao quadrado\n\nVisualizando os vetores \\(\\vec{f}_0\\), \\(\\vec{f}_1\\) e \\(\\vec{f}_2\\).\n\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (termo linear):\", f1)\nprint(\"Vetor f2 (termo quadrático):\", f2)\n\nVetor f0 (intercepto): [1, 1, 1, 1, 1, 1]\nVetor f1 (termo linear): 0    0\n1    1\n2    2\n3    3\n4    4\n5    5\nName: x, dtype: int64\nVetor f2 (termo quadrático): [ 0  1  4  9 16 25]\n\n\n\n\n5.2 Construindo as Matrizes X e Y\nAgora vamos montar as matrizes do sistema polinomial:\n\\[X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 & \\vec{f}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\]\n\n# Matriz X: combinando f0, f1 e f2 em colunas\nX = np.column_stack((f0, f1, f2))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna\nY = np.array(df['y']).reshape(n, 1)\n\n\n\n5.3 Resolvendo o Sistema Normal\nCalculamos os coeficientes usando a mesma fórmula:\n\\[B = (X^T X)^{-1} X^T Y\\]\n\n# Calculando X transposta vezes X\nXTX = X.T @ X  # X.T é a transposta de X\n# Calculando X transposta vezes Y\nXTY = X.T @ Y\n# Calculando a matriz inversa (X^T X)^(-1)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\n# Coeficientes de regressão\nB = XTX_inv @ XTY\n\n\n\n\n\n\n\nInterpretação\n\n\n\n\n\\(\\beta_0\\) (intercepto): valor de y quando x = 0\n\\(\\beta_1\\) (coeficiente linear): relacionado à taxa de variação linear\n\\(\\beta_2\\) (coeficiente quadrático): relacionado à curvatura da parábola\n\nSe \\(\\beta_2 &gt; 0\\): parábola com concavidade para cima\nSe \\(\\beta_2 &lt; 0\\): parábola com concavidade para baixo\n\n\n\n\n\n\n5.4 Obtendo os Valores Ajustados de y\nTendo obtido os coeficientes de regressão, os valores ajustados de y (\\(\\hat{y}\\)) podem ser obtido pela multiplicação matricial:\n\\[F = XB = \\begin{bmatrix} 1 & x_1 & x^2_1 \\\\ 1 & x_2 & x^2_2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x^2_n \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\]\nObs.: denominamos \\(F\\) a matriz de valores ajustados de \\(y\\).\n\n# Valores ajustados (preditos)\nF = X @ B\n\n\n\n5.5 Avaliando a Qualidade do Ajuste\n\n5.5.1 Calculando a Soma dos Quadrados dos Resíduos (\\(SQ_{res}\\))\n\\(SQ_{res}\\) pode ser obtida pela multiplicação matricial:\n\\[SQ_{res} = \\boldsymbol{e}^T \\boldsymbol{e}\\]\nEm que \\(\\boldsymbol{e}\\) é a matriz coluna dos resíduos obtida pela diferença entre os valores observados e ajustados de \\(y\\):\n\\[\\boldsymbol{e} = Y - F = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\]\n\n# Resíduos: diferença entre valores observados e ajustados\ne = Y - F\n\n# Soma dos Quadrados dos Resíduos\nSQres = (e.T @ e)[0, 0]\n\n\n\n5.5.2 Calculando a Soma dos Quadrados Totais (\\(SQ_{tot}\\))\n\\(SQ_{tot}\\) pode ser obtida pela multiplicação matricial:\n\\[SQ_{tot} = \\boldsymbol{D}^T \\boldsymbol{D}\\]\nEm que \\(\\boldsymbol{D}\\) é a matriz coluna dos desvios da médis obtida pela diferença entre os valores observados de \\(y\\) e a média de \\(\\overline{y}\\):\n\\[\\boldsymbol{D} = Y - \\overline{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\overline{y} \\\\ \\overline{y} \\\\ \\vdots \\\\ \\overline{y} \\end{bmatrix} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}\\]\n\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n\n\n\n5.5.3 Calculando o coeficiente de determinação \\(R^2\\):\nO \\(R^2\\) é dado pela expressão:\n\\[R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}\\]\n\n# Coeficiente de Determinação R²\nR2 = 1 - (SQres / SQtot)\n\n\nVisualizando os resultados:\n\nprint(\"📊 Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos Resíduos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de Determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\n\n📊 Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos Resíduos (SQres): 59.2643\nSoma dos Quadrados Total (SQtot): 705.5000\nCoeficiente de Determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#visualizando-o-resultado-final",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#visualizando-o-resultado-final",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "6 📊 Visualizando o Resultado Final",
    "text": "6 📊 Visualizando o Resultado Final\nVamos plotar os dados originais junto com a curva ajustada:\nCriando uma linha contínua para \\(\\hat{y}\\)\n\n# Criando pontos para desenhar a curva suave\nx_curva = np.linspace(min(df['x']), max(df['x']), 100)\ny_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2\n\n\n# Criando o gráfico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nsns.scatterplot(data = df, x = 'x', y = 'y', \n                color = '#0072B2', s=120,\n                label=f'Dados observados (n={n})')\n\n# Valores ajustados\nplt.scatter(df['x'], F[:,0], \n           color='#000000', marker='*', s=120, \n           label='Valores ajustados')\n\n# Curva ajustada\nplt.plot(x_curva, y_curva, \n         color='#D55E00', \n         label=fr'Curva ajustada: $\\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')\n\n# Configurações do gráfico\nplt.title(f'Regressão Polinomial (2º grau) - MMQ\\nR² = {R2:.4f}', \n          fontsize=15, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#resumo-dos-resultados",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#resumo-dos-resultados",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "7 🎯 Resumo dos Resultados",
    "text": "7 🎯 Resumo dos Resultados\n\nprint(\"=\"*60)\nprint(\"         RESUMO DA REGRESSÃO POLINOMIAL\")\nprint(\"=\"*60)\nprint(f\"Equação ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}x²\")\nprint(f\"Coeficiente de determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\nprint(\"=\"*60)\n\n============================================================\n         RESUMO DA REGRESSÃO POLINOMIAL\n============================================================\nEquação ajustada: y = 5.7500 -4.5679x + 1.9821x²\nCoeficiente de determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%\n============================================================"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#comparação-linear-vs-polinomial",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#comparação-linear-vs-polinomial",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "8 🔍 Comparação: Linear vs Polinomial",
    "text": "8 🔍 Comparação: Linear vs Polinomial\nVamos comparar o ajuste linear e polinomial para os mesmos dados:\n\n# Ajuste LINEAR para comparação\nX_linear = np.column_stack((f0, f1))  # Apenas f0 e f1\nB_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)\n\n# R² do modelo linear\nF_linear = X_linear @ B_linear\nresiduos_linear = Y - F_linear\nSQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]\nR2_linear = 1 - (SQres_linear / SQtot)\n\nprint(\"📊 Comparação dos Modelos:\")\nprint(\"-\" * 40)\nprint(f\"Modelo Linear:     R² = {R2_linear:.4f}\")\nprint(f\"Modelo Polinomial: R² = {R2:.4f}\")\nprint(f\"Melhoria no R²:    {R2 - R2_linear:.4f}\")\n\n📊 Comparação dos Modelos:\n----------------------------------------\nModelo Linear:     R² = 0.7081\nModelo Polinomial: R² = 0.9160\nMelhoria no R²:    0.2079\n\n\nGráficos de dispersão\n\ny_linear = B_linear[0, 0] + B_linear[1, 0] * np.array(df['x'])\n\n# Gráfico comparativo\nplt.figure(figsize=(8, 6))\n\n# plt.subplot(1, 2, 1)\nsns.scatterplot(data = df, x = 'x', y = 'y', s=100, color = '#0072B2', label='Dados observados')\nplt.plot(df['x'], y_linear, color='#D55E00', label=f'Modelo Linear\\nR² = {R2_linear:.4f}')\nplt.plot(x_curva, y_curva, color='#009E73', label=f'Modelo Polinomial\\nR² = {R2:.4f}')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# plt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#resumo-do-código-modelo-polinomial",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#resumo-do-código-modelo-polinomial",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "9 🧾 Resumo do Código (modelo polinomial)",
    "text": "9 🧾 Resumo do Código (modelo polinomial)\n\nInserção dos Dados\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n\n\nDefinição das matrizes do sistema\n\n\nn = len(df['x'])\nf0 = [1] * n\nf1 = df['x'].copy()\nf2 = np.array(df['x'])**2\n\nX = np.column_stack((f0, f1, f2))\nY = np.array(df['y']).reshape(n, 1)\n\n\nCálculo dos coeficientes\n\n\nXTX = X.T @ X\nXTY = X.T @ Y\nXTX_inv = np.linalg.inv(XTX)\nB = XTX_inv @ XTY\n\n\nQualidade do ajuste\n\n\nY_ajustado = X @ B\ne = Y - Y_ajustado\nSQres = (e.T @ e)[0, 0]\n\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n\nR2 = 1 - (SQres / SQtot)"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#exercício-prático",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#exercício-prático",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "10 🚀 Exercício Prático",
    "text": "10 🚀 Exercício Prático\nTeste o código com novos dados:\n\n# Experimente com estes dados (padrão quadrático diferente):\ndf_novo = pd.DataFrame({\n  'x_novo': [1, 2, 3, 4, 5, 6, 7],\n  'y_novo': [30, 12, 18, 9, 7, 8, 6]\n})\n\nprint(df_novo)\n\n# Questões para investigar:\n# 1. Qual é o R² do modelo polinomial para estes dados?\n# 2. O coeficiente β₂ é positivo ou negativo? O que isso significa?\n# 3. Compare com o modelo linear - qual é a diferença no R²?\n\n# Implemente todo o processo do MMQ polinomial com os novos dados\n# Dica: você pode copiar e adaptar o código acima!\n\n   x_novo  y_novo\n0       1      30\n1       2      12\n2       3      18\n3       4       9\n4       5       7\n5       6       8\n6       7       6"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#conceitos-importantes-revisados",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#conceitos-importantes-revisados",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "11 💡 Conceitos Importantes Revisados",
    "text": "11 💡 Conceitos Importantes Revisados\n\nRegressão Polinomial: Extensão da regressão linear para relações curvas\nMatriz de Design: Agora com três colunas: \\([1, x, x^2]\\)\nInterpretação dos Coeficientes: Cada coeficiente tem significado específico\nComparação de Modelos: Uso do \\(R^2\\) para avaliar qual modelo é melhor"
  },
  {
    "objectID": "conteudo/regressao_linear/mmq_regressao_polinomial.html#próximos-passos",
    "href": "conteudo/regressao_linear/mmq_regressao_polinomial.html#próximos-passos",
    "title": "Método dos Mínimos Quadrados na Regressão Polinomial",
    "section": "12 🔗 Próximos Passos",
    "text": "12 🔗 Próximos Passos\n\nExperimente com polinômios de grau maior (\\(x^3\\), \\(x^4\\), etc.)\nInvestigue o conceito de overfitting com graus muito altos"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html",
    "title": "Regressão linear múltipla",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(patchwork)\nlibrary(gt)"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#abundância-de-aves-em-fragmentos-de-floresta",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#abundância-de-aves-em-fragmentos-de-floresta",
    "title": "Regressão linear múltipla",
    "section": "1 Abundância de aves em fragmentos de floresta",
    "text": "1 Abundância de aves em fragmentos de floresta\nLoyn (1987) conduziu um estudo para entender quais características do habitat estavam relacionadas à abundância de aves da floresta (acesse o artigo aqui). Para isso, ele selecionou 56 fragmentos de floresta no sudeste de Victoria, Austrália, e registrou a abundância de aves da floresta (ABUND) em cada fragmento como variável de resposta.\nAs variáveis preditoras registradas para cada fragmento incluíram:\n\nÁrea do fragmento (ha): AREA\nDistância ao fragmento mais próximo (km): DIST\nDistância ao fragmento maior mais próximo (km):LDIST\nNúmero de anos desde que o fragmento foi isolado por desmatamento (anos):YR.ISOL\nÍndice de histórico de pastagem, de 1 (leve) a 5 (pesado):GRAZE\nAltitude média (m):ALT\n\nInicialmente, vamos nos concentrar nas variáveis YR.ISOL, GRAZE e ALT.\nImporte a base de dados loyn.csv\n\nloyn = read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/loyn.csv\")\n\nhead(loyn) |&gt; gt()\n\n\n\n\n\n\n\nABUND\nAREA\nYR.ISOL\nDIST\nLDIST\nGRAZE\nALT\n\n\n\n\n5.3\n0.1\n1968\n39\n39\n2\n160\n\n\n2.0\n0.5\n1920\n234\n234\n5\n60\n\n\n1.5\n0.5\n1900\n104\n311\n5\n140\n\n\n17.1\n1.0\n1966\n66\n66\n3\n160\n\n\n13.8\n1.0\n1918\n246\n246\n5\n140\n\n\n14.1\n1.0\n1965\n234\n285\n3\n130"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#gráficos-de-dispersão-entre-abund-e-cada-uma-das-demais-variáveis-preditoras",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#gráficos-de-dispersão-entre-abund-e-cada-uma-das-demais-variáveis-preditoras",
    "title": "Regressão linear múltipla",
    "section": "2 Gráficos de dispersão entre ABUND e cada uma das demais variáveis preditoras",
    "text": "2 Gráficos de dispersão entre ABUND e cada uma das demais variáveis preditoras\n\nplt_gr &lt;- ggplot(loyn) +\n  aes(y = ABUND, x = GRAZE) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1)\n\nplt_al &lt;- ggplot(loyn) +\n  aes(y = ABUND, x = ALT) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1)\n\nplt_yr &lt;- ggplot(loyn) +\n  aes(y = ABUND, x = YR.ISOL) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 1)\n\n\nplt_gr + plt_al + plt_yr"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#multicolinearidade-as-variáveis-preditoras-são-correlacionadas-entre-si",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#multicolinearidade-as-variáveis-preditoras-são-correlacionadas-entre-si",
    "title": "Regressão linear múltipla",
    "section": "3 Multicolinearidade: as variáveis preditoras são correlacionadas entre si?",
    "text": "3 Multicolinearidade: as variáveis preditoras são correlacionadas entre si?\n\nggpairs(loyn |&gt; select(GRAZE, ALT, YR.ISOL))\n\n\n\n\n\n\n\n\nAs variáveis ALT versus GRAZE e GRAZE versus YR.ISOL parecem ter um grau de correlação moderado entre si."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#o-modelo-de-regressão-múltipla",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#o-modelo-de-regressão-múltipla",
    "title": "Regressão linear múltipla",
    "section": "4 O modelo de regressão múltipla",
    "text": "4 O modelo de regressão múltipla\nO modelo de regressão linear múltipla é dado por:\n\\[ABUND_i = \\beta_0 + \\beta_1 ALT_i + \\beta_2 YR.ISIOL_i + \\epsilon_i\\]\nNo R pode ser ajustado por:\n\nmfull &lt;- lm(ABUND ~ ALT + YR.ISOL, data  = loyn)\nmfull\n\n\nCall:\nlm(formula = ABUND ~ ALT + YR.ISOL, data = loyn)\n\nCoefficients:\n(Intercept)          ALT      YR.ISOL  \n -348.47698      0.07006      0.18348  \n\n\nO resumo do modelo pode ser visto com a função summary\n\nsummary(mfull)\n\n\nCall:\nlm(formula = ABUND ~ ALT + YR.ISOL, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.9745  -6.4690   0.6168   7.4408  24.0155 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -348.47698   93.73407  -3.718 0.000486 ***\nALT            0.07006    0.02852   2.457 0.017329 *  \nYR.ISOL        0.18348    0.04852   3.781 0.000398 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.953 on 53 degrees of freedom\nMultiple R-squared:  0.3297,    Adjusted R-squared:  0.3044 \nF-statistic: 13.03 on 2 and 53 DF,  p-value: 2.49e-05"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#hipótese-nula-e-comparação-de-modelos",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#hipótese-nula-e-comparação-de-modelos",
    "title": "Regressão linear múltipla",
    "section": "5 Hipótese nula e comparação de modelos",
    "text": "5 Hipótese nula e comparação de modelos\nA hipótese nula (\\(H_0\\)) básica que podemos testar ao ajustar um modelo de regressão linear múltipla é que todas as inclinações de regressão parciais são iguais a zero, ou seja, \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_j = 0\\). Neste exemplo, \\(H_0\\) é que o coeficiente de inclinação dos níveis de pastagem e os anos de isolamento do fragmento sejam ambos iguais a zero e, consequentemente, não têm influência sobre a abundância.\nTestamos a hipótese nula com a ANOVA na regressão múltipla, que divide a variação total de \\(Y\\) em dois componentes: a variação explicada pela regressão linear com \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_j\\) e a variação residual.\nSe \\(H_0\\) for verdadeira, tanto o quadrado médio da regressão \\(QM_{Regressão}\\) quanto o quadrado médio do resíduo (\\(QM_{Resíduo}\\)) estimarão \\(\\sigma^2\\), e a razão \\(F\\) entre eles será igual a 1. Se \\(H_0\\) for falsa, pelo menos uma das inclinações de regressão parciais não será igual a zero e \\(QM_{Regressão}\\) estimará \\(\\sigma^2\\) mais um termo \\(QM_{Regressão}\\) o que representa essas inclinações de regressão parciais. Portanto, a razão \\(F = \\frac{QM_{Regressão}}{QM_{Regressão}} &gt; 1\\). Neste caso, a decisão de aceitar \\(H_0\\) é feita pela comparação do \\(F\\) calculado com a distribuição \\(F\\) apropriada, da mesma forma que fazemos com a regressão linear simples ou com a Análise de Variância.\nO resultado da razão \\(F\\) aparece no comando summary, que no exemplo acima é F = 13.035, com valor de p = 2.5^{-5}.\nTambém podemos testar as hipóteses nulas sobre cada coeficiente de regressão parcial, ou seja, de que qualquer \\(\\beta_1\\) seja igual a zero. Para isto, podemos usar a estratégia de comparação de modelos em que o modelo completo (aquele com todas as variáveis) é comparado com o modelo reduzido (aquele sem a variável \\(X_1\\) de interesse).\nPara testar o efeito da altitude, por exemplo, o modelo reduzido é:\n\\(ABUND_i = \\beta_0 + \\beta_2 YR.ISIOL_i + \\epsilon_i\\)\nO modelo completo tem soma dos quadrados (\\(SQ\\)) maior que o modelo reduzido. Para comparar o ganho extra que o modelo completo tem sobre o modelo reduzido podemos fazer:\n\\(SS_{extra} = SS_{Regressão_{completo}} - SS_{Regressão_{reduzido}}\\)\nEm seguida, calculamos o quadrado médio extra como \\(QM_{extra} = \\frac{SS_{extra}}{gl}\\) e usamos o teste \\(F\\) como:\n\\(F = \\frac{QM_{extra}}{QM_{Resíduo_{completo}}}\\)\nO mesmo pode ser feito para a variável \\(YR.ISOL\\).\nNo R, podemos testar os efeitos dos coeficientes parciais de regressão com o comando drop1.\n\ndrop1(mfull, test = 'F')\n\nSingle term deletions\n\nModel:\nABUND ~ ALT + YR.ISOL\n        Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;               4248.3 248.42                      \nALT      1    483.79 4732.1 252.46  6.0355 0.0173292 *  \nYR.ISOL  1   1146.10 5394.4 259.80 14.2982 0.0003979 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nVemos aqui que os dois componentes (ALT e YR.ISOL) adicionam uma variação explicada significativa, isto é, para os dois coeficientes \\(p \\le 0,005\\).\nNote que o teste o teste F de comparação de modelos foi equivalente ao teste \\(t\\) aplicado a cada coeficiente e que pode ser visto no resultado da função summary, sendo \\(F = t^2\\). No entanto, a estratégia de comparação de modelos apresentada aqui permite a comparação não somente de coeficientes isolados, mas de qualquer combinação específica dos coeficientes em comparação com o modelo completo."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#coeficiente-de-determinação-r2",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#coeficiente-de-determinação-r2",
    "title": "Regressão linear múltipla",
    "section": "6 Coeficiente de determinação (\\(R^2\\))",
    "text": "6 Coeficiente de determinação (\\(R^2\\))\nNa regressão múltipla, o coeficiente de determinação \\(R^2\\) mede a proporção da variabilidade total da variável resposta que é explicada pelas variáveis preditoras. No entanto, \\(R^2\\) tende a aumentar à medida que mais preditores são adicionados ao modelo, mesmo que não sejam significativos. Para corrigir essa inflação, utilizamos o coeficiente de determinação ajustado (\\(R^2_{ajustado}\\)), que ajusta o \\(R^2\\) considerando o número de preditores no modelo e o tamanho da amostra. O \\(R^2_{ajustado}\\) penaliza a adição de preditores irrelevantes, proporcionando uma avaliação mais precisa da qualidade do ajuste do modelo e pode ser obtido pela expressão:\n\\(R^2_{ajustado} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\\)\nNo resultado da função summary vemos que o \\(R^2 = 0.33\\) e o \\(R^2_{ajustado} = 0.304\\)."
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#pressupostos-da-regressão-linear-múltipla",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#pressupostos-da-regressão-linear-múltipla",
    "title": "Regressão linear múltipla",
    "section": "7 Pressupostos da regressão linear múltipla",
    "text": "7 Pressupostos da regressão linear múltipla\n\n7.1 Normalidade dos resíduos\n\nloyn &lt;- loyn |&gt; \n  mutate(rst = rstudent(mfull),\n         yaj = fitted(mfull))\n\n\nggplot(loyn) +\n  aes(x = rst, y = after_stat(density)) +\n  geom_histogram(bins = 10, density = TRUE, color = 'white') +\n  geom_density(color = 'darkblue', linewidth = 2)\n\n\n\n\n\n\n\n\n\nshapiro.test(loyn$rst)\n\n\n    Shapiro-Wilk normality test\n\ndata:  loyn$rst\nW = 0.97409, p-value = 0.2687\n\n\n\n\n7.2 Gráfico de resíduos\n\nggplot(loyn) +\n  aes(x = yaj, y = rst) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n7.3 Índice de Alavancagem (Leverage)\n\ninfl &lt;- influence.measures(mfull)$infmat |&gt; \n  as.data.frame()\n\nggplot(infl) +\n  aes(y = hat, x = 1:nrow(infl)) +\n  geom_point() +\n  ylab('Leverage')\n\n\n\n\n\n\n\n\n\n\n7.4 Índice de Alavancagem de Cook (Dcook)\nO índice de alavancagem de Cook é uma medida que combina a magnitude do efeito de alavancagem de uma observação com o quanto essa observação influencia a estimativa dos coeficientes de regressão. Uma observação com \\(D_{Cook} &gt; 1\\) é frequentemente considerada influente e devem ser examinada para avaliar seu impacto no modelo.\nPara obter o índice de alavancagem de Cook em R:\n\nggplot(infl) +\n  aes(y = cook.d, x = 1:nrow(infl)) +\n  geom_point() +\n  ylab('Distância de Cook')"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#outros-dignósticos",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#outros-dignósticos",
    "title": "Regressão linear múltipla",
    "section": "8 Outros dignósticos",
    "text": "8 Outros dignósticos\n\n8.1 Resíduos versus variáveis preditoras\n\nggplot(loyn) +\n  aes(x = ALT, y = rst) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(loyn) +\n  aes(x = YR.ISOL, y = rst) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n8.2 Resíduos dos modelos reduzidos versus variáveis preditoras\nNeste gráficos, ajustamos os modelos reduzidos excluindo uma variável preditora por vez e plotamos os resíduos deste modelo com a variável preditora excluída. Uma tendência neste gráfico indica que a inclusão da variável no modelo ajudaria a reduzir a variação residual.\n\nmpalt &lt;- lm(ABUND ~ YR.ISOL, data  = loyn) # Modelo reduzido sem ALT\nplot(rstudent(mpalt) ~ loyn$ALT)\nabline(lm(rstudent(mpalt) ~ loyn$ALT))\n\n\n\n\n\n\n\n\n\nmpisol &lt;- lm(ABUND ~ ALT, data  = loyn) # Modelo reduzido sem YR.ISOL\nplot(rstudent(mpisol) ~ loyn$YR.ISOL)\nabline(lm(rstudent(mpisol) ~ loyn$YR.ISOL))"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#mais-sobre-multicolinearidade",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#mais-sobre-multicolinearidade",
    "title": "Regressão linear múltipla",
    "section": "9 Mais sobre multicolinearidade",
    "text": "9 Mais sobre multicolinearidade\nVariáveis preditoras correlacionadas entre si caracteriza a multicolinearidade. Quando severa, a multicolinearidade pode afetar a estimativa dos parâmetros da regressão, pois pequenas alterações nos dados ou inclusão/remoção de variáveis podem causar grandes mudanças nos coeficientes estimados da regressão. Além disso, a presença de multicolinearidade pode inflar os erros padrões dos coeficientes de regressão, resultando em um modelo globalmente significativo, mas com coeficientes individuais que não são estatisticamente diferentes de zero.\nAvaliar uma matriz de correlação entre pares de variáveis preditoras pode ser a primeira e mais simples forma de explorar a presença de colinearidade. Outra forma é avaliar a tolerância de cada variável preditora \\(X_j\\) por meio de \\(1 - R^2_j\\), em que \\(R^2_j\\) é o coeficiente de determinação do modelo em que \\(X_j\\) é relacionada às demais \\(1 - p\\) variáveis preditoras. Geralmente, esta tolerância é expressa na forma do índice de inflação da variação (variance inflation factor - \\(VIF\\)) para cada variável preditora, em que:\n\\[VIF_j =\\frac{1}{1 - R^2_j}\\]\nValores elevados indicam que a presença de colinearidade devido a variável \\(X_j\\). Diferente níveis de corte são propostos como indicadores da presença de multicolinearidade \\(VIF &gt; 5\\), \\(VIF &gt; 10\\) ou \\(VIF &gt; 20\\)\nTodos os coeficientes \\(VIF_j\\) podem ser encontrados em uma única operação calculando a inversa da matriz de correlação, \\(\\mathbb{R^{1}}\\) entre as variáveis de interesse. Os elementos diagonais dessa matriz inversa são os coeficientes \\(VIF_j\\). Vimos que GRAZE era correlacionada com ALT e com YR.ISOL. Os ceoficientes \\(VIF\\) para estas variáveis podem ser obtidos por:\n\nvif &lt;- loyn |&gt; \n  select(GRAZE, ALT, YR.ISOL) |&gt; \n  cor() |&gt; \n  solve() |&gt; \n  diag()\n\nvif\n\n   GRAZE      ALT  YR.ISOL \n1.904799 1.200372 1.679995 \n\n\nO \\(VIF\\) para GRAZE é maior que os demais, porém longe do limite \\(VIF &gt; 5\\). Vejamos entretanto o que ocorre com o modelo para abundância se inserimos estas três variáveis:\n\nmfull2 &lt;- lm(ABUND ~ GRAZE + ALT + YR.ISOL, data = loyn)\nsummary(mfull2)\n\n\nCall:\nlm(formula = ABUND ~ GRAZE + ALT + YR.ISOL, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.5498  -4.8951   0.6504   4.7798  20.2384 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -73.58185  107.24995  -0.686 0.495712    \nGRAZE        -4.01692    0.99881  -4.022 0.000188 ***\nALT           0.03285    0.02679   1.226 0.225618    \nYR.ISOL       0.05143    0.05393   0.954 0.344719    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.894 on 52 degrees of freedom\nMultiple R-squared:  0.4887,    Adjusted R-squared:  0.4592 \nF-statistic: 16.57 on 3 and 52 DF,  p-value: 1.106e-07\n\n\nNote que agora, somente GRAZE aparece com coeficiente estatísticamente diferente de \\(0\\) e \\(R^2_{ajustado}\\) aumenta de 0.304 para 0.459"
  },
  {
    "objectID": "conteudo/regressao_linear/regressao_linear_multipla.html#tranformações",
    "href": "conteudo/regressao_linear/regressao_linear_multipla.html#tranformações",
    "title": "Regressão linear múltipla",
    "section": "10 Tranformações",
    "text": "10 Tranformações\nTransformações podem frequentemente ser eficazes se a distribuição em situações em que as variáveis preditoras apresentem distribuições assimétricas. Vamos incluir a AREA no modelo de regressão. Vejamos os graficos de disperção entre as variáveis preditoras.\n\nggpairs(loyn |&gt; select(AREA, GRAZE, ALT, YR.ISOL))\n\n\n\n\n\n\n\n\nHá uma relação fortemente assimétrica da variável área, em que poucos framentos são muito maiores. Vejamos as associações par a par utilizando a transfomação \\(log(AREA)\\)\n\nloyn$lAREA &lt;- log(loyn$AREA)\nggpairs(loyn |&gt; select(lAREA, GRAZE, ALT, YR.ISOL))\n\n\n\n\n\n\n\n\nA transformação resolveu o problema da assimetria.\nVamos ajustar agora o modelo de regressão:\n\nmfull3 &lt;- lm(ABUND ~ lAREA + GRAZE + ALT + YR.ISOL, data = loyn)\nsummary(mfull3)\n\n\nCall:\nlm(formula = ABUND ~ lAREA + GRAZE + ALT + YR.ISOL, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.4245  -3.3341   0.6227   2.6759  15.3290 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -141.88574   86.23728  -1.645   0.1061    \nlAREA          3.07303    0.55118   5.575 9.41e-07 ***\nGRAZE         -1.60127    0.90538  -1.769   0.0829 .  \nALT            0.02586    0.02136   1.210   0.2317    \nYR.ISOL        0.07991    0.04323   1.848   0.0703 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.283 on 51 degrees of freedom\nMultiple R-squared:  0.6823,    Adjusted R-squared:  0.6574 \nF-statistic: 27.39 on 4 and 51 DF,  p-value: 3.671e-12\n\n\nO resultado indica que somente o \\(log(AREA)\\) seja importante para predizer a abundância. Entretanto, lembre-se que havia uma certo padrão de colinearidade entre GRAZE, ALT a YR.ISOL. Teste retirar uma a uma estas variáveis do modelo e avalie os resultados."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquantquali.html",
    "href": "conteudo/medidas_associacao/biquantquali.html",
    "title": "Associação entre variáveis quantitativas e qualitativas",
    "section": "",
    "text": "Pacotes e funções utilizadas no capítulo\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(gridExtra)\nsource('scripts/anova_sim.r')\nNeste capítulo vamos descrever a associação entre uma variável \\(Y\\) contínua e uma variável \\(X\\) categórica denominadas respectivamente de variável dependente (ou resposta) e variável independente (ou preditora)\nAssumimos explicitamente que \\(Y\\) é função (depende) de \\(X\\) e não o contrário. O nome variável preditora vem do fato que, se \\(Y\\) e \\(X\\) estão associadas, ao conhecemos \\(X\\) somos capazer de predizer a resposta média em \\(Y\\)."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquantquali.html#visualizando-a-distribuição-de-y-em-diferentes-grupos",
    "href": "conteudo/medidas_associacao/biquantquali.html#visualizando-a-distribuição-de-y-em-diferentes-grupos",
    "title": "Associação entre variáveis quantitativas e qualitativas",
    "section": "1 Visualizando a distribuição de \\(Y\\) em diferentes grupos",
    "text": "1 Visualizando a distribuição de \\(Y\\) em diferentes grupos\nImporte a base de dados medley.csv (disponível também em Chapter 10 - Single factor classification (ANOVA)) que avalia o impacto da presença de metais pesados na diversidade de espécies de diatomácias em riachos (Medley e Clements (1998); Queen, Quinn, e Keough (2002); Logan (2011)).\n\nmedley = read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/medley.csv\") |&gt; \n  mutate(STREAM = factor(STREAM),\n         ZINC = factor(ZINC, ordered = TRUE,\n                       levels = c(\"BACK\", \"LOW\", \"MED\", \"HIGH\")))\nvar_medley = colnames(medley)\nstream_levels = levels(medley$STREAM)\nn_stream = nlevels(medley$STREAM)\nzinc_levels = levels(medley$ZINC)\nn_zinc = nlevels(medley$ZINC)\n\n\nmedley |&gt; gt()\n\n\n\n\n\n\n\nSTREAM\nZINC\nDIVERSITY\n\n\n\n\nEagle\nBACK\n2.27\n\n\nEagle\nHIGH\n1.25\n\n\nEagle\nHIGH\n1.15\n\n\nEagle\nMED\n1.62\n\n\nBlue\nBACK\n1.70\n\n\nBlue\nHIGH\n0.63\n\n\nBlue\nBACK\n2.05\n\n\nBlue\nBACK\n1.98\n\n\nBlue\nHIGH\n1.04\n\n\nBlue\nMED\n2.19\n\n\nBlue\nMED\n2.10\n\n\nSnake\nBACK\n2.20\n\n\nSnake\nMED\n2.06\n\n\nSnake\nHIGH\n1.90\n\n\nSnake\nHIGH\n1.88\n\n\nSnake\nHIGH\n0.85\n\n\nArkan\nLOW\n1.40\n\n\nArkan\nLOW\n2.18\n\n\nArkan\nLOW\n1.83\n\n\nArkan\nLOW\n1.88\n\n\nArkan\nMED\n2.02\n\n\nArkan\nMED\n1.94\n\n\nArkan\nLOW\n2.10\n\n\nChalk\nLOW\n2.38\n\n\nChalk\nHIGH\n1.43\n\n\nChalk\nHIGH\n1.37\n\n\nChalk\nMED\n1.75\n\n\nChalk\nLOW\n2.83\n\n\nSplat\nBACK\n1.53\n\n\nSplat\nBACK\n0.76\n\n\nSplat\nMED\n0.80\n\n\nSplat\nLOW\n1.66\n\n\nSplat\nMED\n0.98\n\n\nSplat\nBACK\n1.89\n\n\n\n\n\n\n\nA coluna STREAM é uma variável categórica contendo o nome dos \\(6\\) riachos amostrados (Arkan, Blue, Chalk, Eagle, Snake, Splat). A coluna ZINC é uma variável categórica ordinal com \\(4\\) níveis de concentração de zinco na água (BACK &lt; LOW &lt; MED &lt; HIGH). O primeiro nível (BACK) é o nível de referência (BACKGROUND). Finalmente, a coluna DIVERSITY é uma variável contínua que contém a diversidade de diatomácieas (medida pelo índice de diversidade de Shannon medida de cada uma das 34 amostras.\nVamos nos concentrar nas variáveis DIVERSITY e ZINC. DIVERSITY será a variável resposta. Em delineamento experimental, dizemos que ZINC é um tratamento, isto é, uma condição experimental sob a qual nossa variável dependente \\(Y\\) foi mensurada.\nPara verificarmos a distribuição de diversidade para cada concentração de zinco poderíamos fazer um gráfico de dispersão. A diferença agora é que \\(X\\) trata-se de uma variável categórica ordinal com \\(4\\) níveis.\n\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_point() +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\n\n1.1 Boxplots para os níveis do tratamento\nNão há problema em apresentarmos um gráfico de dispersão. No entanto, em situações deste tipo estamos comumente interessados em representar medidas-resumo que nos permitam comparar os diferentes níveis do tratamento. A forma mais comum de representar esta situação é por meio de um boxplot para cada nível do tratamento.\n\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_boxplot() +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nNa figura acima estão representadas a mediana, os quartis (\\(1^o\\) e \\(3^o\\)) e os pontos máximo e mínimo para cada nível do tratamento. Alguns pontos extremos podem aparecer isoladamente para indicar que estão muito distantes dos demais. Podemos controlar esta representação com o argumento coef na função geom_boxplot.\n\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_boxplot(coef = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nVemos que o boxplot referente ao nível HIGH está em uma posição inferior aos demais, sugerindo que a diversidade de diatomáceas tende a ser mais baixa para níveis elevados de zinco.\nExitem outras variações que podem nos ajudar a entender melhor os padrões. Podemos sobrepor os pontos individuais sobre os boxplots:\n\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_boxplot(coef = 3) +\n  geom_point(size = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\n\n\n1.2 O gráfico de erros\nNas figuras acima representamos os quartis das distribuições. Podemos estar interessados em apresentar somente os pontos médios (média aritimética) juntamente com barras de erro que representem alguma medida de dispersão (ex. desvio padrão). Para isto é necessário inicialmente criar um data.frame com estas medidas.\n\nmedley_barras = medley |&gt; \n  group_by(ZINC) |&gt; \n  summarise(Media = mean(DIVERSITY),\n            Desvio = sd(DIVERSITY))\nmedley_barras |&gt; \n  gt()\n\n\n\n\n\n\n\nZINC\nMedia\nDesvio\n\n\n\n\nBACK\n1.797500\n0.4852613\n\n\nLOW\n2.032500\n0.4449960\n\n\nMED\n1.717778\n0.5030104\n\n\nHIGH\n1.277778\n0.4268717\n\n\n\n\n\n\n\nE em seguida plotar a figura.\n\nggplot(medley_barras, aes(x = ZINC)) +\n  geom_point(aes(y = Media), size = 3) +\n  geom_errorbar(aes(ymin = Media - Desvio,\n                    ymax = Media + Desvio), width = 0.4) +\n  labs(y = 'DIVERSITY') +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nAqui vemos somente os pontos médios e as barras de erro, que estão à distância de \\(1\\) desvio padrão acima e abaixo da média (\\(\\overline{Y} \\pm 1s\\)). Embora tenhamos expressado as distâncias utilizado o desvio padrão como medida de variação, poderíamos ter utilizado outras medidas como o erro padrão ou o intervalo de confiança O importante é sempre deixar claro qual medida de variação está semdo representada no gráfico de erros (Veja: Krzywinski & Altman, 2013 - Error bars - Points of Significance)."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquantquali.html#partição-das-soma-dos-quadrados",
    "href": "conteudo/medidas_associacao/biquantquali.html#partição-das-soma-dos-quadrados",
    "title": "Associação entre variáveis quantitativas e qualitativas",
    "section": "2 Partição das Soma dos Quadrados",
    "text": "2 Partição das Soma dos Quadrados\nAo representarmos a distribuição de uma variável \\(Y\\) contínua em função de uma variável \\(X\\) categórica, geralmente estamos interessados em determinar se os diferentes níveis de \\(X\\) (diferentes grupos) têm médias similares ou se ao menos um dos níveis têm média diferente dos demais. Queremos uma medida que nos permita diferenciar situações como as apresentadas abaixo.\n\n\n\n\n\n\n\n\n\nNa figura \\(A\\) todos os grupos são provenientes da mesma distribuição e têm médias aproximadamente iguais (\\(\\overline{Y}_A \\approx  \\overline{Y}_B \\approx \\overline{Y}_C \\approx \\overline{Y}_D\\)). Na figura \\(B\\) o segundo grupo tem média mais elevada que os demais, e na da figura \\(C\\), todas as médias parecem ser diferentes entre si (\\(\\overline{Y}_A \\ne  \\overline{Y}_B \\ne \\overline{Y}_C \\ne \\overline{Y}_D\\)).\nPara mensurar o grau de associação entre \\(Y\\) e \\(X\\) e entender como podemos diferenciar as situações acima, vamos introduzir o processo de Partição da Soma dos Quadrados.\nSuponha a situção abaixo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotações\n\n\n\n\nTemos \\(k = 3\\) grupos (A, B ou C) e para cada grupo \\(n =  5\\) observações. Denotamos por \\(n_{ij}\\) o número de observações dentro de cada grupo, em que \\(i\\) é a i-ésima observação (\\(i = 1\\) a \\(5\\)) do j-ésimo grupo (\\(j = 1\\) a \\(3\\) - grupos A ao C). Neste exemplo, o número de observações em cada grupo é o mesmo (\\(n_1 = n_2 = n_3 = n\\)), de modo que o total de observações é dado por:\n\n\\(N = k \\times n = n_1 + n_2 + n_3 = 15\\)\n\nA média de cada grupo será denotada por \\(\\overline{Y}_j\\), que neste exemplo são: \\(Y_1 = 20.64\\) (grupo A), \\(Y_2 = 28.68\\) (grupo B) e \\(Y_3 = 12.18\\) (grupo C).\nVamos denotar por \\(\\overline{\\overline{Y}}\\) a Grande Média, isto é, a média geral de todas as observações independente do grupo de origem.\n\n\\[\\overline{\\overline{Y}} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}\\frac{Y_{ij}}{N} = \\frac{\\overline{Y_1} + \\overline{Y_2} + \\overline{Y_3}}{3} = 20.5\\]\n\n\nPodemos agora observar estes elementos no gráfico de dispersão.\n\n\n\n\n\n\n\n\n\nEm seguida, precisamos calcular \\(3\\) quantias, a Soma dos Quadrados Totais (\\(SQ_{Total}\\)), a Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\) e a Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\).\n\nSoma dos Quadrados Totais \\(SQ_{Total}\\): mede as diferenças entre \\(Y_{ij}\\) e \\(\\overline{\\overline{Y}}\\). Temos nesta expressão o somatório dos desvios ao quadrado de todas as observações com relação à grand, fig.align=‘center’, fig.width=8, fig.height=4e média independente do grupo de origem de cada observação.\n\n\\[SQ_{Total} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\): mede as diferenças entre as médias dos tratamentos \\(\\overline{Y}_j\\) e \\(\\overline{\\overline{Y}}\\), sendo portanto os desvios ao quadrado da média de cada tratamento subtraída da grande média. \\(SQ_{Trat}\\) também é chamada de soma dos quadrados entre grupos ou entre tratamentos\n\n\\[SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Resíduos \\(SQ_{Res}\\): mede as diferenças entre cada observação \\(Y_{ij}\\) e a média de seu próprio grupo \\(\\overline{Y}_{j}\\). \\(SQ_{Res}\\) também é chamada de soma dos quadrados dentro dos grupos ou dentro dos tratamentos\n\n\\[SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2\\]\n\n\n\n\n\n\nA característica aditiva das somas dos quadrados\n\n\n\nA partição da soma dos quadrados consiste em decompor a variação total do experimento em uma parcela atribuída à variação entre tratamentos e outra parcela da variação dentro dos tratamentos. Isto é possível pois as somas dos quadrados definidas acima podem ser expressas de forma aditiva como:\n\\[SQ_{Total} = SQ_{Trat} + SQ_{Res}\\]\nDeste modo, é possível demostrar que:\n\\(\\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(Y_{j} - \\overline{\\overline{Y}})^2 + \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{Y}_{j})^2\\)"
  },
  {
    "objectID": "conteudo/medidas_associacao/biquantquali.html#medindo-a-associação-entre-y-e-x",
    "href": "conteudo/medidas_associacao/biquantquali.html#medindo-a-associação-entre-y-e-x",
    "title": "Associação entre variáveis quantitativas e qualitativas",
    "section": "3 Medindo a associação entre \\(Y\\) e \\(X\\)",
    "text": "3 Medindo a associação entre \\(Y\\) e \\(X\\)\nA característica aditiva das somas dos quadrados pode ser utilizada para mensurar o grau de dependência de \\(Y_{ij}\\) com respeito aos diferentes tratamentos. Compare as duas figuras abaixo:\n\n\n\n\n\n\n\n\n\nA soma dos quadrados dentro dos grupos é a mesma nas duas figuras (\\(SQ_{Res} = 362.6\\)). No entanto, na figura da esquerda, em que as médias dos tratamentos são similares (e consequentemente próximas à grande média), a soma dos quadrados entre os tratamentos é muito menor (\\(SQ_{Trat}^{esquerda} = 15.8\\)) que na figura da direita, em que as médias dos tratamentos estão distantes entre si (\\(SQ_{Trat}^{direita} = 680.8\\)). É desta forma que a partição das somas dos quadrados nos permite diferenciar situações em que: i - a média dos grupos depende dos níveis do tratamento (figura da direita); de situações em que ii - a média não depende dos níveis do tratamento (figura da esquerda).\n\n3.1 O coeficiente de determinação (\\(R^2\\))\nPodemos expressar a relação entre \\(SQ_{Trat}\\) e \\(SQ_{Total}\\) pela expressão:\n\n\\[R^2 = \\frac{SQ_{Trat}}{SQ_{Trat} + SQ_{Res}} = \\frac{SQ_{Trat}}{SQ_{Total}}\\]\n\n\\(R^2\\) é chamado de coeficiente de determinação e varia entre \\(0\\) e \\(1\\). Se \\(R^2 = 0\\) toda a variação em \\(Y\\) é causada por \\(SQ_{Res}\\) (\\(\\overline{\\overline{Y}} = \\overline{Y}_1 = \\overline{Y}_2 = \\cdots = \\overline{Y}_k\\)). À medida que as médias dos tratamentos se distanciam umas das outras, \\(R^2\\) se aproxima de \\(1\\) pois a maior parte da variação em \\(Y\\) é causada por \\(SQ_{Trat}\\)."
  },
  {
    "objectID": "conteudo/medidas_associacao/biquantquali.html#partição-das-soma-dos-quadrados-no-ambiente-r",
    "href": "conteudo/medidas_associacao/biquantquali.html#partição-das-soma-dos-quadrados-no-ambiente-r",
    "title": "Associação entre variáveis quantitativas e qualitativas",
    "section": "4 Partição das Soma dos Quadrados no ambiente R",
    "text": "4 Partição das Soma dos Quadrados no ambiente R\nVoltando ao conjundo de dados medley.csv, uma forma de obter os somatórios dos quadrados no R é utilizando a função aov (mas veja também a função lm).\n\naov(DIVERSITY ~ ZINC, data = medley)\n\nCall:\n   aov(formula = DIVERSITY ~ ZINC, data = medley)\n\nTerms:\n                    ZINC Residuals\nSum of Squares  2.566612  6.516411\nDeg. of Freedom        3        30\n\nResidual standard error: 0.4660619\nEstimated effects may be unbalanced\n\n\nO resultado retorna a soma dos quadrados dos tratamentos (neste caso a coluna ZINC) e dos resíduos (coluna Residuals). Como o \\(SQ_{Total}\\) é simplesmente a soma dos dois anteriores, podemos obtê-lo facilmente:\n\nsq = aov(DIVERSITY ~ ZINC, data = medley)\nSQTrat = anova(sq)$`Sum Sq`[1]\nSQRes = anova(sq)$`Sum Sq`[2]\nSQTotal = SQTrat + SQRes\n\nSQTotal\n\n[1] 9.083024\n\n\nPor fim, o \\(R^2\\) pode ser calculado por:\n\nR2 = SQTrat / SQTotal\nR2\n\n[1] 0.2825725"
  },
  {
    "objectID": "conteudo/medidas_associacao/series.html",
    "href": "conteudo/medidas_associacao/series.html",
    "title": "1 Séries de dados no tempo e no espaço",
    "section": "",
    "text": "1 Séries de dados no tempo e no espaço"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/teorema_bayes.html",
    "href": "conteudo/fundamentos_probabilidade/teorema_bayes.html",
    "title": "Teorema de Bayes",
    "section": "",
    "text": "Pacotes e funções utilizadas\n\n\n\n\n\n\nsource(\"scripts/conditional_tree.r\")\nO Teorema de Bayes provém da definição de probabilidade condicional:\n\\[P(B|A) = \\frac{P(A \\cap B)}{P(A)},\\]\no que implica:\n\\[P(A \\cap B) = P(A)\\,P(B|A).\\]\nPodemos também escrever:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(A \\cap B) = P(B)\\,P(A|B).\\]\nComo \\(P(A \\cap B) = P(B \\cap A)\\), segue:\n\\[P(A)\\,P(B|A) = P(B)\\,P(A|B),\\]\nresultando na forma geral do Teorema de Bayes:\n\\[P(B|A) = \\frac{P(B)\\,P(A|B)}{P(A)}.\\]"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/teorema_bayes.html#teorema-da-probabilidade-total",
    "href": "conteudo/fundamentos_probabilidade/teorema_bayes.html#teorema-da-probabilidade-total",
    "title": "Teorema de Bayes",
    "section": "1 Teorema da probabilidade total",
    "text": "1 Teorema da probabilidade total\nConsidere o esquema de um diagrama de árvore:\n\n\n\n\n\n\n\n\n\nDois caminhos podem levar à ocorrência de \\(A\\): um em que \\(B\\) ocorre \\(\\bigl[P(A \\cap B)\\bigr]\\) e outro em que \\(B\\) não ocorre \\(\\bigl[P(A \\cap \\overline{B})\\bigr]\\). Por serem mutuamente exclusivos:\n\\[P(A) = P(A \\cap B) + P(A \\cap \\overline{B}),\\]\nque pode ser reescrito como:\n\\[P(A) = P(B)\\,P(A|B) + P(\\overline{B})\\,P(A|\\overline{B}).\\]\nEste resultado é conhecido como Teorema da probabilidade total. Assim, o Teorema de Bayes pode ser expresso por:\n\\[\nP(B|A) = \\frac{P(B)\\,P(A|B)}{P(B)\\,P(A|B) + P(\\overline{B})\\,P(A|\\overline{B})}.\n\\]"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/teorema_bayes.html#o-problema-da-detecção-de-espécies",
    "href": "conteudo/fundamentos_probabilidade/teorema_bayes.html#o-problema-da-detecção-de-espécies",
    "title": "Teorema de Bayes",
    "section": "2 O problema da detecção de espécies",
    "text": "2 O problema da detecção de espécies\nSuponha um estudo sobre a presença de uma espécie de peixe em riachos costeiros da Mata Atlântica. Ela ocorre em 5% dos riachos \\(\\bigl[P(O) = 0,05\\bigr]\\), sendo, portanto, rara. A detecção se dá por captura e identificação taxonômica. Se a espécie está presente, a probabilidade de captura é \\(0,99\\), havendo \\(0,01\\) de falso negativo (quando a espécie não é detectada mesmo presente).\nHá também a possibilidade de falso positivo: mesmo quando ausente, existe uma chance de 0,10 de a espécie ser registrada erroneamente, devido à semelhança com outra espécie presente na região.\nPodemos organizar essas probabilidades em um diagrama de árvore:\n\n\n\nDiagrama de árvore representando as probabilidades de ocorrência P(O) e detecção P(D) de uma espécie.\n\n\nAs ramificações mostram as bifurcações do evento “espécie presente ou ausente” e, em seguida, “detecção ou não-detecção”. Assim, são possíveis:\n\n\\(P(O \\cap D) = 0,0495\\)\n\\(P(O \\cap \\overline{D}) = 0,0005\\)\n\\(P(\\overline{O} \\cap D) = 0,095\\)\n\\(P(\\overline{O} \\cap \\overline{D}) = 0,855\\)\n\nA probabilidade total de detecção \\(P(D)\\) é:\n\\[P(D) = P(O \\cap D) + P(\\overline{O} \\cap D) = 0,0495 + 0,095 = 0,1445.\\]\n\n2.1 Razão de verossimilhança, inferência bayesiana e teste de hipóteses\nUma pergunta relevante é:\n\nAo sabermos de uma possível detecção, podemos ter certeza da presença dessa espécie?\n\nEm inferência estatística, buscamos tomar decisões a respeito de um fenômeno desconhecido com base em dados observados. Aqui, exploramos duas abordagens: verossimilhança e inferência bayesiana.\n\n2.1.1 Verossimilhança: uma medida indireta para \\(P(O|D)\\)\nSe \\(P(D|O)\\) for alta, a presença da espécie se torna mais plausível, pois a chance de detectá-la quando está presente é elevada. Já \\(P(D|\\overline{O})\\) alto indicaria que a não-ocorrência é mais provável, pois há muitos falsos positivos.\nEm estatística, \\(P(D|O)\\) é análogo à verossimilhança de \\(O\\) dado \\(D\\). De modo simplificado, contrastamos duas hipóteses:\n\nEspécie ocorre e é detectada;\nEspécie não ocorre, mas há detecção falsa.\n\nA razão de verossimilhança (\\(RV\\)) é:\n\\[RV = \\frac{P(D|O)}{P(D|\\overline{O})} = \\frac{0,99}{0,10} = 9,9.\\]\nInterpretamos como sendo cerca de 10 vezes mais provável a hipótese “espécie ocorre” do que “espécie não ocorre” quando há detecção.\n\n\n2.1.2 Inferência bayesiana: o conhecimento a priori importa?\nNa abordagem bayesiana, incluímos a probabilidade a priori de ocorrência, \\(P(O)\\). Quando recebemos a informação de detecção, atualizamos essa probabilidade, tornando-a \\(P(O|D)\\).\nNo exemplo, conhecemos:\n\n\\(P(O) = 0,05\\) e \\(P(\\overline{O}) = 0,95\\);\n\\(P(D|O) = 0,99\\) e \\(P(D|\\overline{O}) = 0,10\\).\n\nPelo Teorema de Bayes:\n\\[P(O|D) = \\frac{P(O)\\,P(D|O)}{P(O)\\,P(D|O) + P(\\overline{O})\\,P(D|\\overline{O})}.\\]\nSubstituindo valores:\n\\[P(O|D) = \\frac{0,02 \\times 0,99}{0,02 \\times 0,99 + 0,98 \\times 0,10} \\approx 0.17.\\]\nE:\n\\[P(\\overline{O}|D) = \\frac{0,98 \\times 0,10}{0,02 \\times 0,99 + 0,98 \\times 0,10} \\approx 0.83.\\]\nMesmo com a detecção, a chance de não-ocorrência ainda é maior, favorecendo a hipótese de falso positivo.\n\n\n2.1.3 Diferenças entre as abordagens\nA verossimilhança foca em \\(P(D|O)\\), enquanto a inferência bayesiana calcula \\(P(O|D)\\) diretamente, ponderada por \\(P(O)\\). Se \\(P(O) = 0,5\\), então:\n\\[P(O|D) = \\frac{0,5 \\times 0,99}{0,5 \\times 0,99 + 0,5 \\times 0,10} \\approx 0,91,\\] \\[P(\\overline{O}|D) \\approx 0,09,\\]\ne a razão \\(\\frac{P(O|D)}{P(\\overline{O}|D)}\\) será igual a \\(RV = 9,9\\), tal como na verossimilhança. Porém, quando \\(P(O)\\) indica uma espécie muito rara, esse valor a priori altera o resultado final de \\(P(O|D)\\). Por isso, as duas abordagens só coincidem quando usamos uma priori não-informativa (probabilidades iniciais iguais para presença e ausência)."
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/combina_probabilidades.html",
    "href": "conteudo/fundamentos_probabilidade/combina_probabilidades.html",
    "title": "Combinando as probabilidades de eventos",
    "section": "",
    "text": "Pacotes e funções utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggVennDiagram)"
  },
  {
    "objectID": "conteudo/fundamentos_probabilidade/combina_probabilidades.html#eventos-complexos",
    "href": "conteudo/fundamentos_probabilidade/combina_probabilidades.html#eventos-complexos",
    "title": "Combinando as probabilidades de eventos",
    "section": "1 Eventos complexos",
    "text": "1 Eventos complexos\nConsideremos a seguinte situação: há dois tipos principais de estruturas nas quais um animal aquático pode procurar suas presas: folhas e galhos. As folhas podem conter entre 0 e 6 itens, enquanto os galhos podem conter entre 0 e 4 itens. Ao virar qualquer uma dessas estruturas, o predador consome todos os itens encontrados. Embora seja um exemplo altamente hipotético, ele serve para ilustrar nossa discussão. A pergunta que nos interessa é: ao virar uma dessas estruturas, de quantos itens o predador poderá se alimentar?\nVejamos as possibilidades. Denotemos por \\(F\\) ou \\(G\\) o evento de encontrar, respectivamente, uma folha ou um galho, e por \\(0\\) a \\(n\\) o número de itens presentes. O espaço amostral do experimento — virar uma estrutura e contar quantos itens há — é dado por:\n\\[\\Omega = \\{\\text{(F0), (F1), (F2), (F3), (F4), (F5), (F6), (G0), (G1), (G2), (G3), (G4)}\\}.\\]\nNote que há \\(\\text{12}\\) eventos simples e mutuamente exclusivos.\nConsidere agora o evento \\(A\\): “virar uma folha”, que ocorre quando observamos F0 ou F1 ou F2 ou F3 ou F4 ou F5 ou F6. Em notação de conjunto:\n\\[A = \\{\\text{(F0), (F1), (F2), (F3), (F4), (F5), (F6)}\\}.\\]\nO evento \\(A\\) é um evento complexo, pois consiste na união de vários eventos simples. Ou seja, podemos dizer que \\(A\\) acontece se a estrutura virada for uma folha com \\(0\\) OU \\(1\\) OU \\(2\\) OU \\(3\\) OU \\(4\\) OU \\(5\\) OU \\(6\\) itens. A palavra OU indica que basta qualquer uma dessas possibilidades para o evento ser considerado realizado, o que significa que \\(A\\) pode ocorrer de 7 maneiras diferentes.\nAssim, podemos representar \\(A\\) pela expressão:\n\\[A = F0 \\cup F1 \\cup F2 \\cup F3 \\cup F4 \\cup F5 \\cup F6,\\]\nonde, o símbolo \\(\\cap\\) é sendo lido como OU.\n\n1.1 Representação de eventos: diagrama de Venn\nUma forma de visualizar o espaço amostral e seus eventos é por meio de diagramas de Venn. Para isso, vamos considerar também o evento \\(B\\): “encontrar mais de 3 itens”, que consiste em:\n\\[B = \\{\\text{(F3), (F4), (F5), (F6), (G3), (G4)}\\}.\\]\nObservando \\(A\\) e \\(B\\) em um Diagrama de Venn, temos:\n\n\n\n\n\n\n\n\n\nO evento \\(B\\) também é um evento complexo que ocorre quando se encontra uma folha OU um galho que tenham 3 ou mais itens.\nObserve que as ocorrências \\((G0)\\), \\((G1)\\) e \\((G2)\\) não pertencem a \\(A\\) nem a \\(B\\), embora façam parte do espaço amostral \\(\\Omega\\).\nConsidere agora o evento \\(C\\): “virar uma folha com mais de 3 itens”. Ele pode ocorrer ao observar F3 ou F4 ou F5 ou F6. No diagrama de Venn, vemos que essas possibilidades constituem a intersecção de \\(A\\) e \\(B\\) representada por \\(\\cap\\). Portanto, podemos escrever \\(C\\) como:\n\\[C = A \\cap B.\\]\n\n\n1.2 Probabilidade de eventos complexos\nConsiderando o experimento virar uma estrutura e contar o número de itens, qual seria a probabilidade de cada observação? Para isso, lembremos que:\n\nO espaço amostral consiste em \\(N = \\text{12}\\) observações.\nDefiniremos um modelo de probabilidade para cada uma dessas observações.\n\nNeste tópico, vamos assumir um modelo de probabilidade uniforme, ou seja, cada observação tem a mesma probabilidade \\(\\frac{1}{N}\\).\nDessa forma, a probabilidade do evento \\(A\\) ocorrer é dada pelo número de resultados favoráveis a \\(A\\) dividido pelo número total de resultados no espaço amostral. Como \\(A\\) consiste de 7 observações:\n\\[P(A) = \\frac{7}{12} = 0.58\\]\nNaturalmente, a probabilidade de \\(A\\) não ocorrer é:\n\\[P(\\overline{A}) = 1 - \\frac{7}{12} = 1 - 0.58 = 0.42\\]\nO símbolo \\(\\overline{A}\\) representa todas as observações que não pertencem a \\(A\\).\nConsidere também a probabilidade de \\(B\\) que consiste de 6 observações:\n\\[P(B) = \\frac{6}{12} = 0.5\\]\nVemos também que o evento \\(C = A \\cap B\\), consiste de 4 observações e portanto:\n\\[P(C) = P(A \\cap B) = \\frac{4}{12} = 0.33\\]\n\n\n1.3 Probabilidade da união de eventos\nO evento \\(A \\cup B\\) consiste em todas as observações que estejam em \\(A\\) ou \\(B\\):\n\\(A \\cup B = \\{(F0),(F1),(F2),(F3),(F4),(F5),(F6),(G3),(G4) \\}\\)\nHá 9 ocorrências em \\(A \\cup B\\), de modo que:\n\\[P(A \\cup B) = \\frac{9}{12} = 0.75\\]\nDo diagrama de Venn, fica fácil verificar que o número de eventos em \\(A \\cup B\\) pode ser obtido pelo número de eventos em \\(A\\) somados ao número de eventos em \\(B\\) e subtraído pelo número de evendos que ocorrem em ambos (\\(A \\cap B\\)).\nAssim:\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\\[P(A \\cup B) = 0.58 + 0.5 - 0.33 = 0.75\\]"
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_t.html",
    "href": "conteudo/teste_hipoteses/teste_t.html",
    "title": "Comparando médias: teste t de Student",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas no capítulo\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nO modelo normal de probabilidades não é a melhor aproximação para a distribuição das médias amostrais quando não conhecemos \\(\\mu\\) e \\(\\sigma\\) e/ou quando o tamanho amostral \\(n\\) é pequeno. Nesta situação, a distribuição t de Student é mais apropriada para o cálculo do intervalo de confiança. Da mesma forma, o teste \\(Z\\) assume que a distribuição das médias amostrais é normalmente distribuída e que a variância populacional \\(\\sigma\\) seja conhecida, uma informação que não temos na prática científica.\nO teste t de Student é utilizado em substituição ao teste \\(Z\\) quando \\(\\sigma\\) é desconhecido e/ou o tamanho amostral é pequeno. A lógica do teste é a mesma apresentada discutida no teste \\(Z\\), porém estabelece que a distribuição das médias amostrais é melhor descrita pela distribuição \\(t\\) e não pela distribuição normal."
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_t.html#teste-t-para-uma-média-populacional",
    "href": "conteudo/teste_hipoteses/teste_t.html#teste-t-para-uma-média-populacional",
    "title": "Comparando médias: teste t de Student",
    "section": "1 Teste t para uma média populacional",
    "text": "1 Teste t para uma média populacional\nConsidere um exemplo simples. Dados do Banco Central do Brasil dizem que moedas de \\(R\\$ 0,10\\) da segunda geração pesam 4.8 gramas. Você tem \\(8\\) moedas no bolso e resolve testar essa afirmação pesando cada moeda. Os pesos obtidos são: \\(X = 5.1, 5, 4.8, 5, 5, 4.9, 4.9, 4.7\\).\nInicialmente, devemos estabelecer nossa hipótese nula (\\(H_0\\)), nossa hipótese alternativa (\\(H_a\\)) e o nível se significância \\(\\alpha\\). Iremos estabelecer \\(\\alpha = 0,05\\) e as hipóteses como:\n\\(H_0: \\mu = 4.8\\) gramas\n\\(H_a: \\mu \\ne 4.8\\) gramas\nComo não conhecemos \\(\\sigma\\) e temos uma amostra pequena, a posição das médias amostrais seguirá uma distribuição \\(t\\) de Student e a estatística do teste será:\n\\[t = \\frac{\\overline{X} - \\mu}{s_{\\overline{X}}}\\]\nsendo o erro padrão amostral obtido por:\n\\[s_{\\overline{X}} = \\frac{s}{\\sqrt{n}}\\]\nO cálculo de \\(t\\) é muito similar ao escore \\(Z\\). No entanto, substituímos \\(\\sigma\\) por \\(s\\). As distribuições de \\(t\\) e de \\(Z\\) são muito similares. Entretanto, para amostras pequenas e quando \\(\\sigma\\) é desconhecido, a curva de \\(t\\) nos fornece uma melhor estimativa das probabilidades associadas a distribuição das médias amostrais.\nPara este exemplo, temos uma amostra de tamanho \\(n = 8\\) com média \\(\\overline{X} = 4.925\\)g e desvio padrão \\(s = 0.13\\)g. O valor de \\(t\\) pode ser calculado por:\n\\[t_{c} = \\frac{\\overline{X} - \\mu}{s_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{4.925  - 4.8}{\\frac{0.13}{\\sqrt{8}}} = 2.76\\]\nAssim como fizemos para a distribuição \\(Z\\), devemos encontrar a probabilidade de obtermos um valor tão ou maior que o módulo de \\(t_c\\). Na figura abaixo, nosso resultado fica:\n\n\n\n\n\n\n\n\nFigura 2: Valor de p associado ao resultado do teste t.\n\n\n\n\n\nA probabilidade de encontrarmos um valor de \\(t_c\\) tão ou mais extremo segundo a hipótese nula foi de \\(p = 0.028\\). Uma vez que este valor é menor que o nível crítico \\(\\alpha = 0,05\\), concluímos que existe evidência suficiente para rejeitar \\(H_0\\) e aceitar a hipótese alternativa de que as moedas de \\(10\\) centavos não provém de uma população estatística com \\(\\mu = 4,8\\) gramas. Nossa conclusão é portanto, que as moedas de \\(R\\$ 0,10\\) são mais pesadas que \\(4,8\\) gramas.\n\n\n\n\n\n\nTerste t no R\n\n\n\n\nt.test(X, mu = 4.8)\n\n\n    One Sample t-test\n\ndata:  X\nt = 2.7584, df = 7, p-value = 0.02816\nalternative hypothesis: true mean is not equal to 4.8\n95 percent confidence interval:\n 4.817844 5.032156\nsample estimates:\nmean of x \n    4.925 \n\n\n\nNos comandos acima, \\(X\\) é a amostra e o argumento mu representa a expectativa sobre a média populacional segundo \\(H_0\\). Como resultados temos:\n\na indicação de que fizemos um teste \\(t\\) para uma amostra: One Sample t-test;\no valor de \\(t\\) calculado: t = 2.7584;\nos graus de liberdade: df = 8 - 1 = 7; e\no valor de p = 0.028.\n\nA saída da função apresenta ainda o valor da média amostral (\\(\\overline{X} = 4.925\\)) e o intervalo de confiança a \\(95\\%\\) (4.817844 - 5.032156)."
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_t.html#graus-de-liberdade",
    "href": "conteudo/teste_hipoteses/teste_t.html#graus-de-liberdade",
    "title": "Comparando médias: teste t de Student",
    "section": "2 Graus de liberdade",
    "text": "2 Graus de liberdade\nA distribuição \\(t\\) como várias outras distribuições amostrais utilizadas em inferência estatística, muda seu formato em função do que chamamos de graus de liberdade (\\(gl\\)). Os graus de liberdade têm relação com o tamanho amostral. No caso do teste \\(t\\) para \\(1\\) amostra, esta relação é simplesmente: \\(gl = n-1\\).\nÀ medida que os graus de liberdade aumentam, o formato da distribuição \\(t\\) se assemelha ao formato da distribuição Normal padronizada. De fato, para graus de liberdades altos (ex. \\(n \\ge 30\\)), os formatos das distribuições \\(Z\\) e \\(t\\) são praticamente indistinguíveis. Na prática, isto faz que as distribuição \\(Z\\) raramente seja utilizada.\n\n\n\n\n\n\n\n\nFigura 3: Função de densidade de t para diferentes graus de liberdade."
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_t.html#probabilidades-no-teste-t-de-student-a-tabela-t",
    "href": "conteudo/teste_hipoteses/teste_t.html#probabilidades-no-teste-t-de-student-a-tabela-t",
    "title": "Comparando médias: teste t de Student",
    "section": "3 Probabilidades no teste \\(t\\) de Student: a tabela \\(t\\)",
    "text": "3 Probabilidades no teste \\(t\\) de Student: a tabela \\(t\\)\nA rejeição uo aceitação da hipótese nula em um teste \\(t\\) pode ser feita por meio da obtenção do valor de p ou pela comparação do \\(t\\) calculado com valores críticos de referência para determinado nível de significância. O primeiro caso foi o que apresentamos acima e depende de um software estatístico para obtermos valores exatos de \\(p\\). O segundo caso, pode ser feito com auxílio da Tabela \\(t\\), em que limites críticos de \\(t\\) são disponibilizados para diferentes níveis de significância e graus de liberdade.\nAtualmente, o uso da tabela \\(t\\) têm finalidade em grande parte didática e, por este motivo, vamos apresentá-lo aqui rapidamente. No entanto, fora da sala de aula, o teste \\(t\\) será invariavelmente conduzido por meio de um software estatístico este método que permitirá a obtenção do valor exato de \\(p\\).\nNa Tabela t, a primeira coluna mostra os graus de liberdade de \\(1\\) a \\(120\\). O cabeçalho da tabela de \\(90\\%\\) a \\(0,1\\%\\) mostra a área na distribuição de \\(t\\) nas caldas inferior e superior.\nVamos retornar ao exemplo da moedas de \\(R\\$0,10\\) para exemplificar sua utilização. Neste exemplo a tinhamos \\(8\\) (\\(gl = 7\\) graus de liberdade) e o teste foi feito com \\(\\alpha =0,05\\). Se buscarmos na linha \\(gl = 7\\) e a coluna \\(5\\%\\) (\\(\\alpha = 0,05\\)), encontraremos o valor \\(t = 2,3646\\). Este é o chamado \\(t\\) crítico (\\(t_{crítico}\\)). Acima deste valor e abaixo de sua contraparte negativa temos exatamente \\(5\\%\\) da área na disribuição \\(t\\). Deste modo, qualquer valor calulado maior que \\(t_{crítico}\\) estará mais para a extremidade da distribuição e consequentemente estará associado a menores valores de probabilidade. Neste sentido:\n\n\\(t_{calculado} \\ge t_{crítico}\\) leva a rejeição de \\(H_0\\)\n\n\n\\(t_{calculado} &lt; t_{crítico}\\) leva a aceitação de \\(H_0\\)\n\nO resultado do teste estatístico em nosso exemplo foi \\(t_c = 2.76\\) que é maior que \\(2,3646\\). Isto nos leva à mesma decisão anterior (rejeitar \\(H_0\\)), ainda que por meio da tabela \\(t\\) não tenhamos o valor exato de probabilidade."
  },
  {
    "objectID": "conteudo/teste_hipoteses/teste_t.html#teste-t-para-comparação-de-duas-médias-independentes",
    "href": "conteudo/teste_hipoteses/teste_t.html#teste-t-para-comparação-de-duas-médias-independentes",
    "title": "Comparando médias: teste t de Student",
    "section": "4 Teste t para comparação de duas médias independentes",
    "text": "4 Teste t para comparação de duas médias independentes\nO que vimos no teste \\(t\\) para uma amostra pode ser facilmente extendido para testarmos a diferenças entre duas amostras.\nOs dados abaixo mostram o tempo de coagulação sanguínea (em minutos) em ratos machos adultos tratados com dois tipos de drogas, retirado do livro Biostatistical Analysis (Zar 2010), pp. 130-134.\n\n\nCódigo\nra &lt;- data.frame(Droga = factor(c(rep(\"Droga A\", 6), rep(\"Droga B\",7))),\n                Tempo = c(8.8, 8.4, 7.9, 8.7, 9.1, 9.6, \n                          9.9, 9.0, 11.1, 9.6, 8.7, 10.4, 9.5))\n\n\n\n\n\n\nTabela 1: Tempo de coagulação sanguínea (em minutos) para dois tipos de drogas.\n\n\n\n\n\n\n\n\n\nDroga\nTempo\n\n\n\n\nDroga A\n8.80\n\n\nDroga A\n8.40\n\n\nDroga A\n7.90\n\n\nDroga A\n8.70\n\n\nDroga A\n9.10\n\n\nDroga A\n9.60\n\n\nDroga B\n9.90\n\n\nDroga B\n9.00\n\n\nDroga B\n11.10\n\n\nDroga B\n9.60\n\n\nDroga B\n8.70\n\n\nDroga B\n10.40\n\n\nDroga B\n9.50\n\n\n\n\n\n\n\n\n\n\nNosso objetivo é testar se as duas drogas resultam, em média, no mesmo tempo de coagulação. Inicialmente, vamos fazer um gráfico de dispersão para verificar a distribuição do tempo de coagulação para cada droga.\n\n\nCódigo\nggplot(ra, aes(y = Tempo, x = Droga)) +\n  geom_boxplot() +\n  geom_point(col = 2, size = 3) +\n  labs(y = 'Tempo de coalgulação (min)', x = '') +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\nFigura 4: Distribuição do tempo de coagulação sanguínea (em minutos) para dois tipos de drogas.\n\n\n\n\n\nAs médias, desvios padrões e tamanhos amostrais de cada grupo são:\n\n\nCódigo\nra_m = ra |&gt; \n    group_by(Droga) |&gt; \n    summarize('Tempo médio' = round(mean(Tempo),2), \n              Desvio = round(sd(Tempo),2), n = n() )\n\nra_m |&gt; \n  gt()\n\n\n\n\nTabela 2: Tempo de coagulação sanguínea (em minutos) para dois tipos de drogas.\n\n\n\n\n\n\n\n\n\nDroga\nTempo médio\nDesvio\nn\n\n\n\n\nDroga A\n8.75\n0.58\n6\n\n\nDroga B\n9.74\n0.82\n7\n\n\n\n\n\n\n\n\n\n\nPara testarmos se as médias dos grupos provém de populações estatísticas com diferentes \\(\\mu's\\) devemos estabelecer nosso nível de significância (por exemplo \\(\\alpha = 0.05\\)) as hipoteses estatísticas:\n\\(H_0: \\mu_A = \\mu_B\\) gramas\n\\(H_a: \\mu_A \\ne \\mu_B\\) gramas\n\n\n\n\n\n\nTeste de homogeneidade de variâncias\n\n\n\nUm dos pressupostos do teste t que apresentaremos a frente é de que as populações que serão comparadas têm a mesma variância \\(\\sigma^2\\). Devemos portanto testar o pressuposto de homogeneidade de variâncias que pode ser realizado com o teste de razão de variâncias.\nNo R fazemos o teste de homogeneidade de variâncias com o comando abaixo.\n\nvar.test(ra$Tempo ~ ra$Droga)\n\n\n    F test to compare two variances\n\ndata:  ra$Tempo by ra$Droga\nF = 0.50633, num df = 5, denom df = 6, p-value = 0.4722\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.08456359 3.53301988\nsample estimates:\nratio of variances \n           0.50633 \n\n\nNo teste acima, \\(F_{calculado} = 0.51\\) e \\(p = 0.472\\). Assumindo um \\(\\alpha = 0,05\\), concluímos que não há evidências para rejeitar a hipótese de homogeneidade e que assumimos que as duas populações têm a mesma variância. Deste modo podemos continuar com o teste t.\n\n\nO teste t para duas amostras é calculado por:\n\\[t = \\frac{(\\overline{X_A} - \\mu_A) - (\\overline{X_B} - \\mu_B)}{s_{\\overline{X_A}-\\overline{X_B}}}\\]\nAssumindo a hipotese nula em que \\(\\mu_A = \\mu_B\\) a expressão fica\n\\[t = \\frac{\\overline{X_A} - \\overline{X_B}}{s_{\\overline{X_A}-\\overline{X_B}}}\\]\nem que a quantia \\(s_{\\overline{X_A}-\\overline{X_B}}\\) é calculada por:\n\\[s_{\\overline{X_A}-\\overline{X_B}} = \\sqrt{\\frac{s^2_{p}}{n_1} + \\frac{s^2_{p}}{n_2}}\\]\n\\(s_p\\) é denominada de variância conjunta calculada por\n\\[s^2_p = \\frac{(n_1 - 1) \\times s^2_1 + (n_2 - 1) \\times s^2_2}{(n_1 - 1) + (n_2 - 1)}\\]\nPara este exemplo,\n\\(s_p = 0.52\\)\ne\n\\(s_{\\overline{X_A}-\\overline{X_B}} =  0.4\\)\nO valor de t calculado é:\n\\(t_c =  -2.476\\)\nNa distribuição t, a probabilidade de encontrar valores tão ou mais extremos que 2.476 é de \\(p = 0.031\\).\nPortanto:\n\\(P(|t| \\ge 2.476) \\le 0.05\\)\nUma vez que a probabilidade associada ao valor de \\(t\\) é menor que o nível de significância, rejeitamos \\(H_0\\) e assumimos que os tempos médios de coagulação são diferentes. Ao avaliar as média amostrais \\(\\overline{X}_A\\) e \\(\\overline{X}_B\\), concluímos que a droga \\(A\\) resulta, em média, em tempos menores de coagulação.\n\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/anova/anova_simples.html",
    "href": "conteudo/anova/anova_simples.html",
    "title": "Análise de variância de um fator",
    "section": "",
    "text": "Pacotes, funções e base de dados utilizadas\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(gt)\nsource('scripts/anova_sim.r')\nA Análise de Variância (ANOVA) desenvolvida por R. A. Fisher aplica-se à uma classe de desenho experimental em que a variável resposta \\(Y\\) é contínua e a variável explanatória \\(X\\) é categórica com \\(2\\) ou mais níveis. A ANOVA nos permite testar a hipótese de que duas ou mais médias amostrais (\\(\\overline{Y}_i\\)) tenham sido obtidas de uma mesma população estatística com média \\(\\mu\\). Alternativamente, podemos concluir que as médias amostrais diferem umas das outras, de tal forma que devemos assumir que foram amostradas a partir de diferentes populações estatísticas, nas quais ao menos um \\(\\mu_i\\) seja diferente dos demais. Iremos denominar estas duas possibilidades de hipótese estatísticas sobre a relação entre as médias populacionais."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#o-modelo-da-anova-e-as-hipóteses-estatísticas",
    "href": "conteudo/anova/anova_simples.html#o-modelo-da-anova-e-as-hipóteses-estatísticas",
    "title": "Análise de variância de um fator",
    "section": "1 O modelo da ANOVA e as hipóteses estatísticas",
    "text": "1 O modelo da ANOVA e as hipóteses estatísticas\nO modelo pode ser representado por:\n\\[Y_{ij} = \\mu + A_i + \\epsilon_{ij}\\]\nonde \\(Y_{ij}\\) é a variável resposta associada à observação \\(i\\) do tratamento \\(j\\), \\(\\mu\\) representa a média geral e \\(A_i\\) o efeito do tratamento \\(i\\). O termo \\(\\epsilon_{ij}\\) é denominado de resíduo (ou erro) associado a cada observação, que assumimos ter distribuição normal com média zero e variância constante.\n\\[\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\n\n\nHipóteses estatísticas no modelo de ANOVA\n\n\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 =.... = \\mu_k\\) (HIPÓTESE NULA)\n\\(H_a\\): ao menos um par de médias é diferente (HIPÓTESE ALTERNATIVA)\n\n\nA hipótese nula (\\(H_0\\)) define a ausência de diferenças entre as médias populacionais enquanto a hipótese alternativa (\\(H_a\\)) refere-se a qualquer possibilidade diferente de \\(H_0\\). Se temos exatamente dois níveis em \\(X\\), a comparação de médias pode ser feita por meio de um teste \\(t\\). Por outro lado, quando temos mais de dois níveis em \\(X\\), devemos utilizar o modelo de ANOVA."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#partição-das-soma-dos-quadrados",
    "href": "conteudo/anova/anova_simples.html#partição-das-soma-dos-quadrados",
    "title": "Análise de variância de um fator",
    "section": "2 Partição das Soma dos Quadrados",
    "text": "2 Partição das Soma dos Quadrados\nAo representarmos a distribuição de uma variável \\(Y\\) contínua em função de uma variável \\(X\\) categórica, geralmente estamos interessados em determinar se os diferentes níveis de \\(X\\) (diferentes grupos) têm médias similares ou se ao menos um dos níveis têm média diferente dos demais. Queremos uma medida que nos permita diferenciar situações como as apresentadas abaixo.\n\n\n\n\n\n\n\n\n\nNa figura \\(A\\) todos os grupos são provenientes da mesma distribuição e têm médias aproximadamente iguais (\\(\\overline{Y}_A \\approx  \\overline{Y}_B \\approx \\overline{Y}_C \\approx \\overline{Y}_D\\)) e as diferenças são provinientes unicamente da variabilidade amostral. Na figura \\(B\\) o segundo grupo tem média mais elevada que os demais, enquanto na figura \\(C\\), todas as médias parecem ser diferentes entre si (\\(\\overline{Y}_A \\ne  \\overline{Y}_B \\ne \\overline{Y}_C \\ne \\overline{Y}_D\\)).\nPara mensurar o grau de associação entre \\(Y\\) e \\(X\\) de modo a diferenciar as situações acima, vamos introduzir o processo de Partição da Soma dos Quadrados.\nSuponha a situção abaixo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotações\n\n\n\n\nTemos \\(k = 3\\) grupos (A, B ou C) e para cada grupo \\(n =  5\\) observações. Denotamos por \\(n_{ij}\\) o número de observações dentro de cada grupo, em que \\(i\\) é a i-ésima observação (\\(i = 1\\) a \\(5\\)) do j-ésimo grupo (\\(j = 1\\) a \\(3\\) - grupos A ao C). Neste exemplo, o número de observações em cada grupo é o mesmo (\\(n_1 = n_2 = n_3 = n\\)), de modo que o total de observações é dado por:\n\n\\(N = k \\times n = n_1 + n_2 + n_3 = 15\\)\n\nA média de cada grupo será denotada por \\(\\overline{Y}_j\\), que neste exemplo são: \\(Y_1 = 20.64\\) (grupo A), \\(Y_2 = 28.68\\) (grupo B) e \\(Y_3 = 12.18\\) (grupo C).\nVamos denotar por \\(\\overline{\\overline{Y}}\\) a Grande Média, isto é, a média geral de todas as observações independente do grupo de origem.\n\n\\[\\overline{\\overline{Y}} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}\\frac{Y_{ij}}{N} = \\frac{\\overline{Y_1} + \\overline{Y_2} + \\overline{Y_3}}{3} = 20.5\\]\n\n\nPodemos agora observar estes elementos no gráfico de dispersão.\n\n\n\n\n\n\n\n\n\nEm seguida, precisamos calcular \\(3\\) quantias, a Soma dos Quadrados Totais (\\(SQ_{Total}\\)), a Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\) e a Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\).\n\nSoma dos Quadrados Totais \\(SQ_{Total}\\): mede as diferenças entre \\(Y_{ij}\\) e \\(\\overline{\\overline{Y}}\\). Temos nesta expressão o somatório dos desvios ao quadrado de todas as observações com relação à grande média independente do grupo de origem de cada observação.\n\n\\[SQ_{Total} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\): mede as diferenças entre as médias dos tratamentos \\(\\overline{Y}_j\\) e \\(\\overline{\\overline{Y}}\\), sendo portanto os desvios ao quadrado da média de cada tratamento subtraída da grande média. \\(SQ_{Trat}\\) também é chamada de soma dos quadrados entre grupos ou entre tratamentos\n\n\\[SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Resíduos \\(SQ_{Res}\\): mede as diferenças entre cada observação \\(Y_{ij}\\) e a média de seu próprio grupo \\(\\overline{Y}_{j}\\). \\(SQ_{Res}\\) também é chamada de soma dos quadrados dentro dos grupos ou dentro dos tratamentos\n\n\\[SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2\\]\n\n\n\n\n\n\nA característica aditiva das somas dos quadrados\n\n\n\nA partição da soma dos quadrados consiste em decompor a variação total do experimento em uma parcela atribuída à variação entre tratamentos e outra parcela da variação dentro dos tratamentos. Isto é possível pois as somas dos quadrados definidas acima podem ser expressas de forma aditiva como:\n\\[SQ_{Total} = SQ_{Trat} + SQ_{Res}\\]\nDeste modo, é possível demostrar que:\n\\(\\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(Y_{j} - \\overline{\\overline{Y}})^2 + \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{Y}_{j})^2\\)"
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#medindo-a-associação-entre-y-e-x",
    "href": "conteudo/anova/anova_simples.html#medindo-a-associação-entre-y-e-x",
    "title": "Análise de variância de um fator",
    "section": "3 Medindo a associação entre \\(Y\\) e \\(X\\)",
    "text": "3 Medindo a associação entre \\(Y\\) e \\(X\\)\nA característica aditiva das somas dos quadrados pode ser utilizada para mensurar o grau de dependência de \\(Y_{ij}\\) com respeito aos diferentes tratamentos. Compare as duas figuras abaixo:\n\n\n\n\n\n\n\n\n\nA soma dos quadrados dentro dos grupos é a mesma nas duas figuras (\\(SQ_{Res} = 362.6\\)). No entanto, na figura da esquerda, em que as médias dos tratamentos são similares (e consequentemente próximas à grande média), a soma dos quadrados entre os tratamentos é muito menor (\\(SQ_{Trat}^{esquerda} = 15.8\\)) que na figura da direita, em que as médias dos tratamentos estão distantes entre si (\\(SQ_{Trat}^{direita} = 680.8\\)). É desta forma que a partição das somas dos quadrados nos permite diferenciar situações em que: i - a média dos grupos depende dos níveis do tratamento (figura da direita); de situações em que ii - a média não depende dos níveis do tratamento (figura da esquerda)."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#quadrados-médios-e-graus-de-liberdade",
    "href": "conteudo/anova/anova_simples.html#quadrados-médios-e-graus-de-liberdade",
    "title": "Análise de variância de um fator",
    "section": "4 Quadrados médios e graus de liberdade",
    "text": "4 Quadrados médios e graus de liberdade\nPara que os somatórios dos quadrados expressem uma medida de variação é necessário corrigi-los em função dos graus de liberdade (\\(gl\\)), obtendo assim os Quadrados médios conforme abaixo:\n\nQuadrado Médio Total (\\(QM_{Total}\\))\n\n\\[QM_{Total} = \\frac{SQ_{Total}}{gl_{Total}}\\]\nem que \\(gl_{Total} = N - 1\\)\n\nQuadrado Médio dos Tratamentos (\\(QM_{Trat}\\))\n\n\\[QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}}\\]\nem que \\(gl_{Trat} = k - 1\\)\n\nQuadrado Médio dos Resíduos (\\(QM_{Res}\\))\n\n\\[QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}}\\]\nem que \\(gl_{Res} = N-k\\)\nAssim como a soma dos quadrados, os graus de liberdade também têm característica aditiva.\n\\[gl_{Total} = gl_{Trat} + gl_{Res} = (k - 1) + (N - K) = N - 1\\]\nOs quadrados médios que são estimativas de variâncias. Compare por exemplo a expressão do \\(QM_{Total}\\) com a fórmula da variância amostral (\\(s^2\\)) e verá que excetuando mudanças de notação, as expressões são essencialmente as mesmas."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#estatística-f-e-teste-de-hipóteses",
    "href": "conteudo/anova/anova_simples.html#estatística-f-e-teste-de-hipóteses",
    "title": "Análise de variância de um fator",
    "section": "5 Estatística \\(F\\) e teste de hipóteses",
    "text": "5 Estatística \\(F\\) e teste de hipóteses\nUma vez que os quadrados médios são estimativas de variância, uma estatística de teste apropriada é:\n\\[F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}}\\]\nA estatística \\(F\\) (ou razão-\\(F\\)) está associada à distribuição de probabilidades \\(F\\) e nos permite comparar a variância associada ao tratamento com a variância associada aos resíduos. Com o valor de \\(F_{calculado}\\), o teste de hipóteses é possível após a definição do nível de significância \\(\\alpha\\).\n\n5.1 Nível de significância\nAssim como discutimos nos testes \\(Z\\) e \\(t\\), o valor de \\(\\alpha\\) estabelece um limite de aceitação para \\(H_0\\), isto é, um limite a partir do qual a estatística do teste se torna tão extrema que nos leva a assumir que \\(H_0\\) é improvável, devendo portanto ser rejeitada em favor de \\(H_a\\). Este passo é possível pois o valor de \\(F_{calculado}\\) pode ser associado à distribuição \\(F\\) de probabilidades, o que nos permite calcular a probabilidade:\n\\[P(F_{calculado}) \\le \\alpha\\]\nPara facilitar a notação denominaremos \\(P(F_{calculado})\\) simplesmente de valor de \\(p\\) expresso em vermelho na figura abaixo:\n\n\n\n\n\n\nTomada de decisão na ANOVA\n\n\n\nSe \\(p &gt; \\alpha\\) –&gt; ACEITAMOS \\(H_0\\)\nSe \\(p \\le \\alpha\\) –&gt; REJEITAMOS \\(H_0\\) (e assumimos \\(H_a\\) como verdadeira)\n\n\n\n\n\n\n\n\n\n\n\nTradicionalmente utiliza-se \\(\\alpha = 0.05\\). Neste caso, \\(H_0\\) seria rejeitada somente de \\(p \\le 0.05\\). Em algumas situações podemos utilizar \\(\\alpha = 0.01\\), o que torna o experimento mais rigoroso, isto é, menos propenso ao erro do tipo I."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#um-exemplo-de-anova-os-níveis-de-metais-pesados-afetam-a-diversidade-de-espécies",
    "href": "conteudo/anova/anova_simples.html#um-exemplo-de-anova-os-níveis-de-metais-pesados-afetam-a-diversidade-de-espécies",
    "title": "Análise de variância de um fator",
    "section": "6 Um exemplo de ANOVA: os níveis de metais pesados afetam a diversidade de espécies?",
    "text": "6 Um exemplo de ANOVA: os níveis de metais pesados afetam a diversidade de espécies?\nImporte a base de dados medley.csv (disponível também em Chapter 10 - Single factor classification (ANOVA)) que avalia o impacto da presença de metais pesados na diversidade de espécies de diatomácias em riachos (Medley e Clements (1998); Queen, Quinn, e Keough (2002); Logan (2011)).\n\nmedley = read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/medley.csv\") |&gt; \n  mutate(STREAM = factor(STREAM),\n         ZINC = factor(ZINC, ordered = TRUE,\n                       levels = c(\"BACK\", \"LOW\", \"MED\", \"HIGH\")))\nvar_medley = colnames(medley)\nstream_levels = levels(medley$STREAM)\nn_stream = nlevels(medley$STREAM)\nzinc_levels = levels(medley$ZINC)\nn_zinc = nlevels(medley$ZINC)\n\n\nmedley |&gt; gt()\n\n\n\n\n\n\n\nSTREAM\nZINC\nDIVERSITY\n\n\n\n\nEagle\nBACK\n2.27\n\n\nEagle\nHIGH\n1.25\n\n\nEagle\nHIGH\n1.15\n\n\nEagle\nMED\n1.62\n\n\nBlue\nBACK\n1.70\n\n\nBlue\nHIGH\n0.63\n\n\nBlue\nBACK\n2.05\n\n\nBlue\nBACK\n1.98\n\n\nBlue\nHIGH\n1.04\n\n\nBlue\nMED\n2.19\n\n\nBlue\nMED\n2.10\n\n\nSnake\nBACK\n2.20\n\n\nSnake\nMED\n2.06\n\n\nSnake\nHIGH\n1.90\n\n\nSnake\nHIGH\n1.88\n\n\nSnake\nHIGH\n0.85\n\n\nArkan\nLOW\n1.40\n\n\nArkan\nLOW\n2.18\n\n\nArkan\nLOW\n1.83\n\n\nArkan\nLOW\n1.88\n\n\nArkan\nMED\n2.02\n\n\nArkan\nMED\n1.94\n\n\nArkan\nLOW\n2.10\n\n\nChalk\nLOW\n2.38\n\n\nChalk\nHIGH\n1.43\n\n\nChalk\nHIGH\n1.37\n\n\nChalk\nMED\n1.75\n\n\nChalk\nLOW\n2.83\n\n\nSplat\nBACK\n1.53\n\n\nSplat\nBACK\n0.76\n\n\nSplat\nMED\n0.80\n\n\nSplat\nLOW\n1.66\n\n\nSplat\nMED\n0.98\n\n\nSplat\nBACK\n1.89\n\n\n\n\n\n\n\nA coluna STREAM é uma variável categórica contendo o nome dos \\(6\\) riachos amostrados (Arkan, Blue, Chalk, Eagle, Snake, Splat). A coluna ZINC é uma variável categórica ordinal com \\(4\\) níveis de concentração de zinco na água (BACK &lt; LOW &lt; MED &lt; HIGH). O primeiro nível (BACK) é o nível de referência. Finalmente, a coluna DIVERSITY é uma variável contínua que contém a diversidade de diatomácieas medida pelo índice de Shannon em cada uma das 34 amostras.\nVamos nos concentrar nas variáveis DIVERSITY e ZINC. DIVERSITY será a variável resposta. Dizemos que ZINC é um tratamento, isto é, uma condição experimental (ou observacional) sob a qual a variável dependente \\(Y\\) foi medida.\nPara verificarmos a distribuição de diversidade para cada concentração de zinco vamos fazer um boxplot da variável DIVERSITY em função de ZINC.\n\n\nCódigo\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_boxplot(coef = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\n\n\n\n\n\nVemos que a concentração HIGH aparenta ter menor diversidade que as demais concentralções. A ANOVA nos permitirá testar esta suposição.\n\n\n\n\n\n\nHipópteses estatísticas\n\n\n\n\\(H_0: \\mu_{BACK} = \\mu_{LOW} = \\mu_{MED}  = \\mu_{HIGH}\\)\n\\(H_a\\): ao menos um \\(\\mu\\) é diferente\n\\(\\alpha = 0.05\\)\n\n\n\n6.1 Calculando a ANOVA\ni. Somatórios dos quadrados\n\\(SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = 2.5666124\\)\n\\(SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2 = 6.5164111\\)\nii. Graus de liberdade\n\\(gl_{Trat} = k - 1 = 3\\)\n\\(gl_{Res} = N-k = 30\\)\niii. Quadrados médios\n\\(QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}} = 0.8555375\\)\n\\(QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}} = 0.2172137\\)\niv. Estatística \\(F\\)\n\\(F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}} = 3.939\\)\n\n\n\n\n\n\nTabela da ANOVA**\n\n\n\nAs quantias acima são tradicionalmente expressas em uma Tabela de ANOVA.\n\n\nCódigo\naov_ex = aov(DIVERSITY ~ ZINC, data = medley)\nanova_ex = anova(aov_ex)\n\n\n\n\n\n\nTabela 1: Tabela da ANOVA para a base de dados medley.\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\n3\n2.566612\n0.8555375\n3.93869\n0.01755956\n\n\n30\n6.516411\n0.2172137\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nem que:\nDf: graus de liberdade\nSum Sq: soma dos quadrados\nMean Sq: quadrados médios\nF value: valor de \\(F_{calculado}\\)\nPr(&gt;F): valor de p\nA primeira linha refere-se aos valores associados aos tratamentos e a segunda linha aos resíduos. Note que o cômputo de \\(SQ_{Total}\\), \\(gl_{Total}\\) e \\(QM_{Total}\\) não é realmente necessário.\n\n\nO valor de \\(p = 0.0175596\\) mostrado na Tabela 1 refere-se à área na distribuição \\(F\\) que fica acima de \\(F_{calculado}\\) e que está representado em vermelho na Figura 1.\n\n\n\n\n\n\n\n\nFigura 1: Distribuição F com indicação do valor de p.\n\n\n\n\n\nAo verificar que \\(p \\le \\alpha\\), nossa conclusão deve ser de REJEITAR \\(H_0\\), pois \\(F_{calculado}\\) é muito extremo para ser resultante da hipótese nula. Neste caso, assumimos que a \\(H_a\\) é mais condizente com a estrutura dos dados, de modo que os tratamentos devem ser provenientes de populações estatísticas com diferentes médias \\(\\mu\\)."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#testes-a-posteriori-de-comparação-de-médias-o-teste-de-tukey",
    "href": "conteudo/anova/anova_simples.html#testes-a-posteriori-de-comparação-de-médias-o-teste-de-tukey",
    "title": "Análise de variância de um fator",
    "section": "7 Testes a posteriori de comparação de médias: o teste de Tukey",
    "text": "7 Testes a posteriori de comparação de médias: o teste de Tukey\nTendo rejeitado \\(H_0\\) concluímos que ao menos 1 par médias é diferente entre si. Nos resta saber quais pares são estatisticamente diferentes, o que nos leva a buscar por um teste que permita fazer comparações par-a-par. Os testes a posteriori são uma alternativa.\nEntre os diferentes testes a posteriori na literatura discutiremos o teste de Tukey, em que o objetivo é estabelecer uma Diferença Honesta Significativa (DHS) entre um dado par de médias. Considerando a diferença entre um par de médias e o erro padrão das diferenças de médias, a estatística do teste de Tukey é:\n\\[q = \\frac{\\overline{Y}_1 - \\overline{Y}_2}{SE}\\]\nem que:\n\\[SE = \\sqrt{\\frac{QM_{Res}}{2}(\\frac{1}{n_1} + \\frac{1}{n_2})}\\]\nonde:\n\\(q\\): é e estatística do teste\n\\(\\overline{Y}_1\\): é a maior das médias do par consideraddo;\n\\(\\overline{Y}_2\\): é a menor das médias do par consideraddo\n\\(QM_{Res}\\): é quadrado médio do resíduo obtido na ANOVA, e;\n\\(n_1\\), \\(n_2\\): os tamanhos amostrais de cada grupo envolvido na comparação.\nO valor crítico de \\(q\\) pode ser obtido de uma tabela estatística da distribuição de amplitude normalizada (studentized range q table). Para um dado \\(\\alpha\\), o valor desejado de \\(q\\) é encontrado cruzando a linha contento o número \\(k\\) de tratamentos do experimento com a linha contendo os graus de liberdade do resíduo (\\(gl_{Res}\\)). Veja um exemplo desta tabela no link: Studentized Range q Table.\nEm nosso exemplo, os valores de \\(q\\) entre os pares de médias serão:\n\nTukey_tab |&gt; \n  gt() |&gt; \n  cols_label(\n    combinacoes = \"Combinações\",\n    diff = \"Diferença\",\n    n1 = \"n1\",\n    n2 = \"n2\",\n    se = \"Erro Padrão\",\n    q = \"Estatística q\",\n    H0 = \"Decisão\"\n  ) |&gt; \n  fmt_number(\n    columns = c(diff, se, q),\n    decimals = 3\n  ) |&gt; \n  tab_style(\n    style = cell_fill(color = \"orange\"),\n    locations = cells_body(\n      columns = H0,\n      rows = q &gt;= qc\n    )\n  ) |&gt; \n  tab_options(\n    table.width = \"100%\",\n    column_labels.font.weight = \"bold\"\n  )\n\n\n\n\n\n\n\nCombinações\nDiferença\nn1\nn2\nErro Padrão\nEstatística q\nDecisão\n\n\n\n\nLOW-BACK\n0.235\n8\n8\n0.165\n1.426\nAceita H0\n\n\nMED-BACK\n0.080\n8\n8\n0.165\n0.484\nAceita H0\n\n\nHIGH-BACK\n0.520\n9\n8\n0.160\n3.246\nAceita H0\n\n\nMED-LOW\n0.315\n9\n8\n0.160\n1.965\nAceita H0\n\n\nHIGH-LOW\n0.755\n9\n8\n0.160\n4.713\nRejeita H0\n\n\nHIGH-MED\n0.440\n9\n9\n0.155\n2.832\nAceita H0\n\n\n\n\n\n\n\nO limite crítico para o valor de \\(q\\) tabelado é \\(q_{0.95,30,4} = 3.845\\) (veja em: Studentized Range q Table), deste modo somente a comparação entre HIGH-LOW sugere ter médias significativamente diferentes."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#ajustando-a-anova-no-r",
    "href": "conteudo/anova/anova_simples.html#ajustando-a-anova-no-r",
    "title": "Análise de variância de um fator",
    "section": "8 Ajustando a ANOVA no R",
    "text": "8 Ajustando a ANOVA no R\nA ANOVA no R pode ser feita com o comando aov.\n\najuste = aov(DIVERSITY ~ ZINC, data = medley)\najuste\n\nCall:\n   aov(formula = DIVERSITY ~ ZINC, data = medley)\n\nTerms:\n                    ZINC Residuals\nSum of Squares  2.566612  6.516411\nDeg. of Freedom        3        30\n\nResidual standard error: 0.4660619\nEstimated effects may be unbalanced\n\n\n\n\n\n\n\n\nFórmula no R\n\n\n\nA notação de fórmula no R é escrita como: Y ~ Xonde lê-se \\(Y\\) é função de \\(X\\).\n\n\nO comando acima fez os cálculos da ANOVA, isto é, computou as somas dos quadrados, os graus de liberdade, os quadrados médios, o \\(F_{calculado}\\) e o valor de \\(p\\). Para visualizarmos a tabela da ANOVA escrevemos:\n\nanova(ajuste)\n\nAnalysis of Variance Table\n\nResponse: DIVERSITY\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nZINC       3 2.5666 0.85554  3.9387 0.01756 *\nResiduals 30 6.5164 0.21721                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote que os resultados coincidem com o que apresentamos anteriormente. Como o valor de \\(p\\) foi menor que \\(\\alpha = 0.05\\), concluimos que a ANOVA foi significativa, isto é, indicou que ao menos um par de médias difere entre si. Podemos fazer o teste a posteriori de Tukey com o comando:\n\nalfa = 0.05\nTukeyHSD(ajuste, conf.level = 1-alfa)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = DIVERSITY ~ ZINC, data = medley)\n\n$ZINC\n                 diff        lwr         upr     p adj\nLOW-BACK   0.23500000 -0.3986367  0.86863665 0.7457444\nMED-BACK  -0.07972222 -0.6955064  0.53606192 0.9847376\nHIGH-BACK -0.51972222 -1.1355064  0.09606192 0.1218677\nMED-LOW   -0.31472222 -0.9305064  0.30106192 0.5153456\nHIGH-LOW  -0.75472222 -1.3705064 -0.13893808 0.0116543\nHIGH-MED  -0.44000000 -1.0373984  0.15739837 0.2095597\n\n\nO resultado apresenta todas as comparações possíveis entre os grupos, mostrando as diferenças de médias, seus intervalos de confiança a \\(95\\%\\) e os valores de \\(p\\), indicando quais destas diferenças são significativas (\\(p \\le \\alpha\\)). Nvamente, estes resultados nos permitem concluir que somente o par HIGH-LOW difere entre si, pois p adj &lt; 0.05.\nO gráfico abaixo facilita a visualização das comparações, sobretudo em situações com muitos pares de médias envolvidos:\n\nplot(TukeyHSD(ajuste))\n\n\n\n\n\n\n\n\nNeste gráfico, são consideradas estatisticamente significativas as comparações em que o intervalo de confiança não inclui o zero."
  },
  {
    "objectID": "conteudo/anova/anova_simples.html#pressupostos-da-anova",
    "href": "conteudo/anova/anova_simples.html#pressupostos-da-anova",
    "title": "Análise de variância de um fator",
    "section": "9 Pressupostos da ANOVA",
    "text": "9 Pressupostos da ANOVA\nOs pressupostos da ANOVA são:\n\nAs observações são independentes e;\nA variância dos resíduos é homogênea e;\nOs resíduos têm distribuição normal com média \\(0\\) e variância consante \\(\\sigma^2\\).\n\nVamos inicialmente testar o pressuposto de homogeneidade de variâncias com um teste \\(F\\).\n\nmedley |&gt; group_by(ZINC) |&gt; \n  summarise(Var = var(DIVERSITY)) |&gt; \n  gt()\n\n\n\n\n\n\n\nZINC\nVar\n\n\n\n\nBACK\n0.2354786\n\n\nLOW\n0.1980214\n\n\nMED\n0.2530194\n\n\nHIGH\n0.1822194\n\n\n\n\n\n\n\nNote que a maior variância é \\(0.2530194\\) e a menor \\(0.1822194\\).\nO teste \\(F\\) consiste em dividir a maior variância pela menor:\n\nvmax = medley$DIVERSITY[medley$ZINC == \"MED\"]\nvmin = medley$DIVERSITY[medley$ZINC == \"HIGH\"]\nvar.test(vmax, vmin)\n\n\n    F test to compare two variances\n\ndata:  vmax and vmin\nF = 1.3885, num df = 8, denom df = 8, p-value = 0.6534\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3132103 6.1557698\nsample estimates:\nratio of variances \n          1.388543 \n\n\nA maior variância foi 1.39 vezes maior que a menor variância e o test F sugere que esta diferença é não-significativa a \\(5\\%\\) (\\(p &lt; 0.05\\)). Isto indica que as variâncias são homogêneas.\nA verificação visual de que as variâncias são homogêneas pode também ser inspecionada pelo gráfico de resíduos:\n\nplot(rstudent(ajuste) ~ fitted(ajuste), pch = 16)\nabline(h = 0, col = 2)\n\n\n\n\n\n\n\n\nEm seguida avaliamos o histograma dos resíduos e aplicamos um teste de normalidade (ex. teste de Shapiro-Wilk) para verificar se o pressuposto de normalidade pode ser aceito.\n\nhist(rstudent(ajuste), breaks = 10)\n\n\n\n\n\n\n\nshapiro.test(rstudent(ajuste))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstudent(ajuste)\nW = 0.96696, p-value = 0.3828\n\n\nNeste caso, o valor de \\(p &gt; 0.05\\) indica não haver desvio da normalidade."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html",
    "href": "conteudo/manipulacao-dados-R/pipe.html",
    "title": "Operadores pipe",
    "section": "",
    "text": "Em R, operadores pipe são usados para passar a saída de uma função para a entrada de outra, tornando o código mais legível e conciso. Este tutorial compara o operador pipe nativo |&gt; introduzido no R 4.1.0 e o operador %&gt;% do pacote magrittr."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#operador-pipe-nativo",
    "href": "conteudo/manipulacao-dados-R/pipe.html#operador-pipe-nativo",
    "title": "Operadores pipe",
    "section": "1 Operador Pipe nativo |>",
    "text": "1 Operador Pipe nativo |&gt;\nO operador pipe nativo |&gt; é uma nova adição ao R base. Ele permite escrever código mais limpo e legível ao encadear funções.\n\n# Exemplo usando o operador pipe nativo |&gt;\nresultado &lt;- 1:10 |&gt; \n  sum() |&gt;\n  sqrt()\n\nresultado\n\n[1] 7.416198\n\n\nNeste exemplo, a sequência de 1 a 10 é passada para a função sum(), e o resultado é então passado para a função `sqrt()```.\nO mesmo resultado é obtido sem o operador pipe por:\n\nresultado &lt;- sqrt(sum(1:10))\n\nresultado\n\n[1] 7.416198"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#operador-pipe-do-pacote-magrittr",
    "href": "conteudo/manipulacao-dados-R/pipe.html#operador-pipe-do-pacote-magrittr",
    "title": "Operadores pipe",
    "section": "2 Operador Pipe do pacote magrittr %>%",
    "text": "2 Operador Pipe do pacote magrittr %&gt;%\nO operador %&gt;% do pacote `magrittr``` tem sido amplamente usado na comunidade R há vários anos. Ele serve ao mesmo propósito que o operador pipe nativo, mas possui alguns recursos adicionais.\n\nlibrary(magrittr)\n\n# Exemplo usando o operador pipe do magrittr %&gt;%\nresultado &lt;- 1:10 %&gt;%\n  sum() %&gt;%\n  sqrt()\n\nresultado\n\n[1] 7.416198\n\n\nEste exemplo alcança o mesmo resultado que o anterior, mas usa o operador %&gt;% do pacote magrittr."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#diferenças-e-considerações",
    "href": "conteudo/manipulacao-dados-R/pipe.html#diferenças-e-considerações",
    "title": "Operadores pipe",
    "section": "3 Diferenças e Considerações",
    "text": "3 Diferenças e Considerações\n\n3.1 Suporte a Placeholder\nUma diferença chave é que %&gt;% suporta placeholders (.), que podem ser úteis para pipelines mais complexos.\n\n# Usando placeholder com %&gt;%\nresultado &lt;- 1:10 %&gt;%\n  sum() %&gt;%\n  { . / 2 } %&gt;%\n  sqrt()\n\nresultado\n\n[1] 5.244044\n\n\nO operador pipe nativo |&gt; não suporta placeholders diretamente.\nUsando o pipe nativo, a mesma expressão ficaria:\n\nresultado &lt;- 1:10 |&gt;\n  sum() |&gt;\n  (\\(x) x / 2)() |&gt;  # Esta linha é similar à: `(function(x) x / 2)()`\n  sqrt()\n\nPortanto, torna-se necessário declarar uma função dentro da sequência se comandos."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#tratamento-de-erros-e-depuração",
    "href": "conteudo/manipulacao-dados-R/pipe.html#tratamento-de-erros-e-depuração",
    "title": "Operadores pipe",
    "section": "4 Tratamento de Erros e Depuração",
    "text": "4 Tratamento de Erros e Depuração\nO operador %&gt;% do magrittr fornece mensagens de erro mais detalhadas e melhores capacidades de depuração. Se você encontrar um erro em um pipeline usando |&gt;, a mensagem de erro pode ser menos informativa em comparação com o uso de %&gt;%."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#desempenho",
    "href": "conteudo/manipulacao-dados-R/pipe.html#desempenho",
    "title": "Operadores pipe",
    "section": "5 Desempenho",
    "text": "5 Desempenho\nAmbos os operadores pipe têm desempenho semelhante na maioria dos casos. No entanto, |&gt; por ser parte do R base, pode ter ligeiras vantagens de desempenho em alguns cenários devido à sua integração com a linguagem principal."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#quando-usar-operadores-pipe",
    "href": "conteudo/manipulacao-dados-R/pipe.html#quando-usar-operadores-pipe",
    "title": "Operadores pipe",
    "section": "6 Quando usar operadores pipe",
    "text": "6 Quando usar operadores pipe\nTanto o operador pipe nativo |&gt; quanto o operador %&gt;% do magrittr são ferramentas poderosas para escrever código R limpo e legível. A escolha entre eles depende de suas necessidades específicas e preferências. Se você precisa de suporte a placeholders e depuração aprimorada, %&gt;% é uma boa escolha. Para uma dependência mais leve e potencialmente melhor desempenho, |&gt; é uma opção sólida. A seguir algumas sugestões para a escolha entre os operadores.\n\nLeitura e Legibilidade: Use operadores pipe quando você deseja aumentar a legibilidade do código. Eles ajudam a encadear operações de forma linear, tornando o fluxo de dados claro e fácil de seguir.\n\n\nresultado &lt;- dados %&gt;%\n  filter(variavel1 &gt; 10) %&gt;%  # Filtra os dados\n  mutate(nova_variavel = variavel2 * 2) %&gt;%  # insere `nova_variavel` no data frame\n  summarise(media = mean(nova_variavel))  # extrai a média da `nova_variavel`\n\n\nTransformações Sequenciais: Use operadores pipe quando você precisa aplicar uma série de transformações sequenciais nos dados. Eles permitem que você evite a criação de variáveis temporárias.\n\n\nresultado &lt;- dados |&gt;\n  filter(variavel1 &gt; 10) |&gt;\n  mutate(nova_variavel = variavel2 * 2) |&gt;\n  summarise(media = mean(nova_variavel))\n\n# Sem o operador pipe, esta sequância de códigos poderia ficar:\nres1 &lt;- filter(dados, variável1 &gt; 10)\nres2 &lt;- mutate(res1, nova_variavel = variavel2 * 2)\nresultado &lt;- summarise(res2, media = mean(nova_variavel))\n\n\nConsistência de Sintaxe: Utilize pipes para manter uma sintaxe consistente em todo o seu código, especialmente se você estiver trabalhando em um projeto colaborativo onde a consistência de estilo é importante.\nSimplificação de Funções Aninhadas: Empregue operadores pipe para simplificar a leitura de funções aninhadas, evitando a necessidade de múltiplos parênteses.\n\n\nresultado &lt;- sqrt(sum(1:10))\n\n# versus\n\nresultado_pipe &lt;- 1:10 |&gt;\n  sum() |&gt;\n  sqrt()\n\n\nresultado\n\n[1] 7.416198\n\nresultado_pipe\n\n[1] 7.416198\n\n\n\nCodificação Explorativa e Prototipagem Rápida: Use pipes durante a exploração de dados e prototipagem rápida, pois eles permitem que você altere e teste rapidamente diferentes transformações."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/pipe.html#quando-não-usar-operadores-pipe",
    "href": "conteudo/manipulacao-dados-R/pipe.html#quando-não-usar-operadores-pipe",
    "title": "Operadores pipe",
    "section": "7 Quando Não Usar Operadores Pipe",
    "text": "7 Quando Não Usar Operadores Pipe\n\nSimplicidade Excessiva: Evite usar operadores pipe para operações extremamente simples onde o uso de pipes não adiciona clareza. Por exemplo, sum(1:10) é mais claro sem o pipe.\nDepuração de Código: Não use pipes se você está tendo dificuldades para depurar uma sequência de operações. Em vez disso, atribua resultados intermediários a variáveis temporárias para inspecioná-los.\n\n\npasso1 &lt;- filter(dados, variavel1 &gt; 10)\npasso2 &lt;- mutate(passo1, nova_variavel = variavel2 * 2)\nresultado &lt;- summarise(passo2, media = mean(nova_variavel))\n\n\nOperações Complexas com Várias Etapas: Evite usar pipes em operações muito complexas que envolvem várias etapas interdependentes, onde a clareza do código pode ser comprometida. Por exemplo se você precisa manipular dois data frames independentes e depois uní-los, fazer isso em uma única sequencia de operadores pipe pode tornar o código difícil de interpretar.\n\n\n# Criando exemplos de data frames\ndados1 &lt;- data.frame(\n  categoria = rep(c(\"A\", \"B\", \"C\"), each = 4),\n  variavel1 = rnorm(12, mean = 6, sd = 2)\n)\n\ndados2 &lt;- data.frame(\n  categoria = rep(c(\"A\", \"B\", \"C\"), each = 4),\n  variavel2 = rnorm(12, mean = 10, sd = 5)\n)\n\nlibrary(dplyr)\n# Operações complexas em uma única sequência de operadores pipe\nresultado &lt;- dados1 |&gt;\n  group_by(categoria) |&gt;\n  summarise(media_variavel1 = mean(variavel1)) |&gt;\n  inner_join(\n    dados2 |&gt; \n      group_by(categoria) |&gt; \n      summarise(soma_variavel2 = sum(variavel2)),\n    by = \"categoria\"\n  ) |&gt;\n  mutate(nova_variavel = soma_variavel2 / media_variavel1) |&gt;\n  arrange(desc(nova_variavel))\n\nresultado\n\n# A tibble: 3 × 4\n  categoria media_variavel1 soma_variavel2 nova_variavel\n  &lt;chr&gt;               &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 C                    6.15           51.7          8.41\n2 B                    6.38           39.8          6.24\n3 A                    5.87           27.0          4.60\n\n\nO exemplo acima pode ser reescrito de forma que cada etapa tenha uma leitura mais clara.\n\n# Passo 1: Filtrar e resumir dados1\nresumo_dados1 &lt;- dados1 |&gt;\n  group_by(categoria) |&gt;\n  summarise(media_variavel1 = mean(variavel1))\n\n# Passo 2: Filtrar e resumir dados2\nresumo_dados2 &lt;- dados2 |&gt;\n  group_by(categoria) |&gt;\n  summarise(soma_variavel2 = sum(variavel2))\n\n# Passo 3: Unir os resultados dos dois data frames\nresultado_unido &lt;- inner_join(resumo_dados1, \n                                     resumo_dados2, \n                                     by = \"categoria\")  |&gt;  \n  mutate(nova_variavel = soma_variavel2 / media_variavel1) |&gt;\n  arrange(desc(nova_variavel))\n\nresultado_unido\n\n# A tibble: 3 × 4\n  categoria media_variavel1 soma_variavel2 nova_variavel\n  &lt;chr&gt;               &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 C                    6.15           51.7          8.41\n2 B                    6.38           39.8          6.24\n3 A                    5.87           27.0          4.60\n\n\nEmbora o código tenha ficado mais longo, fica também mais simples de ser inspecionado.\n\nDesempenho Crítico: Se você está preocupado com o desempenho crítico e a eficiência, pode ser melhor evitar pipes, já que eles podem adicionar alguma sobrecarga.\nAmbiguidade de Funções: Evite pipes se o uso deles torna a ordem das operações ou a origem dos dados ambígua. Certifique-se de que a sequência de operações é clara e lógica."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html",
    "href": "conteudo/manipulacao-dados-R/transform.html",
    "title": "Transformação de Dados",
    "section": "",
    "text": "Após importar uma base de dados para o R, os pacotes dplyr e tidyr são essenciais para transformação de data frames. As funções desses pacotes ajudam na análise, modelagem e comunicação de dados. Neste seção são apresentadas as principais funções para transformar observações (linhas) e variáveis em um data frame a partir de uma ou mais tabelas. A Cheatsheets do dplyr apresenta outros recursos não discutidos nesta seção."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#ordenando-as-linhas-funções-arrange-e-desc",
    "href": "conteudo/manipulacao-dados-R/transform.html#ordenando-as-linhas-funções-arrange-e-desc",
    "title": "Transformação de Dados",
    "section": "1 Ordenando as linhas: funções arrange() e desc()",
    "text": "1 Ordenando as linhas: funções arrange() e desc()\nAs funções arrange() e desc() permitem ordenar a base de dados com base nos valores de uma ou mais colunas. Usará-se o conjunto de dados iris como exemplo.\nCarregue os pacote dplyr, tidyr e readr.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\nPara carregar e visualizar as primeiras linhas da base de dados iris:\n\ndata(\"iris\")\nhead(iris, 10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\nPara ordenar a tabela pela coluna Sepal.Length em ordem crescente:\n\niris |&gt; \n  arrange(Sepal.Length)\n\nPara ordenar em ordem decrescente:\n\niris |&gt; \n  arrange(desc(Sepal.Length))\n\nÉ possível também combinar duas colunas, ordenando a tabela pela coluna Species (em ordem alfabética decrescente) e Sepal.Length (em ordem crescente):\n\niris |&gt; \n  arrange(desc(Species), Sepal.Length)\n\nPara criar um novo objeto com a tabela ordenada\n\niris_ordenado &lt;- iris |&gt; \n  arrange(Sepal.Length)\n\niris_ordenado"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#filtrando-linhas-função-filter",
    "href": "conteudo/manipulacao-dados-R/transform.html#filtrando-linhas-função-filter",
    "title": "Transformação de Dados",
    "section": "2 Filtrando linhas: função filter()",
    "text": "2 Filtrando linhas: função filter()\nA função filter() extrai linhas que satisfazem uma condição lógica. Para filtrar as linhas referentes à espécie virginica:\n\niris |&gt; \n  filter(Species == \"virginica\")\n\nPara filtrar espécies diferentes de virginica.\n\niris |&gt; \n  filter(Species != \"virginica\")\n\nPara filtrar linhas onde o comprimento das pétalas seja menor que \\(1.3\\):\n\niris |&gt; \n  filter(Petal.Length &lt; 1.3)\n\nPara filtrar onde o comprimento das pétalas seja menor que \\(1.3\\) e o comprimento das sépalas seja maior ou igual a \\(5\\):\n\niris |&gt; \n  filter(Petal.Length &lt; 1.3 & Sepal.Length &gt;= 5)"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#selecionando-colunas-função-select",
    "href": "conteudo/manipulacao-dados-R/transform.html#selecionando-colunas-função-select",
    "title": "Transformação de Dados",
    "section": "3 Selecionando colunas: função select()",
    "text": "3 Selecionando colunas: função select()\nA função select() permite extrair ou reorganizar um subconjunto de colunas de um data frame.\nPara extrair uma coluna:\n\niris |&gt; \n  select(Petal.Length)\n\nPara extrair múltiplas colunas:\n\niris |&gt; \n  select(Petal.Length, Species)\n\nPara extrair um intervalo de colunas:\n\niris |&gt; \n  select(Petal.Length:Species)\n\nPara excluir uma coluna:\n\niris |&gt; \n  select(-Petal.Length)\n\nPara excluir colunas específicas:\n\niris |&gt; \n  select(!c(Petal.Length, Species))\n\nPara selecionar colunas que começam com “Sepal”:\n\niris |&gt; \n  select(starts_with(\"Sepal\"))\n\nPara combinar filter() e select() a fim de extrair um subconjunto do data frame:\n\niris |&gt; \n  select(starts_with(\"Sepal\")) |&gt; \n  filter(Sepal.Length &lt;= 4.5)\n\n\n3.1 Selecionando/Excluindo variáveis numéricas e categóricas\nImporte o conjuntoi de dados Reservatorios_Parana_parcial.csv:\n\nres = read_delim(file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n                  delim = ',',\n                  locale = locale(decimal_mark = '.',\n                                  encoding = 'latin1'))\n\n\n3.1.1 Seleção de variáveis categóricas\n\nres |&gt;\n  select(Reservatorio, Bacia, Trofia)\n\n# A tibble: 31 × 3\n   Reservatorio Bacia  Trofia       \n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;        \n 1 Cavernoso    Iguacu OligotrÃ³fico\n 2 Curucaca     Iguacu OligotrÃ³fico\n 3 Foz do Areia Iguacu OligotrÃ³fico\n 4 Irai         Iguacu EutrÃ³fico   \n 5 JMF          Iguacu MesotrÃ³fico \n 6 Jordao       Iguacu OligotrÃ³fico\n 7 Passauna     Iguacu OligotrÃ³fico\n 8 Piraquara    Iguacu OligotrÃ³fico\n 9 Salto Caxias Iguacu OligotrÃ³fico\n10 Salto do Vau Iguacu OligotrÃ³fico\n# ℹ 21 more rows\n\nres |&gt;\n  select(where(is.character))\n\n# A tibble: 31 × 3\n   Reservatorio Bacia  Trofia       \n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;        \n 1 Cavernoso    Iguacu OligotrÃ³fico\n 2 Curucaca     Iguacu OligotrÃ³fico\n 3 Foz do Areia Iguacu OligotrÃ³fico\n 4 Irai         Iguacu EutrÃ³fico   \n 5 JMF          Iguacu MesotrÃ³fico \n 6 Jordao       Iguacu OligotrÃ³fico\n 7 Passauna     Iguacu OligotrÃ³fico\n 8 Piraquara    Iguacu OligotrÃ³fico\n 9 Salto Caxias Iguacu OligotrÃ³fico\n10 Salto do Vau Iguacu OligotrÃ³fico\n# ℹ 21 more rows\n\n\n\n\n3.1.2 Seleção de variáveis numéricas\n\nres |&gt;\n  select(Fechamento, Area, pH, Condutividade, Alcalinidade, P.total, Riqueza, CPUE)\n\n# A tibble: 31 × 8\n   Fechamento   Area    pH Condutividade Alcalinidade P.total Riqueza  CPUE\n        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       1965   2.9    7.4          33.1        140.      7.8      18  9.22\n 2       1982   2      7            32.4        126.      4.7      16 28.7 \n 3       1980 139      7.3          35.5         97      14.3      19 11.6 \n 4       2000  15      6.9          50.2          3.3    53.4      12 30.8 \n 5       1970   0.45   7.3          40.2          3.7    41.2      18  5.95\n 6       1996   3.4    7.1          23.7        153.      3.3      17  7.75\n 7       1978  14      8.8         126.         526      15.2      11  7.51\n 8       1979   3.3    7.1          22.8         50.7     4.5       8  4.01\n 9       1998 124      7.3          39.6        106      12.1      21 20.8 \n10       1959   2.9    6.5          23.2        279      11         8  2.43\n# ℹ 21 more rows\n\nres |&gt;\n  select(Fechamento, Area, pH:CPUE)\n\n# A tibble: 31 × 8\n   Fechamento   Area    pH Condutividade Alcalinidade P.total Riqueza  CPUE\n        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       1965   2.9    7.4          33.1        140.      7.8      18  9.22\n 2       1982   2      7            32.4        126.      4.7      16 28.7 \n 3       1980 139      7.3          35.5         97      14.3      19 11.6 \n 4       2000  15      6.9          50.2          3.3    53.4      12 30.8 \n 5       1970   0.45   7.3          40.2          3.7    41.2      18  5.95\n 6       1996   3.4    7.1          23.7        153.      3.3      17  7.75\n 7       1978  14      8.8         126.         526      15.2      11  7.51\n 8       1979   3.3    7.1          22.8         50.7     4.5       8  4.01\n 9       1998 124      7.3          39.6        106      12.1      21 20.8 \n10       1959   2.9    6.5          23.2        279      11         8  2.43\n# ℹ 21 more rows\n\nres |&gt;\n  select(where(is.numeric))\n\n# A tibble: 31 × 8\n   Fechamento   Area    pH Condutividade Alcalinidade P.total Riqueza  CPUE\n        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       1965   2.9    7.4          33.1        140.      7.8      18  9.22\n 2       1982   2      7            32.4        126.      4.7      16 28.7 \n 3       1980 139      7.3          35.5         97      14.3      19 11.6 \n 4       2000  15      6.9          50.2          3.3    53.4      12 30.8 \n 5       1970   0.45   7.3          40.2          3.7    41.2      18  5.95\n 6       1996   3.4    7.1          23.7        153.      3.3      17  7.75\n 7       1978  14      8.8         126.         526      15.2      11  7.51\n 8       1979   3.3    7.1          22.8         50.7     4.5       8  4.01\n 9       1998 124      7.3          39.6        106      12.1      21 20.8 \n10       1959   2.9    6.5          23.2        279      11         8  2.43\n# ℹ 21 more rows\n\n\n\n\n\n3.2 Exclusão de variáveis\n\nres |&gt;\n  select(-Fechamento, -Area)\n\n# A tibble: 31 × 9\n   Reservatorio Bacia  Trofia      pH Condutividade Alcalinidade P.total Riqueza\n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Cavernoso    Iguacu Oligotr…   7.4          33.1        140.      7.8      18\n 2 Curucaca     Iguacu Oligotr…   7            32.4        126.      4.7      16\n 3 Foz do Areia Iguacu Oligotr…   7.3          35.5         97      14.3      19\n 4 Irai         Iguacu EutrÃ³f…   6.9          50.2          3.3    53.4      12\n 5 JMF          Iguacu MesotrÃ…   7.3          40.2          3.7    41.2      18\n 6 Jordao       Iguacu Oligotr…   7.1          23.7        153.      3.3      17\n 7 Passauna     Iguacu Oligotr…   8.8         126.         526      15.2      11\n 8 Piraquara    Iguacu Oligotr…   7.1          22.8         50.7     4.5       8\n 9 Salto Caxias Iguacu Oligotr…   7.3          39.6        106      12.1      21\n10 Salto do Vau Iguacu Oligotr…   6.5          23.2        279      11         8\n# ℹ 21 more rows\n# ℹ 1 more variable: CPUE &lt;dbl&gt;\n\nres |&gt;\n  select(!where(is.numeric))\n\n# A tibble: 31 × 3\n   Reservatorio Bacia  Trofia       \n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;        \n 1 Cavernoso    Iguacu OligotrÃ³fico\n 2 Curucaca     Iguacu OligotrÃ³fico\n 3 Foz do Areia Iguacu OligotrÃ³fico\n 4 Irai         Iguacu EutrÃ³fico   \n 5 JMF          Iguacu MesotrÃ³fico \n 6 Jordao       Iguacu OligotrÃ³fico\n 7 Passauna     Iguacu OligotrÃ³fico\n 8 Piraquara    Iguacu OligotrÃ³fico\n 9 Salto Caxias Iguacu OligotrÃ³fico\n10 Salto do Vau Iguacu OligotrÃ³fico\n# ℹ 21 more rows\n\nres |&gt;\n  select(!where(is.character))\n\n# A tibble: 31 × 8\n   Fechamento   Area    pH Condutividade Alcalinidade P.total Riqueza  CPUE\n        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       1965   2.9    7.4          33.1        140.      7.8      18  9.22\n 2       1982   2      7            32.4        126.      4.7      16 28.7 \n 3       1980 139      7.3          35.5         97      14.3      19 11.6 \n 4       2000  15      6.9          50.2          3.3    53.4      12 30.8 \n 5       1970   0.45   7.3          40.2          3.7    41.2      18  5.95\n 6       1996   3.4    7.1          23.7        153.      3.3      17  7.75\n 7       1978  14      8.8         126.         526      15.2      11  7.51\n 8       1979   3.3    7.1          22.8         50.7     4.5       8  4.01\n 9       1998 124      7.3          39.6        106      12.1      21 20.8 \n10       1959   2.9    6.5          23.2        279      11         8  2.43\n# ℹ 21 more rows"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#outros-exemplos-de-seleçãoexclusão-de-variáveis",
    "href": "conteudo/manipulacao-dados-R/transform.html#outros-exemplos-de-seleçãoexclusão-de-variáveis",
    "title": "Transformação de Dados",
    "section": "4 Outros exemplos de seleção/exclusão de variáveis",
    "text": "4 Outros exemplos de seleção/exclusão de variáveis\n\n4.1 all_off(), any_of(), one_of()\n\n# 1. all_of(): Seleciona todas as colunas mencionadas\nres |&gt;\n  select(all_of(c('Reservatorio', 'Bacia')))\n\n# A tibble: 31 × 2\n   Reservatorio Bacia \n   &lt;chr&gt;        &lt;chr&gt; \n 1 Cavernoso    Iguacu\n 2 Curucaca     Iguacu\n 3 Foz do Areia Iguacu\n 4 Irai         Iguacu\n 5 JMF          Iguacu\n 6 Jordao       Iguacu\n 7 Passauna     Iguacu\n 8 Piraquara    Iguacu\n 9 Salto Caxias Iguacu\n10 Salto do Vau Iguacu\n# ℹ 21 more rows\n\n# 2. any_of(): Seleciona qualquer coluna que exista na lista (ignora colunas inexistentes)\nres |&gt;\n  select(any_of(c('Reservatorio', 'Bacia', 'Turbidez'))) # funciona, any_of() ignora que `Turbidez` não existe\n\n# A tibble: 31 × 2\n   Reservatorio Bacia \n   &lt;chr&gt;        &lt;chr&gt; \n 1 Cavernoso    Iguacu\n 2 Curucaca     Iguacu\n 3 Foz do Areia Iguacu\n 4 Irai         Iguacu\n 5 JMF          Iguacu\n 6 Jordao       Iguacu\n 7 Passauna     Iguacu\n 8 Piraquara    Iguacu\n 9 Salto Caxias Iguacu\n10 Salto do Vau Iguacu\n# ℹ 21 more rows\n\nres |&gt;\n  select(one_of(c('Reservatorio', 'Trofia', 'Turbidez'))) # funciona, one_of() avisa que `Turbidez` não existe\n\n# A tibble: 31 × 2\n   Reservatorio Trofia       \n   &lt;chr&gt;        &lt;chr&gt;        \n 1 Cavernoso    OligotrÃ³fico\n 2 Curucaca     OligotrÃ³fico\n 3 Foz do Areia OligotrÃ³fico\n 4 Irai         EutrÃ³fico   \n 5 JMF          MesotrÃ³fico \n 6 Jordao       OligotrÃ³fico\n 7 Passauna     OligotrÃ³fico\n 8 Piraquara    OligotrÃ³fico\n 9 Salto Caxias OligotrÃ³fico\n10 Salto do Vau OligotrÃ³fico\n# ℹ 21 more rows\n\n# res |&gt;\n#   select(Reservatorio, Bacia, Turbidez) # Não funciona, pois `Turbidez` não existe\n\n\n\n4.2 contains(), ends_with(), everything(), last_col()\n\n# 3. contains(): Seleciona colunas cujos nomes contêm uma string específica\nres |&gt;\n  select(contains('to'))\n\n# A tibble: 31 × 3\n   Reservatorio Fechamento P.total\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 Cavernoso          1965     7.8\n 2 Curucaca           1982     4.7\n 3 Foz do Areia       1980    14.3\n 4 Irai               2000    53.4\n 5 JMF                1970    41.2\n 6 Jordao             1996     3.3\n 7 Passauna           1978    15.2\n 8 Piraquara          1979     4.5\n 9 Salto Caxias       1998    12.1\n10 Salto do Vau       1959    11  \n# ℹ 21 more rows\n\n# 4. ends_with(): Seleciona colunas que terminam com uma string específica\nres |&gt;\n  select(ends_with('dade'))\n\n# A tibble: 31 × 2\n   Condutividade Alcalinidade\n           &lt;dbl&gt;        &lt;dbl&gt;\n 1          33.1        140. \n 2          32.4        126. \n 3          35.5         97  \n 4          50.2          3.3\n 5          40.2          3.7\n 6          23.7        153. \n 7         126.         526  \n 8          22.8         50.7\n 9          39.6        106  \n10          23.2        279  \n# ℹ 21 more rows\n\n# 5. everything(): Seleciona todas as colunas (pode ser usado para reorganizar)\nres |&gt;\n  select(Fechamento, pH, everything())\n\n# A tibble: 31 × 11\n   Fechamento    pH Reservatorio Bacia    Area Trofia Condutividade Alcalinidade\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1       1965   7.4 Cavernoso    Iguacu   2.9  Oligo…          33.1        140. \n 2       1982   7   Curucaca     Iguacu   2    Oligo…          32.4        126. \n 3       1980   7.3 Foz do Areia Iguacu 139    Oligo…          35.5         97  \n 4       2000   6.9 Irai         Iguacu  15    EutrÃ…          50.2          3.3\n 5       1970   7.3 JMF          Iguacu   0.45 Mesot…          40.2          3.7\n 6       1996   7.1 Jordao       Iguacu   3.4  Oligo…          23.7        153. \n 7       1978   8.8 Passauna     Iguacu  14    Oligo…         126.         526  \n 8       1979   7.1 Piraquara    Iguacu   3.3  Oligo…          22.8         50.7\n 9       1998   7.3 Salto Caxias Iguacu 124    Oligo…          39.6        106  \n10       1959   6.5 Salto do Vau Iguacu   2.9  Oligo…          23.2        279  \n# ℹ 21 more rows\n# ℹ 3 more variables: P.total &lt;dbl&gt;, Riqueza &lt;dbl&gt;, CPUE &lt;dbl&gt;\n\n# 6. last_col(): Seleciona a última coluna\nres |&gt;\n  select(last_col())\n\n# A tibble: 31 × 1\n    CPUE\n   &lt;dbl&gt;\n 1  9.22\n 2 28.7 \n 3 11.6 \n 4 30.8 \n 5  5.95\n 6  7.75\n 7  7.51\n 8  4.01\n 9 20.8 \n10  2.43\n# ℹ 21 more rows\n\n\n\n\n4.3 Expressoes regulares\n\n# 7. matches(): Seleciona colunas que correspondem a uma expressão regular\nres |&gt;\n  select(matches('^[FA]')) # Colunas que começam com 'F ou A'\n\n# A tibble: 31 × 3\n   Fechamento   Area Alcalinidade\n        &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1       1965   2.9         140. \n 2       1982   2           126. \n 3       1980 139            97  \n 4       2000  15             3.3\n 5       1970   0.45          3.7\n 6       1996   3.4         153. \n 7       1978  14           526  \n 8       1979   3.3          50.7\n 9       1998 124           106  \n10       1959   2.9         279  \n# ℹ 21 more rows\n\n# 8. Seleciona colunas cujos nomes:\n# 1. Começam com a letra \"A\" ou \"C\"\n# 2. Podem conter qualquer sequência de caracteres após a primeira letra\n# 3. Terminam com as letras \"a\" ou \"e\"\nres |&gt;\n  select(matches('^[AC].*[ae]$'))\n\n# A tibble: 31 × 4\n     Area Condutividade Alcalinidade  CPUE\n    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1   2.9           33.1        140.   9.22\n 2   2             32.4        126.  28.7 \n 3 139             35.5         97   11.6 \n 4  15             50.2          3.3 30.8 \n 5   0.45          40.2          3.7  5.95\n 6   3.4           23.7        153.   7.75\n 7  14            126.         526    7.51\n 8   3.3           22.8         50.7  4.01\n 9 124             39.6        106   20.8 \n10   2.9           23.2        279    2.43\n# ℹ 21 more rows"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#renomeando-colunas",
    "href": "conteudo/manipulacao-dados-R/transform.html#renomeando-colunas",
    "title": "Transformação de Dados",
    "section": "5 Renomeando colunas",
    "text": "5 Renomeando colunas\n\nres |&gt;\n  rename(Fosforo_total = P.total,\n         Captura_kg = CPUE)\n\n# A tibble: 31 × 11\n   Reservatorio Bacia  Fechamento   Area Trofia    pH Condutividade Alcalinidade\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n 1 Cavernoso    Iguacu       1965   2.9  Oligo…   7.4          33.1        140. \n 2 Curucaca     Iguacu       1982   2    Oligo…   7            32.4        126. \n 3 Foz do Areia Iguacu       1980 139    Oligo…   7.3          35.5         97  \n 4 Irai         Iguacu       2000  15    EutrÃ…   6.9          50.2          3.3\n 5 JMF          Iguacu       1970   0.45 Mesot…   7.3          40.2          3.7\n 6 Jordao       Iguacu       1996   3.4  Oligo…   7.1          23.7        153. \n 7 Passauna     Iguacu       1978  14    Oligo…   8.8         126.         526  \n 8 Piraquara    Iguacu       1979   3.3  Oligo…   7.1          22.8         50.7\n 9 Salto Caxias Iguacu       1998 124    Oligo…   7.3          39.6        106  \n10 Salto do Vau Iguacu       1959   2.9  Oligo…   6.5          23.2        279  \n# ℹ 21 more rows\n# ℹ 3 more variables: Fosforo_total &lt;dbl&gt;, Riqueza &lt;dbl&gt;, Captura_kg &lt;dbl&gt;"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#agrupando-tabelas-funções-do-grupo-join",
    "href": "conteudo/manipulacao-dados-R/transform.html#agrupando-tabelas-funções-do-grupo-join",
    "title": "Transformação de Dados",
    "section": "6 Agrupando tabelas: funções do grupo join",
    "text": "6 Agrupando tabelas: funções do grupo join\nAs funções left_join(), right_join(), inner_join(), anti_join() e full_join() do pacote dplyr em R são utilizadas para combinar dois data frames baseados em uma coluna ou colunas comuns. Esses tipos de joins são amplamente utilizados em operações de banco de dados e manipulação de dados.\nConsidere os arquivos regiao.csv e habitat.csv do repositório datasets.\n\nregiao &lt;- read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regiao.csv\")\nhabitat &lt;- read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/habitat.csv\")\nregiao\n\n# A tibble: 10 × 4\n   Riacho Bacia      Município      Área\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n 1 R1     Boicucanga São Sebastião  30.3\n 2 R4     Boicucanga São Sebastião  30.3\n 3 R8     Boicucanga São Sebastião  30.3\n 4 R2     Cubatão    Cubatão       189  \n 5 R5     Cubatão    Cubatão       189  \n 6 R10    Cubatão    Cubatão       189  \n 7 R13    Cubatão    Cubatão       189  \n 8 R6     Quilombo   Santos         86  \n 9 R9     Quilombo   Santos         86  \n10 R7     Quilombo   Santos         86  \n\nhabitat\n\n# A tibble: 8 × 4\n  Riacho Altitude Largura Profundidade\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 R1           74     7.8         20.2\n2 R4           14    10.9         17.7\n3 R8          245     8.3         19.5\n4 R11         241     2.2         20.3\n5 R2           29     1.6         11.8\n6 R6           86    15.2         35.3\n7 R9           77     4.1         18.9\n8 R7           63    14.2         42.1\n\n\nTabela regiao: Contém informações sobre a bacia hidrográfica, área da bacia e município de alguns riachos da região litorânea de São Paulo.\nTabela habitat: Contém informações sobre a largura e profundidade desses riachos. Algumas entradas são comuns às duas tabelas, enquanto outras são exclusivas de uma delas. A coluna Riacho serve como chave para combinar as informações.\n\nFunção left_join()\nRetorna todas as linhas da tabela à esquerda (regiao) e adiciona colunas da tabela à direita (habitat). Linhas sem correspondência na tabela da direita terão valores de NA.\n\nregiao |&gt; left_join(y = habitat)\n\n# A tibble: 10 × 7\n   Riacho Bacia      Município      Área Altitude Largura Profundidade\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 R1     Boicucanga São Sebastião  30.3       74     7.8         20.2\n 2 R4     Boicucanga São Sebastião  30.3       14    10.9         17.7\n 3 R8     Boicucanga São Sebastião  30.3      245     8.3         19.5\n 4 R2     Cubatão    Cubatão       189         29     1.6         11.8\n 5 R5     Cubatão    Cubatão       189         NA    NA           NA  \n 6 R10    Cubatão    Cubatão       189         NA    NA           NA  \n 7 R13    Cubatão    Cubatão       189         NA    NA           NA  \n 8 R6     Quilombo   Santos         86         86    15.2         35.3\n 9 R9     Quilombo   Santos         86         77     4.1         18.9\n10 R7     Quilombo   Santos         86         63    14.2         42.1\n\n\n\n\nFunção right_join()\nRetorna todas as linhas da tabela à direita (habitat) e adiciona colunas da tabela à esquerda (regiao). Linhas sem correspondência na tabela da esquerda terão valores de NA.\n\nregiao |&gt; right_join(y = habitat, keep=TRUE)\n\n# A tibble: 8 × 8\n  Riacho.x Bacia      Município      Área Riacho.y Altitude Largura Profundidade\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 R1       Boicucanga São Sebastião  30.3 R1             74     7.8         20.2\n2 R4       Boicucanga São Sebastião  30.3 R4             14    10.9         17.7\n3 R8       Boicucanga São Sebastião  30.3 R8            245     8.3         19.5\n4 R2       Cubatão    Cubatão       189   R2             29     1.6         11.8\n5 R6       Quilombo   Santos         86   R6             86    15.2         35.3\n6 R9       Quilombo   Santos         86   R9             77     4.1         18.9\n7 R7       Quilombo   Santos         86   R7             63    14.2         42.1\n8 &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;           NA   R11           241     2.2         20.3\n\n\n\n\nFunção inner_join()\nRetorna apenas as linhas que têm correspondência em ambas as tabelas.\n\nregiao |&gt; inner_join(y = habitat)\n\n# A tibble: 7 × 7\n  Riacho Bacia      Município      Área Altitude Largura Profundidade\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 R1     Boicucanga São Sebastião  30.3       74     7.8         20.2\n2 R4     Boicucanga São Sebastião  30.3       14    10.9         17.7\n3 R8     Boicucanga São Sebastião  30.3      245     8.3         19.5\n4 R2     Cubatão    Cubatão       189         29     1.6         11.8\n5 R6     Quilombo   Santos         86         86    15.2         35.3\n6 R9     Quilombo   Santos         86         77     4.1         18.9\n7 R7     Quilombo   Santos         86         63    14.2         42.1\n\n\n\n\nFunção anti_join()\nRetorna as linhas da tabela à esquerda que não têm correspondência na tabela à direita. Também retorna todas as colunas da tabela à esquerda.\n\nregiao |&gt; anti_join(y = habitat)\n\n# A tibble: 3 × 4\n  Riacho Bacia   Município  Área\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 R5     Cubatão Cubatão     189\n2 R10    Cubatão Cubatão     189\n3 R13    Cubatão Cubatão     189\n\n\n\nhabitat |&gt; anti_join(y = regiao)\n\n# A tibble: 1 × 4\n  Riacho Altitude Largura Profundidade\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 R11         241     2.2         20.3\n\n\n\n\nFunção full_join()\nRetorna todas as linhas e colunas de ambas as tabelas. Nas células onde não houver correspondência, retorna NA.\n\nregiao |&gt; full_join(y = habitat, keep = TRUE)\n\n# A tibble: 11 × 8\n   Riacho.x Bacia      Município     Área Riacho.y Altitude Largura Profundidade\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 R1       Boicucanga São Sebasti…  30.3 R1             74     7.8         20.2\n 2 R4       Boicucanga São Sebasti…  30.3 R4             14    10.9         17.7\n 3 R8       Boicucanga São Sebasti…  30.3 R8            245     8.3         19.5\n 4 R2       Cubatão    Cubatão      189   R2             29     1.6         11.8\n 5 R5       Cubatão    Cubatão      189   &lt;NA&gt;           NA    NA           NA  \n 6 R10      Cubatão    Cubatão      189   &lt;NA&gt;           NA    NA           NA  \n 7 R13      Cubatão    Cubatão      189   &lt;NA&gt;           NA    NA           NA  \n 8 R6       Quilombo   Santos        86   R6             86    15.2         35.3\n 9 R9       Quilombo   Santos        86   R9             77     4.1         18.9\n10 R7       Quilombo   Santos        86   R7             63    14.2         42.1\n11 &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;          NA   R11           241     2.2         20.3"
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#criando-e-modificando-colunas-com-mutate",
    "href": "conteudo/manipulacao-dados-R/transform.html#criando-e-modificando-colunas-com-mutate",
    "title": "Transformação de Dados",
    "section": "7 Criando e modificando colunas com mutate()",
    "text": "7 Criando e modificando colunas com mutate()\nA função mutate() permite criar e modificar colunas em um data frame. Usando a base de dados Doubs river:\n\nlibrary(ade4)\ndata(doubs)\ndbenv &lt;- doubs$env\nhead(dbenv)\n\n  dfs alt   slo flo pH har pho nit amm oxy bdo\n1   3 934 6.176  84 79  45   1  20   0 122  27\n2  22 932 3.434 100 80  40   2  20  10 103  19\n3 102 914 3.638 180 83  52   5  22   5 105  35\n4 185 854 3.497 253 80  72  10  21   0 110  13\n5 215 849 3.178 264 81  84  38  52  20  80  62\n6 324 846 3.497 286 79  60  20  15   0 102  53\n\n\n\nAjustando a escala de pH\nA coluna pH está multiplicada por \\(10\\). Vamos ajustar isso:\n\ndbenv &lt;- dbenv  |&gt; \n  mutate(pH = pH / 10)\n\nhead(dbenv)\n\n  dfs alt   slo flo  pH har pho nit amm oxy bdo\n1   3 934 6.176  84 7.9  45   1  20   0 122  27\n2  22 932 3.434 100 8.0  40   2  20  10 103  19\n3 102 914 3.638 180 8.3  52   5  22   5 105  35\n4 185 854 3.497 253 8.0  72  10  21   0 110  13\n5 215 849 3.178 264 8.1  84  38  52  20  80  62\n6 324 846 3.497 286 7.9  60  20  15   0 102  53\n\n\n\n\nCriando variável categórica\nCriar uma variável categórica pH_cat com níveis Elevado (maior ou igual a \\(8\\)) e Neutro (menor que \\(8\\)):\n\ndbenv &lt;- dbenv |&gt; \n  mutate(pH = pH / 10) |&gt; \n  mutate(pH_cat = if_else(pH &lt; 8, true = \"Neutro\", false = \"Elevado\"),\n         , .after = pH)\n\nhead(dbenv)\n\n  dfs alt   slo flo   pH pH_cat har pho nit amm oxy bdo\n1   3 934 6.176  84 0.79 Neutro  45   1  20   0 122  27\n2  22 932 3.434 100 0.80 Neutro  40   2  20  10 103  19\n3 102 914 3.638 180 0.83 Neutro  52   5  22   5 105  35\n4 185 854 3.497 253 0.80 Neutro  72  10  21   0 110  13\n5 215 849 3.178 264 0.81 Neutro  84  38  52  20  80  62\n6 324 846 3.497 286 0.79 Neutro  60  20  15   0 102  53\n\n\n\n\nUnindo colunas com unite()\nA função unite() do tidyr combina duas colunas em uma nova coluna. Usando a tabela iris:\n\niris2 &lt;- iris |&gt; \n  mutate(Genus = \"Iris\", .before = Species)  |&gt;  \n  unite(scientic_name, Genus, Species, sep = \" \")\n\nhead(iris2)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width scientic_name\n1          5.1         3.5          1.4         0.2   Iris setosa\n2          4.9         3.0          1.4         0.2   Iris setosa\n3          4.7         3.2          1.3         0.2   Iris setosa\n4          4.6         3.1          1.5         0.2   Iris setosa\n5          5.0         3.6          1.4         0.2   Iris setosa\n6          5.4         3.9          1.7         0.4   Iris setosa\n\n\n\n\n\n\n\n\nObservação\n\n\n\nA função unite() excluiu as colunas que foram unificadas da tabela. Mara mantê-las na tabela utilize o argumento remove = FALSE."
  },
  {
    "objectID": "conteudo/manipulacao-dados-R/transform.html#reformatando-data-frames-funções-pivot_wider-e-pivot_longer",
    "href": "conteudo/manipulacao-dados-R/transform.html#reformatando-data-frames-funções-pivot_wider-e-pivot_longer",
    "title": "Transformação de Dados",
    "section": "8 Reformatando data frames: funções pivot_wider() e pivot_longer()",
    "text": "8 Reformatando data frames: funções pivot_wider() e pivot_longer()\nA tabela HubbardBrook.csv (datasets) contém dados anuais de vazão e precipitação em dois bacias hidrográficas (Hornbeck et al. 1993). A primeira (Deforested) teve toda a vegetação removida como parte de um experimento de longa duração enquanto a outra se manteve intacta (Referenca). Os daods de origem e o experimento detalhado são apresentados am Os dados foram retirados de tiee.esa.org\n\nhbrook &lt;- read_csv(\"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/HubbardBrook.csv\")\nhbrook\n\n# A tibble: 62 × 4\n    Year Treatment   Flow Precipitation\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n 1  1958 Deforested  645.         1168.\n 2  1959 Deforested 1012.         1483.\n 3  1960 Deforested  825.         1321.\n 4  1961 Deforested  470.          980.\n 5  1962 Deforested  777.         1232.\n 6  1963 Deforested  774.         1139.\n 7  1964 Deforested  712.         1175.\n 8  1965 Deforested  599.         1115.\n 9  1966 Deforested 1189.         1222.\n10  1967 Deforested 1132.         1315.\n# ℹ 52 more rows\n\n\n\nReorganizando data frames de formato longo para formato largo\nA função pivot_wider() é utilizada para transformar dados do formato longo para o formato largo. A seguir, será feito isso apenas para a variável Flow, excluindo Precipitation, separando os dados nas colunas Deforested e Reference.\n\nhbrook_largo &lt;- hbrook |&gt;\n  select(-Precipitation) |&gt;\n  pivot_wider(names_from = Treatment, values_from = Flow)\nhbrook_largo\n\n# A tibble: 31 × 3\n    Year Deforested Reference\n   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  1958       645.      567.\n 2  1959      1012.      918.\n 3  1960       825.      752.\n 4  1961       470.      436.\n 5  1962       777.      699.\n 6  1963       774.      663.\n 7  1964       712.      630.\n 8  1965       599.      547.\n 9  1966      1189.      727.\n10  1967      1132.      781.\n# ℹ 21 more rows\n\n\n\n\nReorganizando data frames de formato largo para formato longo\nA função pivot_longer() é utilizada para transformar dados do formato largo para o formato longo, fazendo o caminho inverso de pivot_wider().\n\nhbrook_longo &lt;- hbrook_largo |&gt;\n  pivot_longer(!Year, names_to = \"Desmatamento\", values_to = \"Flow\")\nhbrook_longo\n\n# A tibble: 62 × 3\n    Year Desmatamento  Flow\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1  1958 Deforested    645.\n 2  1958 Reference     567.\n 3  1959 Deforested   1012.\n 4  1959 Reference     918.\n 5  1960 Deforested    825.\n 6  1960 Reference     752.\n 7  1961 Deforested    470.\n 8  1961 Reference     436.\n 9  1962 Deforested    777.\n10  1962 Reference     699.\n# ℹ 52 more rows"
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html",
    "href": "conteudo/introducao_r/estrutura-linguagem.html",
    "title": "Estrutura da linguagem",
    "section": "",
    "text": "R é um ambiente de software livre para computação estatística e gráfica que roda em uma variedade de plataformas UNIX, Windows e MacOS (R Project). A instalação pode ser feita a partir do site oficial CRAN, seguindo as instruções para cada plataforma. O RStudio é uma Interface de Desenvolvimento Integrado (IDE) dedicada ao ambiente R, embora existam outras opções como Jupyter Notebook, Jupyter Lab, Visual Studio Code e Google Colab."
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html#o-r-para-cálculos-aritméticos",
    "href": "conteudo/introducao_r/estrutura-linguagem.html#o-r-para-cálculos-aritméticos",
    "title": "Estrutura da linguagem",
    "section": "1 O R para cálculos aritméticos",
    "text": "1 O R para cálculos aritméticos\nVamos iniciar nossa introdução ao R com seu uso mais simples, um ambiente para cálculos aritméticos. Como você verá, o R usa os operadores matemáticos de subtração (-), adição (+), multiplicação (*), divisão (/) e potenciação (^) do modo análogo a outros softwares.\n\n2 + 4\n\n[1] 6\n\n2 * 4\n\n[1] 8\n\n2 - 4\n\n[1] -2\n\n2^4\n\n[1] 16\n\n\nAlém destes, temos operadores para extrairmos a parte inteira (%%) e o resto (%/%) de uma divisão.\n\n13%/%2\n\n[1] 6\n\n13%%2\n\n[1] 1\n\n\nO uso de parênteses também permite o controle das operações matemáticas seguindo as prioridades conhecidas nestas operações. Por exemplo, a expressão:\n\n5 * (9 + 2)\n\n[1] 55\n\n\né diferente de:\n\n5 * 9 + 2\n\n[1] 47\n\n\nAssim como a expressão:\n\n(3 + 4)^2\n\n[1] 49\n\n\né diferente de:\n\n3 + 4^2\n\n[1] 19\n\n\nExistem também funções aritméticas comuns como \\(log(x)\\), \\(\\sqrt(x)\\), \\(\\sin(x)\\), o número \\(\\pi\\), etc.\n\nlog(100)\n\n[1] 4.60517\n\nlog10(100)\n\n[1] 2\n\nlog(100, base = 2)\n\n[1] 6.643856\n\nsqrt(36)\n\n[1] 6\n\npi\n\n[1] 3.141593\n\nsin(0.5 * pi)\n\n[1] 1"
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html#atribuição-de-valores",
    "href": "conteudo/introducao_r/estrutura-linguagem.html#atribuição-de-valores",
    "title": "Estrutura da linguagem",
    "section": "2 Atribuição de valores",
    "text": "2 Atribuição de valores\nO R se estrutura por meio de objetos. Ao atribuir um valor a uma variável, esta se torna um objeto que fica disponível na memória. Para atribuir valor \\(2\\) à variável x fazemos:\n\nx &lt;- 2\nx\n\n[1] 2\n\n\nApós atribuir um valor à variável, esta fica disponível na memória da seção atual e pode ser utilizada em operações subsequentes.\n\ny &lt;- x + 10\ny\n\n[1] 12\n\n\nAo atribuir outro valor à mesma variável, o valor inicial é substituído:\n\nx &lt;- 5\ny &lt;- x + 10\ny\n\n[1] 15\n\n\nO R diferencia caracteres minúsculos de MAIÚSCULOS. Portanto:\n\na &lt;- sqrt(49)\nA &lt;- sqrt(81)\na\n\n[1] 7\n\nA\n\n[1] 9"
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html#estruturas-de-dados",
    "href": "conteudo/introducao_r/estrutura-linguagem.html#estruturas-de-dados",
    "title": "Estrutura da linguagem",
    "section": "3 Estruturas de dados",
    "text": "3 Estruturas de dados\nOs objetos em R podem ser vetores (numéricos, alfanuméricos ou fatores), matrizes (numéricas ou alfanumétricas), data frames (estrutura bidimensional que pode combinar colunas de diferentes tipos como vetores numéricos, alfanuméricos ou fatores) ou listas (que pode combinar em sua estrutura, todos os objetos descritos acima). As funções em R são sequências de comandos que podem transformar objetos.\n\n3.1 Vetores numéricos\nOs objetos podem guardar mais de um único valor. A função concatenar c() pode ser utilizada para criar um vetor com múltiplos valores. Dizemos que cada valor individual é uma entrada.\n\nx &lt;- c(4, 3.0, 5, 9, 10)\nx\n\n[1]  4  3  5  9 10\n\n\nPodemos utilizar estes vetores em nossas operações.\n\ny &lt;- x * 2\ny\n\n[1]  8  6 10 18 20\n\n\nNote que na operação acima, cada entrada foi multiplicada por \\(2\\).\nPodemos ainda acessar e modificar entradas individuais. Por exemplo, o objeto y criado acima tem 5 elementos. O segundo elemento pode ser acessado com o comando:\n\ny[2]\n\n[1] 6\n\n\nE alterado com o comando:\n\ny[2] &lt;- 300\ny\n\n[1]   8 300  10  18  20\n\n\nSe quisermos excluir o quarto elemento de y e gravar o resultado em um novo objeto z fazemos:\n\nz &lt;- y[-4]\nz\n\n[1]   8 300  10  20\n\n\nObs: Veja que o quarto elemento, 18, foi excluído.\nPodemos obter a informação sobre o número de elementro do vetor. O vetor y tem tamanho igual a 5, enquanto o vetor z tem 4 elementos.\n\nlength(y)\n\n[1] 5\n\nlength(z)\n\n[1] 4\n\n\n\nSequências regulares e repetições\nPodemos criar sequencias regulares.\n\n2:10\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(2, 10, by = 2)\n\n[1]  2  4  6  8 10\n\nseq(2, 10, length = 4)\n\n[1]  2.000000  4.666667  7.333333 10.000000\n\nseq(2, 10, length = 10)\n\n [1]  2.000000  2.888889  3.777778  4.666667  5.555556  6.444444  7.333333\n [8]  8.222222  9.111111 10.000000\n\n\nE repetições de valores e vetores.\n\nrep(4, times = 6)\n\n[1] 4 4 4 4 4 4\n\nrep(c(2, 5), times = 3)\n\n[1] 2 5 2 5 2 5\n\nrep(c(2, 5), each = 3)\n\n[1] 2 2 2 5 5 5\n\n\nOs resultados destas sequências podem ser guardadas em um objeto para utilização subsequente.\n\na &lt;- seq(2, 10, by = 2)\na\n\n[1]  2  4  6  8 10\n\nb &lt;- seq(10, 2, by = -2)\nb\n\n[1] 10  8  6  4  2\n\nc &lt;- a + b\nc\n\n[1] 12 12 12 12 12\n\n\n\n\n\n3.2 Vetores alfanuméricos\nSão vetores em que cada entrada é um caracter alfanumerico.\n\nespecie = c(\"Deuterodon iguape\", \n            \"Characidium japuhybense\", \n            \"Trichomycterus zonatus\")\nespecie\n\n[1] \"Deuterodon iguape\"       \"Characidium japuhybense\"\n[3] \"Trichomycterus zonatus\" \n\n\nExiste uma variedade de funções para manipulação de vetores alfanuméricos.\nA função sort() por exemplo, aplicada a um vetor numérico é utilizada para ordená-lo de forma crescente:\n\na = c(5,2,15,12)\na\n\n[1]  5  2 15 12\n\nsort(a)\n\n[1]  2  5 12 15\n\n\nou decrescente:\n\nsort(a, decreasing = T)\n\n[1] 15 12  5  2\n\n\nSe aplicada a um vetor alfanumerico esta função ordena o vetor em ordem alfabética:\n\nsort(especie, decreasing = FALSE)\n\n[1] \"Characidium japuhybense\" \"Deuterodon iguape\"      \n[3] \"Trichomycterus zonatus\" \n\nsort(especie, decreasing = TRUE)\n\n[1] \"Trichomycterus zonatus\"  \"Deuterodon iguape\"      \n[3] \"Characidium japuhybense\"\n\n\n\n\n3.3 Unindo vetores: comando paste\nSuponha que desejamos unir dois vetores alfanuméricos\n\nx1 &lt;- c(\"Experimento\")\nx2 &lt;- c(\"A\", \"B\", \"C\")\nx3 &lt;- paste(x1, x2, sep = \"_\")\n\nO mesmo resultado pode ser obtido de forma mais concisa com o comando:\n\nx4 &lt;- paste(\"Experimento\", LETTERS[1:3], sep = \"_\")\nx4\n\n[1] \"Experimento_A\" \"Experimento_B\" \"Experimento_C\"\n\n\n\n\n3.4 Fatores\nFatores são como vetores alfanuméricos, porém com um atributo adicional. Fatores são compostos por diferentes níveis. Por exemplo, podemos criar o objeto dosagem com o comando:\n\ndosagem &lt;- c(\"Alta\", \"Alta\", \"Alta\", \n            \"Media\", \"Media\", \"Media\", \n            \"Baixa\", \"Baixa\", \"Baixa\")\ndosagem\n\n[1] \"Alta\"  \"Alta\"  \"Alta\"  \"Media\" \"Media\" \"Media\" \"Baixa\" \"Baixa\" \"Baixa\"\n\n\nNo exemplo acima, o R não reconhece as palavras Alta, Media e Baixa como diferentes níveis. Para isto devemos transformá-lo em um fator:\n\ndosagem &lt;- factor(dosagem)\ndosagem\n\n[1] Alta  Alta  Alta  Media Media Media Baixa Baixa Baixa\nLevels: Alta Baixa Media\n\n\nO objeto dosagem agora é um fator com 3 níveis.\n\nlevels(dosagem)\n\n[1] \"Alta\"  \"Baixa\" \"Media\"\n\nnlevels(dosagem)\n\n[1] 3\n\nlevels(dosagem)[2]\n\n[1] \"Baixa\"\n\n\nNote entretanto que os níveis foram reconhecidos em ordem alfabética. Se quisermos ordenar este níveis de outro modo fazemos:\n\ndosagem &lt;- factor(dosagem, ordered = TRUE, \n                 levels = c(\"Baixa\", \"Media\", \"Alta\"))\ndosagem\n\n[1] Alta  Alta  Alta  Media Media Media Baixa Baixa Baixa\nLevels: Baixa &lt; Media &lt; Alta\n\n\nComo veremos a frente, esta operação facilita a visualização gráfica de fatores ordenados.\n\n\n3.5 Matrizes\nMatrizes são objetos compostos por linhas e colunas. No R, uma matriz pode ser construída inicialmente criando um vetor numérico:\n\na &lt;- c(21,26,5,18,17,28,20,15,13,14,27,22)\na\n\n [1] 21 26  5 18 17 28 20 15 13 14 27 22\n\n\nEm seguida o vetor pode ser organizado em uma matriz definindo-se o número de linhas e de colunas que sejam compatíveis com o tamanho do vetor. No exemplo acima o vetor tem comprimento 12 e pode ser organizado em uma matriz de \\(3\\) linhas e \\(4\\) colunas:\n\nx &lt;- matrix(a, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   18   20   14\n[2,]   26   17   15   27\n[3,]    5   28   13   22\n\n\nNote que os elementos foram adicionados um por vez de coluna em coluna. Se quisermos preencher a matriz por linhas adicionamos ao comando, o argumento byrow = TRUE.\n\nx &lt;- matrix(a, nrow = 3, ncol = 4, byrow = TRUE)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26    5   18\n[2,]   17   28   20   15\n[3,]   13   14   27   22\n\n\nOs elementos de uma matriz podem ser acessados indicando sua posição na linha e na coluna. Por exemplo, o elemento da \\(2^a\\) linha e \\(3^a\\) coluna de x pode ser acessados pelo comando:\n\nx[2, 3]\n\n[1] 20\n\n\nDe modo análogo, a \\(2^a\\) linha pode ser acessada por:\n\nx[2, ]\n\n[1] 17 28 20 15\n\n\nE a \\(4^a\\) coluna por:\n\nx[, 4]\n\n[1] 18 15 22\n\n\nValores individuais em matrizes podem ser alterados de forma similar ao que é realizasdo em vetores. Por exemplo, para alterar o elemento da \\(2^a\\) linha e \\(3^a\\) coluna de x por \\(1000\\) fazemos:\n\nx[2, 3] &lt;- 1000\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26    5   18\n[2,]   17   28 1000   15\n[3,]   13   14   27   22\n\n\nTambém podemos excluir linhas e colunas de uma matriz.\n\nx[-2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26    5   18\n[2,]   13   14   27   22\n\nx[,-3]\n\n     [,1] [,2] [,3]\n[1,]   21   26   18\n[2,]   17   28   15\n[3,]   13   14   22\n\n\nNote que, acima, não salvamos os resultados da exclusão das linhas e colunas de x em nenhum objeto, de modo que x continua inalterado.\n\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26    5   18\n[2,]   17   28 1000   15\n[3,]   13   14   27   22\n\n\nPodemos criar matrizes unindo vetores de tamanho iguais em linhas ou colunas.\n\nx &lt;- 3:12\ny &lt;- 12:3\nrbind(x, y)\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\nx    3    4    5    6    7    8    9   10   11    12\ny   12   11   10    9    8    7    6    5    4     3\n\ncbind(x, y)\n\n       x  y\n [1,]  3 12\n [2,]  4 11\n [3,]  5 10\n [4,]  6  9\n [5,]  7  8\n [6,]  8  7\n [7,]  9  6\n [8,] 10  5\n [9,] 11  4\n[10,] 12  3\n\n\nEventualmente, se desejarmos atribuir nomes às linhas e às colunas de uma matriz, podemos fazê-lo por meio das funções rownames() e colnames() respectivamente:\n\nx_mat &lt;- matrix(1:12, nrow = 3, ncol = 4)\nx_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nrownames(x_mat) &lt;- paste(\"Linha\", 1:3, sep = \"\")\nx_mat\n\n       [,1] [,2] [,3] [,4]\nLinha1    1    4    7   10\nLinha2    2    5    8   11\nLinha3    3    6    9   12\n\ncolnames(x_mat) &lt;- paste(\"Coluna\", 1:4, sep = \"\")\nx_mat\n\n       Coluna1 Coluna2 Coluna3 Coluna4\nLinha1       1       4       7      10\nLinha2       2       5       8      11\nLinha3       3       6       9      12\n\n\n\n\n3.6 Data frames\nAssim como Matrizes, Data frames são estruturas que permitem organizar dados em formato de linhas e colunas. No R entanto, as Matrizes não podem guardar objetos de diferentes características. Por exemplo, uma matriz pode ser composta inteiramente numérica:\n\nmatrix(1:12, nrow = 4, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nOu alfanumérica:\n\nmatrix(letters[1:12], nrow = 4, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"e\"  \"i\" \n[2,] \"b\"  \"f\"  \"j\" \n[3,] \"c\"  \"g\"  \"k\" \n[4,] \"d\"  \"h\"  \"l\" \n\n\nPorém, se tentarmos unir um vetor numérico a um vetor alfanumérico, toda a matriz será convertida no formato alfanumérico.\n\nz &lt;- LETTERS[3:12]\nz\n\n [1] \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\"\n\ncbind(x, z)\n\n      x    z  \n [1,] \"3\"  \"C\"\n [2,] \"4\"  \"D\"\n [3,] \"5\"  \"E\"\n [4,] \"6\"  \"F\"\n [5,] \"7\"  \"G\"\n [6,] \"8\"  \"H\"\n [7,] \"9\"  \"I\"\n [8,] \"10\" \"J\"\n [9,] \"11\" \"K\"\n[10,] \"12\" \"L\"\n\n\nPara unir diferentes tipos de vetores devemos usar transformar a matriaz para um objeto do tipo data.frame que cria uma estrutura com colunas independentes, permitindo que estas tenham diferentes formatos. Podemos unir os objetos x e z acima em um data frame como segue:\n\ndata.frame(x, z)\n\n    x z\n1   3 C\n2   4 D\n3   5 E\n4   6 F\n5   7 G\n6   8 H\n7   9 I\n8  10 J\n9  11 K\n10 12 L\n\n\nNote que automaticamente, a função atribui nomes as colunas (x e z) e às linhas (\\(1\\) a 10). Estes nomes podem ser alterados com as funções rownames() e colnames().\nNeste caso, a coluna x continua sendo numérica e a coluna z continua alfanumérica.\nPodemos acessar os elementos de um data frame do mesmo modo que fizemos para matrizes.\nPodemos criar um data frame diretamente:\n\nDados &lt;- data.frame(Regiao = factor(c(\"Santos\", \"Santos\", \n                                     \"Bertioga\", \"Bertioga\", \n                                     \"Peruibe\", \"Peruibe\")),\n                   Especie_A = c(12,43,80,91,75,115), \n                   Especie_B = c(0, 59, 300, 350, 154, 200))\n\nE acessá-lo de diferentes formas:\n\nDados\n\n    Regiao Especie_A Especie_B\n1   Santos        12         0\n2   Santos        43        59\n3 Bertioga        80       300\n4 Bertioga        91       350\n5  Peruibe        75       154\n6  Peruibe       115       200\n\nDados$Regiao\n\n[1] Santos   Santos   Bertioga Bertioga Peruibe  Peruibe \nLevels: Bertioga Peruibe Santos\n\nDados[\"Regiao\"]\n\n    Regiao\n1   Santos\n2   Santos\n3 Bertioga\n4 Bertioga\n5  Peruibe\n6  Peruibe\n\nDados[,\"Regiao\"]\n\n[1] Santos   Santos   Bertioga Bertioga Peruibe  Peruibe \nLevels: Bertioga Peruibe Santos\n\nDados[,c(\"Especie_A\",\"Especie_B\")]\n\n  Especie_A Especie_B\n1        12         0\n2        43        59\n3        80       300\n4        91       350\n5        75       154\n6       115       200\n\n\n\n\n3.7 Listas\nCombinam em um único objeto todas as estruturas anteriores. Veja o exemplo em que combinamos um vetor alfanumérico, um vetor nominal e um data frame dentro da mesma lista.\n\nnossalista &lt;- list(Ilha = c(\"Ilhabela\", \"Anchieta\", \"Cardoso\"), \n                  Areaskm2 = c(347.5, 8.3, 131), \n                  Localizacao = data.frame(\n                    Bioma = rep(\"Mata Atlantica\", 3),\n                  Lat = c(23, 25, 23),\n                  Long = c(45, 47, 45)))\nnossalista\n\n$Ilha\n[1] \"Ilhabela\" \"Anchieta\" \"Cardoso\" \n\n$Areaskm2\n[1] 347.5   8.3 131.0\n\n$Localizacao\n           Bioma Lat Long\n1 Mata Atlantica  23   45\n2 Mata Atlantica  25   47\n3 Mata Atlantica  23   45\n\n\nPodemos ainda inserir listas dentro de outras listas, criando estruturas altamente complexas.\nPara acessar os elementos de uma lista podemos identificar seu nome após o operador $ ou sua posição das formas que se seguem:\n\nnossalista$Ilha\n\n[1] \"Ilhabela\" \"Anchieta\" \"Cardoso\" \n\nnossalista[[1]]\n\n[1] \"Ilhabela\" \"Anchieta\" \"Cardoso\" \n\nnossalista$Localizacao\n\n           Bioma Lat Long\n1 Mata Atlantica  23   45\n2 Mata Atlantica  25   47\n3 Mata Atlantica  23   45\n\nnossalista[[3]]\n\n           Bioma Lat Long\n1 Mata Atlantica  23   45\n2 Mata Atlantica  25   47\n3 Mata Atlantica  23   45"
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html#operadores-relacionais",
    "href": "conteudo/introducao_r/estrutura-linguagem.html#operadores-relacionais",
    "title": "Estrutura da linguagem",
    "section": "4 Operadores relacionais",
    "text": "4 Operadores relacionais\nOperadores relacionais são aqueles de verificam as relações de menor que (&lt;), maior que (&gt;), menor ou igual (&lt;=), maior ou igual (&gt;=), igual a (==) ou diferente de (!=). O resultado de uma comparação retorna um objeto com o argumento verdadeiro (TRUE) ou falso (FALSE). Veja por exemplo:\n\n3 &gt; 5\n\n[1] FALSE\n\n\n\n3 &gt; 3\n\n[1] FALSE\n\n\n\n3 &gt;= 3\n\n[1] TRUE\n\n\n\na &lt;- 5\nb &lt;- 7\na == b\n\n[1] FALSE\n\na != b\n\n[1] TRUE\n\n\nSe os objetos têm mais de um elemento, no caso de vetores, matrizes ou data frames, a comparação é feita elemento-a-elemento, comparando aqueles que estão na mesma posição, ou seja, os que têm o mesmo índice de posição.\n\na &lt;- c(3,5,5,7,1)\nb &lt;- c(3,6,1,9,-3)\na &lt; b\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n\nOs operadores TRUE e FALSE, quanto utilizados em operações aritméticas se comportam respectivamente como valores 1 e 0.\n\na &lt;- 5\nb &lt;- c(3,6,1,9,-3)\ny &lt;- b &gt; a\ny\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n\nSomando os elementos de y temos o número de elementos que atendendem à condição acima:\n\nsum(y)\n\n[1] 2\n\n\nE se tirarmos a média aritmética, teremos a proporção de 1’s no vetor y.\n\nmean(y)\n\n[1] 0.4\n\n\n\n\n\n\n\n\nNota\n\n\n\nLembre-se que ao compararmos vetores de tamanhos distintos, o R não retorna um erro, mas recicla os elementos do vetor menor para compensar elementos faltantes."
  },
  {
    "objectID": "conteudo/introducao_r/estrutura-linguagem.html#operadores-lógicos",
    "href": "conteudo/introducao_r/estrutura-linguagem.html#operadores-lógicos",
    "title": "Estrutura da linguagem",
    "section": "5 Operadores lógicos",
    "text": "5 Operadores lógicos\nOperadores lógicos são os de NEGAÇÃO (!), E lógico, OU lógico versão vetorizada (|) e OU exclusivo (xor()). Exemplos destes operadores são:\n\nx &lt;- 3:5\ny &lt;- 5:3\n\n\n(x &lt; 4)\n\n[1]  TRUE FALSE FALSE\n\n!(x &lt; 4)\n\n[1] FALSE  TRUE  TRUE\n\n\n\n(x &lt; 4) & (y &gt; 4)\n\n[1]  TRUE FALSE FALSE\n\n\n\n(x &lt; 4) | (y &gt; 4)\n\n[1]  TRUE FALSE FALSE\n\n\n\nxor(x,y)\n\n[1] FALSE FALSE FALSE"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html",
    "href": "conteudo/introducao_r/graficos-r.html",
    "title": "(Básico da) Visualização gráfica",
    "section": "",
    "text": "A visualização gráfica consiste em representar visualmente os padrões de distribuição de uma variável ou a associação entre duas ou mais variáveis. Os tipos de gráficos utilizados dependem do tipo de variável (categórica ou numérica) e do número de variáveis envolvidas. Temos gráficos univariados para uma única variável, gráficos bivariados para associação entre duas variáveis e gráficos multivariados para mais de duas variáveis.\nAs funções gráficas discutidas nesta seção estão disponíveis no pacote graphics, que vem instalado por padrão no R, não sendo necessário instalar pacotes adicionais. Essas funções oferecem elevado controle sobre elementos gráficos (fontes, tamanhos, cores), mas podem ser complexas para criar figuras elaboradas. Apesar de muitas nomenclaturas serem compatíveis para o controle de eixos, títulos e tamanhos de fonte, os argumentos nem sempre são coesos entre os diferentes tipos de gráficos, o que pode dificultar o aprendizado. No entanto, essas funções fornecem uma base sólida sobre a estrutura gráfica no R, permitindo resolver rapidamente muitas situações do dia a dia da análise exploratória."
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#doubs-river-dataset",
    "href": "conteudo/introducao_r/graficos-r.html#doubs-river-dataset",
    "title": "(Básico da) Visualização gráfica",
    "section": "1 Doubs river dataset",
    "text": "1 Doubs river dataset\nPara demonstrar algumas ferramentas gráficas, será utilizado o conjunto de dados Doubs River data, disponível no pacote ade4 (Dray, Dufour, e Thioulouse 2015). Esse conjunto de dados foi apresentado na seção anterior sobre manipulação de data frames, onde foi importado o arquivo dbenv.csv. Agora, será usado o conjunto de dados completo.\nO conjunto de dados do Rio Doubs (Verneaux 1973) consiste de amostras sequenciais da cabeceira à foz do rio, em condições que variam de águas bem oxigenadas e oligotróficas a águas eutróficas e desprovidas de oxigênio. O conjunto de dados é uma lista com quatro data frames:\n\n$env: data frame com variáveis ambientais relacionadas à hidrologia, geomorfologia e química do\n$fish: data frame com abundâncias das espécies de peixes capturadas nos locais de amostragem.\n$xy: data frame com coordenadas geográficas de cada ponto de amostragem.\n$species: data frame com os nomes científicos, populares em francês e inglês, e códigos abreviados das espécies capturadas.\n\n\n1.1 Instalando o pacote ade4 e carregando os dados\n\nInstale o pacote ade4:\n\n\ninstall.packages(\"ade4\")\n\n\nCarregue o pacote:\n\n\nlibrary(ade4)\n\n\nHabilite o conjunto de dados doubs\n\n\ndata(doubs)\n\n\nConfira se consiste de uma lista:\n\n\nclass(doubs)\nstr(doubs)\n\n\nLeia a descrição do conjunto de dados para conchecê-lo melhor.\n\n\n?doubs\n\n\nExtraia os dados ambientais para um novo data.frame:\n\n\nambiente &lt;- doubs$env\n\n\nAdicione a este data frame uma nova variável categórica denominada secao com quatro níveis (Borcard, Gillet, e Legendre 2018).\n\n\nambiente$secao &lt;- c(rep(\"Seção 1\", 16), rep(\"Seção 4\", 14))\nambiente$secao[c(5,9,17)] &lt;- \"Seção 2\"\nambiente$secao[23:25] &lt;- \"Seção 3\"\nambiente$secao &lt;- factor(ambiente$secao)\n\n\nAdicione outra variável categórica, indicando três níveis de saturação de oxigênio em cada ponto.\n\n\nambiente$saturacao &lt;- cut(ambiente$oxy, breaks = c(0, 80, 109, 124), \n           labels = c(\"Pobre\", \"Médio\", \"Saturado\"))\nhead(ambiente, 10)\n\n   dfs alt   slo  flo pH har pho nit amm oxy bdo   secao saturacao\n1    3 934 6.176   84 79  45   1  20   0 122  27 Seção 1  Saturado\n2   22 932 3.434  100 80  40   2  20  10 103  19 Seção 1     Médio\n3  102 914 3.638  180 83  52   5  22   5 105  35 Seção 1     Médio\n4  185 854 3.497  253 80  72  10  21   0 110  13 Seção 1  Saturado\n5  215 849 3.178  264 81  84  38  52  20  80  62 Seção 2     Pobre\n6  324 846 3.497  286 79  60  20  15   0 102  53 Seção 1     Médio\n7  268 841 4.205  400 81  88   7  15   0 111  22 Seção 1  Saturado\n8  491 792 3.258  130 81  94  20  41  12  70  81 Seção 1     Pobre\n9  705 752 2.565  480 80  90  30  82  12  72  52 Seção 2     Pobre\n10 990 617 4.605 1000 77  82   6  75   1 100  43 Seção 1     Médio"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#descrevendo-os-padrões-de-uma-variável",
    "href": "conteudo/introducao_r/graficos-r.html#descrevendo-os-padrões-de-uma-variável",
    "title": "(Básico da) Visualização gráfica",
    "section": "2 Descrevendo os padrões de uma variável",
    "text": "2 Descrevendo os padrões de uma variável\n\n2.1 Gráfico de barras\nUm gráfico de barras é utilizado para verificar a contagem de cada nível de uma variável categórica. Faça um gráfico de barras para a variável saturacao.\nInicialmente, monte uma tabela de frequencia:\n\ntab1 &lt;- table(ambiente$saturacao)\ntab1\n\n\n   Pobre    Médio Saturado \n       8       14        8 \n\n\nEm seguida represente-a em um gráfico de barras:\n\nbarplot(tab1)\n\n\n\n\n\n\n\n\nAdicionando elementos de formatação gráfica:\n\nbarplot(tab1,\n        main = \"Concentração de oxigênio\",\n        ylab = \"Frequência\",\n        ylim = c(0, 18), col = \"black\")\nbox()\n\n\n\n\n\n\n\n\n\n\n2.2 Histograma\nUm histograma descreve o padrão de distribuição de uma variável quantitativa a partir da divisão desta variável em intervalos de classe.\nO histograma abaixo para a coluna oxy expressa a saturação de oxigênio (mg/l \\(\\times\\) 10).\n\nhist(ambiente$oxy)\n\n\n\n\n\n\n\n\n\n\nNo histograma, o intervalo de classes determina o formato exato do gráfico. No exemplo acima, a escolha foi feita automaticamente. No entanto, é possível definir o intervalo desejado com o argumento breaks:\n\nclasses &lt;- seq(40, 140, by = 20)\nhist(ambiente$oxy, breaks = classes)\n\n\n\n\n\n\n\n\nA divisão foi feita em intervalos de tamanho 20, iniciando em 40 e terminando em 140. A escolha deve ser a que evidencie da melhor forma possível o padrão de distribuição da variável.\n\n\n2.3 Boxplot\nBoxplots oferecem um resumo gráfico da distribuição de uma variável quantitativa. Abaixo está representada a variável oxy.\n\nboxplot(ambiente$oxy)\n\n\n\n\n\n\n\n\nA linha do meio representa a mediana da variável, enquanto os limites das caixas representam o \\(1^o\\) e \\(3^o\\) quartis e as linhas externas representam os pontos mínimo e máximo. Estes limites podem ser obtidos com o comando:\n\nquantile(ambiente$oxy, probs = c(0, 0.25, 0.5, 0.75, 1))\n\n    0%    25%    50%    75%   100% \n 41.00  80.25 102.00 109.00 124.00"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#associação-entre-duas-variáveis",
    "href": "conteudo/introducao_r/graficos-r.html#associação-entre-duas-variáveis",
    "title": "(Básico da) Visualização gráfica",
    "section": "3 Associação entre duas variáveis",
    "text": "3 Associação entre duas variáveis\n\n3.1 Gráfico de barras\nUm gráfico de barras pode combinar duas variáveis categóricas como secao e saturacao. Inicialmente, monta-se uma tabela de frequência, combinandos as contagens para cada nível das variáveis.\n\ntab2 &lt;- table(ambiente[,c(\"secao\", \"saturacao\")])\ntab2\n\n         saturacao\nsecao     Pobre Médio Saturado\n  Seção 1     1     5        8\n  Seção 2     2     1        0\n  Seção 3     3     0        0\n  Seção 4     2     8        0\n\n\nNeste caso, é possível representar estas contagens de diferentes formas:\n\nlayout(mat = matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE))\nbarplot(tab2, legend = TRUE)\nbarplot(tab2, legend = TRUE, beside = TRUE)\nbarplot(t(tab2), legend = TRUE)\nbarplot(t(tab2), legend = TRUE, beside = TRUE)\n\n\n\n\n\n\n\n\n\nA função layout(mat = matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)) organiza o espaço gráfico em um formato matricial com 2 linhas por 2 colunas, permitindo a inserção de 4 figuras. O argumento byrow = TRUE define que as figuras serão adicionais linha-a-linha.\nA função t() transpõe a tabela, o que consequentemente altera a referência da figura. No primeiro caso, a concentração de oxigênio é a variável principal e, no segundo caso, são as seções.\nO argumento beside = TRUE faz com que as barras apareçam lado-a-lado e beside = FALSE resulta em cada barra representa a variável principal subdividida nos níveis da variável secubdária.\nEm todos os gráficos foi adicionada uma legenda.\n\nAdiocionando elementos de formatação:\n\ncores &lt;- 1:4\nlimy1 &lt;- c(0, 17)\nlimy2 &lt;- c(0, 16)\nlegenda &lt;- list(cex = 0.8)\n\nlayout(mat = matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE))\nbarplot(tab2, legend = TRUE, col = cores, ylim = limy1, \n        args.legend = legenda)\nbox()\nbarplot(tab2, legend = TRUE, beside = TRUE, col = cores, \n        ylim = limy1, args.legend = legenda)\nbox()\nbarplot(t(tab2), legend = TRUE, col = cores, ylim = limy2, \n        args.legend = legenda)\nbox()\nbarplot(t(tab2), legend = TRUE, beside = TRUE, col = cores, \n        ylim = limy2, args.legend = legenda)\nbox()\n\n\n\n\n\n\n\n\n\n\n3.2 Boxplot\nO boxplot também pode ser utilizado para representar uma variável \\(X_1\\) para diferentes níveis de uma variável categórica \\(X_2\\), por exemplo oxy para cada nível de secao.\n\nboxplot(oxy ~ secao, data = ambiente)\n\n\n\n\n\n\n\n\nOs pontos associados à Seção 1 têm maiores concentrações de oxigênio (mediana = 110.5) e que os pontos associados à Seção 3 (mediana = 52).\nNa função boxplot foi utilizada a representação de fórmula no R (y ~ x) em que a variável no eixo y depende de x. Esta notação é amplamente utilizada em modelos estatísticos (ex. regressão linear, e análise de variância, etc.).\n\nAo invés de acessar a variável por ambiente$oxy, utilizou-se o nome da coluna (oxy) e adicionou-se o argumento data = ambiente para indicar em qual data frame a função irá buscar as variáveis.\n\n\n\n3.3 Gráfico de dispersão\nUm gráfico de dispersão mostra a associação entre duas variáveis quantitativas, por exemplo, concentração de nitrato (mg/l \\(\\times\\) 100) e distância da foz (km \\(\\times\\) 10). Neste caso a concentração de nitrato será representada como dependente da distância da foz.\n\nplot(nit ~ dfs, data = ambiente)\n\n\n\n\n\n\n\n\nOs resultados expressam uma relação em que a concentração de nutrientes aumenta à medida que distancia-se da foz.\nAdicionando formatação gráfica: nomes dos eixos (argumentos xlab e ylab) e tipo de ponto (argumento pch).\n\nplot(nit ~ dfs, data = ambiente,\n     xlab = bquote(\"Vazão média mínima (m\" ^3/\"seg x 100)\"),\n     ylab = bquote(\"Concentração de Nitrato (mg\"/\"l x 100)\"),\n     pch = 19\n)"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#compreendendo-o-ambiente-por-meio-de-suas-variáveis",
    "href": "conteudo/introducao_r/graficos-r.html#compreendendo-o-ambiente-por-meio-de-suas-variáveis",
    "title": "(Básico da) Visualização gráfica",
    "section": "4 Compreendendo o ambiente por meio de suas variáveis",
    "text": "4 Compreendendo o ambiente por meio de suas variáveis\nUm dos objetivo da descrição gráfica é representar o sistema por meio das variáveis escolhidas para quantificá-lo. Além dos gráficos apresentados anteriormente, há outras formas de incorporar essas variáveis em uma figura, utilizando cores, símbolos e textos no ambiente gráfico. Nesta seção, serão exploradas algumas possibilidades.\nOs pontos de amostragem foram obtidos ao longo do gradiente cabeceira-foz. As informações incluem as coordenadas geográficas desses pontos (no data frame $xy). A sequência dos pontos segue uma ordem crescente de distância da foz. Inicialmente, serão plotadas as coordenadas geográficas de todos os pontos utilizando um gráfico de linhas.\n\nplot(x = doubs$xy$x, y = doubs$xy$y, type = \"l\",\n     xlab = \"Coordenada em x (km)\", \n     ylab = \"Coordenada em y (km)\",\n     col = \"#4287f5\", lwd = 3)\n\n\n\n\n\n\n\n\nCompare a figura com o desenho do rio Doubs.\n\n\n\n\n\n\nNota\n\n\n\nUtilizamos a definição de cores em HEXADECIMAL. Você pode fazer o mesmo, escolhendo a cor desejada aqui: hex color picker.\n\n\nRepresentando os pontos de amostragem.\n\npontos_extremos &lt;- doubs$xy[which(doubs$env$dfs == min(doubs$env$dfs) | \n                                      doubs$env$dfs == max(doubs$env$dfs)),]\nplot(x = doubs$xy$x, y = doubs$xy$y, type = \"l\",\n     xlab = \"Coordenada em x (km)\", \n     ylab = \"Coordenada em y (km)\",\n     col = \"#4287f5\", lwd = 3)\ntext(x = pontos_extremos$x, \n       y = pontos_extremos$y,\n       labels = c(\"Cabeceira\", \"Foz\"))\n\n\n\n\n\n\n\n\nRepresentando as \\(4\\) seções do rio.\n\nsecao_cor &lt;- as.numeric(ambiente$secao)\n\nplot(x = doubs$xy$x, y = doubs$xy$y, type = \"l\",\n     xlab = \"Coordenada em x (km)\", \n     ylab = \"Coordenada em y (km)\",\n     col = \"#4287f5\", lwd = 3)\npoints(x = doubs$xy$x, y = doubs$xy$y, pch = 21, \n       bg = secao_cor, cex = 3)\nlegend(x = \"bottomright\", col = 1:4, \n       legend = levels(ambiente$secao), bty = \"n\", pch = 19)\n\n\n\n\n\n\n\n\nRepresentando a concentração de amônia (amm).\n\nsecao_cor &lt;- as.numeric(ambiente$secao) + 1\n\nplot(x = doubs$xy$x, y = doubs$xy$y, type = \"l\",\n     xlab = \"Coordenada em x (km)\", \n     ylab = \"Coordenada em y (km)\",\n     col = \"#4287f5\", lwd = 3)\npoints(x = doubs$xy$x, y = doubs$xy$y, pch = 21, \n       bg = secao_cor, cex = 4)\nlegend(x = \"bottomright\", col = 1:4, \n       legend = levels(ambiente$secao), bty = \"n\", pch = 19)\ntext(x = doubs$xy$x, y = doubs$xy$y, labels = doubs$env$amm, \n     cex = 0.8, font = 2)\ntext(x = 55, y = 220, labels = \"Concentração de amônia\")\ntext(x = 25, y = 120, label = \"Foz\")\ntext(x = 60, y = 20, label = \"Cabeceira\")\n\n\n\n\n\n\n\n\nA figura nos informa sobre a distribuição espacial da concentração de amônia entre as seções. Verifica-se que a concentração de amônia é mais nas seções \\(4\\) e \\(3\\).\n\n\n\n\n\n\nNota\n\n\n\nUtilizamos uma série de funções novas: text, points, legend. Para entender como elas funcionam, rode os comandos acima linha por linha e veja como cada função adiciona uma informação adicional à figura."
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#outros-argumentos-de-formatação-gráfica",
    "href": "conteudo/introducao_r/graficos-r.html#outros-argumentos-de-formatação-gráfica",
    "title": "(Básico da) Visualização gráfica",
    "section": "5 Outros argumentos de formatação gráfica",
    "text": "5 Outros argumentos de formatação gráfica\nA capacidade de formatação gráfica no R é extensa e qualquer tentativa de resumí-las seria incompleta. Abaixo exemplificam alguns argumentos comuns de formatação gráfica.\n\nplot(nit ~ dfs, data = ambiente)\nplot(nit ~ dfs, data = ambiente, pch = 2)\nplot(nit ~ dfs, data = ambiente, pch = 19)\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"b\")\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"b\",\n     xlab = \"Nitrato\", ylab = \"Vazão\")\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"b\", \n     xlab = \"Nitrato\", ylab = \"Vazão\", font.lab = 3)\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"l\", \n     lty = 2)\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"l\", \n     lty = 2, lwd = 3)\nplot(nit ~ dfs, data = ambiente, pch = 19, type = \"l\", \n     lty = 2, lwd = 3, col = 2)"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#figuras-compostas",
    "href": "conteudo/introducao_r/graficos-r.html#figuras-compostas",
    "title": "(Básico da) Visualização gráfica",
    "section": "6 Figuras compostas",
    "text": "6 Figuras compostas\nUma das formas mais simples para inserir múltiplas figuras no mesmo espaço gráfico é por meio da função layout. Abaixo, serõ inseridos \\(6\\) gráficos em uma espaço de \\(3\\) colunas por \\(2\\) linhas.\n\nlayout(mat = matrix(1:6, nrow = 3, ncol = 2))\nplot(alt ~ dfs, data = ambiente)\nplot(amm ~ alt, data = ambiente)\nplot(nit ~ alt, data = ambiente)\nplot(pH ~ alt, data = ambiente)\nplot(bdo ~ alt, data = ambiente)\nplot(oxy ~ alt, data = ambiente)"
  },
  {
    "objectID": "conteudo/introducao_r/graficos-r.html#exportando-figuras-funções-png-tiff-jpeg-e-bmp",
    "href": "conteudo/introducao_r/graficos-r.html#exportando-figuras-funções-png-tiff-jpeg-e-bmp",
    "title": "(Básico da) Visualização gráfica",
    "section": "7 Exportando figuras: funções png, tiff, jpeg e bmp",
    "text": "7 Exportando figuras: funções png, tiff, jpeg e bmp\nÉ possível exportar figuras em diversos formatos e resoluções. A função png é exemplificada abaixo. As funções para exportar em outros formatos são similares.\n\npng(filename = \"Exemplo_figura.png\",\n    width = 15, height = 15, units = \"cm\", \n    pointsize = 10, bg = \"white\", res = 800)\n\nplot(alt ~ dfs, data = ambiente, pch = 19, type = \"b\", \n     xlab = \"Vazão\", ylab = \"Elevação\")\n\ndev.off()\n\nA figura foi salva do diretório atual de sua seção de trabalho. Você pode conferir este diretório com o comando:\n\ngetwd()\n\nExperimente alterar os argumentos width, height, pointsize, units (com \"px\", \"in\", \"cm\" ou \"mm\") e res.\nAs capacidades gráficas no R incluem ainda muitos outros argumentos. Alguns deles são: cores (col), tipos da fonte (font), tamanhos de símbolos (cex), dos labels (cex.lab), dos rótulos dos eixos (cex.axis), título (main), etc. Pode-se ainda inserir legendas (função legend) e textos (função text). Veja o help de cada uma destas funções e a lista de argumentos possíveis para o ambiente gráfico do R em ?par. Veja também uma demonstração com demo(graphics), demo(image), demo(persp) e demo(plotmath).\nExistem diversos outros pacotes gráficos além do graphics:\n\nggplot2\nggvis\nLattice\nhighcharter\nLeaflet\nRColorBrewer\nPlotly\nsunburstR\nRGL\ndygraphs"
  },
  {
    "objectID": "conteudo/inferencia_estatistica/int_conf.html",
    "href": "conteudo/inferencia_estatistica/int_conf.html",
    "title": "Estimando a média populacional",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\nPacotes:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nsource('scripts/normal_empirica_gg.r')"
  },
  {
    "objectID": "conteudo/inferencia_estatistica/int_conf.html#estimação-pontual-e-estimação-intervalar",
    "href": "conteudo/inferencia_estatistica/int_conf.html#estimação-pontual-e-estimação-intervalar",
    "title": "Estimando a média populacional",
    "section": "1 Estimação pontual e estimação intervalar",
    "text": "1 Estimação pontual e estimação intervalar\nA média \\(\\overline{X}\\) obtida a partir de uma determinada amostra varia em função das características das unidades amostrais que foram selecionadas. Portanto, \\(\\overline{X}\\) não será igual à média \\(\\mu\\). No entanto, o TLC nos garante que a distribuição esperada das médias amostrais terá uma distribuição normal e que a média das médias (\\(\\mu_{\\overline{X}}\\)) será igual a \\(\\mu\\). Vimos ainda que o desvio padrão da distribuição das médias amostrais (conhecido como erro padrão - \\(\\sigma_{\\overline{X}}\\)) dependerá do tamanho da amostra \\(n\\), de acordo com a expressão:\n\\[\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nUma vez que não conhecemos \\(\\mu\\), temos que estimá-lo a partir da amostra. Neste caso, \\(\\overline{X}\\) será nossa melhor estimativa da média populacional. Dizemos que \\(\\overline{X}\\) é o estimador pontual de \\(\\mu\\).\nComo \\(\\overline{X}\\) varia em função de nossa amostra particular, devemos obter além da estimativa pontual, uma estimativa intervalar que nos é fornecida pelo intervalo de confiança.\n\n1.1 Intervalo de confiança\n\n\n\n\n\n\nIntervalo de confiança: definição\n\n\n\nÉ o intervalo de valores associado a um determinado nível de significância (\\(\\alpha\\)). Quando dizemos que um intervalo foi calculado a um nível de confiança de \\(95\\%\\) (\\(1 - \\alpha\\)), estamos dizendo que a probabilidade do IC conter o valor da média populacional \\(\\mu\\) é de \\(95\\%\\).\n\n\nO IC é calculado por:\n\\[IC_{1-\\alpha} = \\mu \\pm z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\]\nO valor de \\(z_{\\alpha/2}\\) é o valor do índice \\(z\\) associado ao nível de confiança desejado.\nSe desejamos definir o intervalo de confiança a 95% precisamos garantir que haja uma probabilidade de 95% de que a média amostral esteja ao redor da média populacional. Deste modo, o limite deve excluir 2.5% da porção superior e 2.5% da porção inferior da curva. Para isto, definimos \\(z_{\\alpha/2} = 1.96\\), sendo \\(\\alpha\\) fixado em 0.05.\n\nO valor \\(z_{\\alpha/2} = 1.96\\) foi retirado da Tabela \\(Z\\) como o módulo do valor de \\(z\\) que delimida uma área inferior igual a \\(0.025\\).\n\nSe queremos um nível de confiança diferente, basta ajustar o valor de \\(\\alpha\\). Por exemplo, se queremos um nível de significância a 99%, fixamos \\(\\alpha\\) em \\(0.01\\) e portanto \\(z = 2.58\\). Da mesma forma, o \\(IC_{90\\%}\\) poderá ser obtido com \\(\\alpha = 0.10\\) e consequentemente \\(z = 1.64\\). Estes e outros limites descrevem as probabilidades em uma distribuição normal padronizada, que podem ser obtidos com o uso da maioria dos softwares estatísticos, além de estarem incluso na Tabela Z, encontrada na grande maioria dos livros de estatística básica.\n\n\nCódigo\n# Ver função completa no arquivo 'scripts/normal_empirica_gg.r'\nnormal_empirica_gg(xlabels = c(-4:4))\n\n\n\n\n\n\n\n\nFigura 1: Áreas de probabilidade em uma distribuição Normal Padronizada (Distribuição Z).\n\n\n\n\n\nPara o cálculo do intervalo de confiança, assumimos que as médias amostrais seguem uma distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\frac{\\sigma}{\\sqrt{n}}\\). Ao fazer isso, estamos aplicando o Teorema Central do Limite (TCL). Geralmente, não temos os valores de \\(\\mu\\) e \\(\\sigma\\), por isso utilizamos os valores de \\(\\overline{X}\\) e \\(s\\), calculados a partir de nossa amostra. Quando o tamanho das amostras é grande (\\(n \\geq 30\\)), é aceitável utilizar o valor de \\(z_{\\alpha/2}\\). Assim::\n\\[IC_{1-\\alpha} = \\overline{X} \\pm z_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\]\n\n1.1.1 Distribuiçao \\(t\\) de Student: \\(\\mu\\) e \\(\\sigma\\) desconhecidos\nQuando não conhecemos \\(\\mu\\) e \\(\\sigma\\) e as amostras são pequenas (ex. \\(n&lt;30\\)), a dsitribuição normal não é a melhor aproximação para o comportamento das médias amostrais. Nestes casos, substituímos a distribuição de \\(z\\) pela Distribuição \\(t\\) de Student, sendo o intervalo de confiança obtido por:\n\\[IC_{1-\\alpha} = \\overline{X} \\pm t_{\\alpha/2, gl} \\times \\frac{s}{\\sqrt{n}}\\]\nEm que \\(\\alpha\\) continua sendo o nível de significância e \\(gl\\) é definido como os graus de liberdade. Neste caso, os graus de liberdade são dados por:\n\\[gl = n-1\\]\nO formato da distribuição \\(t\\) de student não é constante. À medida que o tamanho amostral aumenta, o formado da distribuição \\(t\\) converge para a distribuição normal. Isto faz com que na prática raremente se utiliza a distribuição \\(Z\\), substituindo-a pela distribuição \\(t\\) de Student.\n\n\n\n\n\n\n\n\nFigura 2: Função de densidade de t para diferentes graus de liberdade.\n\n\n\n\n\nPara amostras pequenas (\\(n = 2\\)) o formato da distribuição de \\(t\\) é distinto da distribuição normal. No entanto, para tamanhos amostrais maiores (\\(n = 30\\)) as o formato da distribuição \\(t\\) tende a a convergir para o mesmo formato a distribuição normal. Esta característica implica que a área a partir de um determinado limite \\(t_i\\) não é constante como na distribuição normal, mas depende do tamanho da amostra, como pode ser visto abaixo.\n\n\n\n\n\n\n\n\nFigura 3: Função de densidade de t para diferentes graus de liberdade."
  },
  {
    "objectID": "conteudo/inferencia_estatistica/int_conf.html#introdução-à-suficiência-amostral",
    "href": "conteudo/inferencia_estatistica/int_conf.html#introdução-à-suficiência-amostral",
    "title": "Estimando a média populacional",
    "section": "2 Introdução à suficiência amostral",
    "text": "2 Introdução à suficiência amostral\nUma decisão central ao planejamento de um experimento é quanto recurso (ex. tempo, dinheiro, mão de obra) devem ser investidos para se obter boas estimativas dos parâmetros populacionais. Por boas estimativas, entendemos amostras precisas, ou seja, que podem ser definida por amostras com baixo erro padrão e acuradas, que em média apontem para o verdadeiro valor do parâmetro. Neste caso, uma das primeiras questões a ser feita é “Qual tamanho amostral aplicar em meu estudo?”.\nVimos que aumentar o tamanho amostral resulta em estimativas mais precisas, isto é com menor erro padrão. Portanto, um bom delineamento amostral é aquele que permita, a um custo mínimo, obter estimativas com a precisão desejada. Uma pesquisa que resulte em estimativas demasiadamente imprecisas pode se mostrar inútil. O que dizer por exemplo, se um estudo conclui que o comprimento médios de uma espécie de pescado é de \\(35\\) cm com uma incerteza a \\(95\\%\\) entre \\(15\\) e \\(55\\) cm? Uma estimativa com tal nível de imprecisão não terá qualquer implicação prática.\nPor outro lado, partir de um determinado tamanho amostral o ganho em precisão torna-se mínimo. Isto significa que amostras demasiadamente grandes podem ter um custo muto alto porém não serem capazes aumentar de forma relevante a precisão do experimento.\nVeja o que ocorre com o erro padrão de uma amostra à medida que aumenta o tamanho \\(n\\).\n\n\n\n\n\n\n\n\nFigura 4: Efeito do aumento do tamanho amostral n sobre o erro padrão da média.\n\n\n\n\n\nNeste exemplo, para amostras de tamanho 1, \\(\\sigma_{\\overline{X}} = 4\\). Se tivermos agora amostras de tamanho 10, \\(\\sigma_{\\overline{X}} = 1.2\\), uma redução de mais de 50%. No entanto aumentarmos o tamanho amostral para 50 o erro padrão cai somente de \\(1,2\\) para \\(0,56\\). Isto significa que a partir de determinado ponto (neste exemplo a partir de \\(10\\) ou \\(20\\) amostras), a redução no erro padrão torna-se mínima. Neste momento podemos podemos refletir sobre o custo de continuar aumentando o tamanho amostral para obter um ganho cada vez menor em precisão.\nPara encontrarmos o tamanho amostral desejado, devemos decidir sobre dois pontos: i - que nível de acurácia desejado, ou seja, quão distante do valor real (média populacional) queremos que nossa esimativa esteja; e ii - qual o nível de confiança do resultado, ou seja, com que precisão queremos fazer esta estimativa.\n\n2.1 Nível de acurácia desejado (margem de erro) e nível de confiança na estimativa\nO nível de acurácia desejado é comumente conhecido com margem de Erro (E), definida como diferença máxima provável (com probabilidade \\(1-\\alpha\\)) entre a média amostral e a média populacional.\nA margem de erro para a média amostral pode ser obtida por (compare esta expressão com a do intervalo de confiança):\n\\[E = z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\]\nO nível de confiança na estimativa nos garante que nossa estimativa estará dentro da margem de erro assumida com probabilidade \\(1-\\alpha\\). Como vimos acima, valores típicos para o nível de confiança são \\(99\\%\\), \\(95\\%\\) e \\(90\\%\\).\nUma representação esquemática do erro amostral e do nível de confiança na distribuição de \\(z\\) pode ser vista abaixo:\n\n\n\n\n\n\n\n\nFigura 5: Erro amostra e nível de confiança na distribuição Z.\n\n\n\n\n\nA definição da margem de erro e do nível de confiança depende de estimativas prévias dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Estas estimativas podem ser obtidas na literatura, buscando estudos similares, ou por meio de um projeto piloto. Em um experimento piloto, o pesquisador irá conduzir seu plano de amostragem com um tamanho mínimo, justamente para avaliar a eficiência metodológica, adequabilidade dos resultados e prever o esforço amostral adequado. As informações de um pequeno estudo piloto, se bem aproveitadas, podem evitar erros simples de delineamento, além de invariavelmente, permitir economia de recusros e consequentemente ganho em qualidade.\n\n\n2.2 Determinando o tamanho de uma amostra\nPodemos voltar a nossa questão anterior sobre Qual tamanho amostral aplicar em meu estudo?. Esta questão pode ser reformulada como:\n\nQual tamanho amostral aplicar para obter uma estimativa de \\(\\mu\\) que possua uma margem de erro \\(E\\) e nivel de confiança \\(1-\\alpha\\) pré-determinados.\n\nIniciando com a fórmula da margem de erro:\n\\[E = z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\]\nisolamos a variável \\(n\\) para obter:\n\\[n = (\\frac{ z_{\\alpha/2} \\times \\sigma}{E})^2\\]\nNovamente, uma vez que não conhecemos o desvio padrão populacional \\(\\sigma\\) podemos substituí-lo pelo desvio padrão (\\(s\\)) de um experimento piloto ou estimá-lo a partir da literatura.\n\n\n\n\n\n\nVídeo-aulas"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varqualit.html",
    "href": "conteudo/estatistica_descritiva/varqualit.html",
    "title": "Descrevendo variáveis qualitativas",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nVariáveis qualitativas podem ser categóricas não ordenadas ou categóricas ordenadas. A descrição de variáveis desta natureza se dá por meio da contagem e da representação dos níveis destas variáveis por meio da contagem total, pelos valores relativos ou percentuais.\nImporte a base de dados Reservatorios_Parana_parcial.csv.\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\nNa tabela, temos 3 variáveis categóricas: Reservatorio, Bacia e Trofia. A primeira identifica cada reservatório pelo seu nome. A segunda é uma variável categórica não ordenada (nível de mensuração nominal) e a terceira uma variável categórica ordenada (nível de mensuração ordinal)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varqualit.html#representação-em-tabelas-de-frequência",
    "href": "conteudo/estatistica_descritiva/varqualit.html#representação-em-tabelas-de-frequência",
    "title": "Descrevendo variáveis qualitativas",
    "section": "1 Representação em tabelas de frequência",
    "text": "1 Representação em tabelas de frequência\nSe uma variável é descrita no nível de mensuração nominal, como é o caso de Bacia, podemos obter a frequência com que cada um dos níveis aparece na variável. Essa contagem pode ser obtida por meio de uma tabela de frequências.\n\nfbacia &lt;- res |&gt; \n  group_by(Bacia) |&gt; \n  summarise(Frequencia = n())\n\nfbacia |&gt; \n  gt()\n\n\n\n\n\n\n\nBacia\nFrequencia\n\n\n\n\nIguacu\n13\n\n\nIvai\n2\n\n\nLitoranea\n4\n\n\nParanapanema\n7\n\n\nPiriqui\n2\n\n\nTibagi\n3\n\n\n\n\n\n\n\nO resultado mostra que existem 13 reservatórios na tabela pertencentes à bacia do rio Iguacu, 2 à bacia do rio Ivai e assim por diante. Confira estas contagens na base de dados.\nAs linhas da tabela estão organizadas em ordem alfabética. Para facilitar a visualização, podemos ordená-las de modo decrescente como função do número de reservatórios por bacia.\n\nfbacia &lt;- fbacia |&gt; \n  arrange(desc(Frequencia))\n\nfbacia |&gt; \n  gt()\n\n\n\n\n\n\n\nBacia\nFrequencia\n\n\n\n\nIguacu\n13\n\n\nParanapanema\n7\n\n\nLitoranea\n4\n\n\nTibagi\n3\n\n\nIvai\n2\n\n\nPiriqui\n2\n\n\n\n\n\n\n\nPodemos olhar também para a frequência relativa do número de reservatórios por bacia.\n\nfbacia_rel &lt;- fbacia |&gt; \n  mutate(Freq_relativa = Frequencia / sum(Frequencia))\n\nfbacia_rel |&gt; \n  gt()\n\n\n\n\n\n\n\nBacia\nFrequencia\nFreq_relativa\n\n\n\n\nIguacu\n13\n0.41935484\n\n\nParanapanema\n7\n0.22580645\n\n\nLitoranea\n4\n0.12903226\n\n\nTibagi\n3\n0.09677419\n\n\nIvai\n2\n0.06451613\n\n\nPiriqui\n2\n0.06451613\n\n\n\n\n\n\n\nA característica da frequência relativa é que o somatório da coluna deve ser igual a 1, enquanto a frequência numérica tem o somatório igual ao número de linhas na tabela.\n\nfbacia_rel |&gt; \n  summarise_if(is.numeric, sum) |&gt; \n  gt()\n\n\n\n\n\n\n\nFrequencia\nFreq_relativa\n\n\n\n\n31\n1"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varqualit.html#tabelas-de-frequência-para-variáveis-categóricas-ordenadas",
    "href": "conteudo/estatistica_descritiva/varqualit.html#tabelas-de-frequência-para-variáveis-categóricas-ordenadas",
    "title": "Descrevendo variáveis qualitativas",
    "section": "2 Tabelas de frequência para variáveis categóricas ordenadas",
    "text": "2 Tabelas de frequência para variáveis categóricas ordenadas\nA característica da variável Trofia difere da anterior unicamente por ser uma variável categórica ordenada que, no caso, expressa o grau de eutrofização dos reservatórios. Neste sentido, a única mudança na representação da variável se deve ao fato de que existe uma sequência natural para representar os níveis. Podemos indicar que uma determinada variável é categórica ordenada fazendo uma pequena alteração na base de dados.\nSe montarmos uma tabela de frequência da variável Trofia, teremos as linhas organizadas em ordem alfabética:\n\nftrofia &lt;- res |&gt; \n  group_by(Trofia) |&gt; \n  summarise(Frequencia = n())\n\nftrofia |&gt; \n  gt()\n\n\n\n\n\n\n\nTrofia\nFrequencia\n\n\n\n\nEutrÃ³fico\n2\n\n\nMesotrÃ³fico\n3\n\n\nOligotrÃ³fico\n24\n\n\nNA\n2\n\n\n\n\n\n\n\nSe desejarmos que as colunas apareçam como função do nível de eutrofização, devemos primeiro transformar a variável Trofia em um fator ordenado, que é o modo como o R interpreta uma variável categórica ordenada.\nInicialmente, use o comando abaixo para verificar que o R entende a variável Trofia como um character (&lt;chr&gt;).\n\nglimpse(res)\n\nRows: 31\nColumns: 11\n$ Reservatorio  &lt;chr&gt; \"Cavernoso\", \"Curucaca\", \"Foz do Areia\", \"Irai\", \"JMF\", …\n$ Bacia         &lt;chr&gt; \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguac…\n$ Fechamento    &lt;dbl&gt; 1965, 1982, 1980, 2000, 1970, 1996, 1978, 1979, 1998, 19…\n$ Area          &lt;dbl&gt; 2.90, 2.00, 139.00, 15.00, 0.45, 3.40, 14.00, 3.30, 124.…\n$ Trofia        &lt;chr&gt; \"OligotrÃ³fico\", \"OligotrÃ³fico\", \"OligotrÃ³fico\", \"Eutr…\n$ pH            &lt;dbl&gt; 7.4, 7.0, 7.3, 6.9, 7.3, 7.1, 8.8, 7.1, 7.3, 6.5, 8.6, 9…\n$ Condutividade &lt;dbl&gt; 33.1, 32.4, 35.5, 50.2, 40.2, 23.7, 125.6, 22.8, 39.6, 2…\n$ Alcalinidade  &lt;dbl&gt; 139.80, 125.70, 97.00, 3.30, 3.70, 152.70, 526.00, 50.67…\n$ P.total       &lt;dbl&gt; 7.8, 4.7, 14.3, 53.4, 41.2, 3.3, 15.2, 4.5, 12.1, 11.0, …\n$ Riqueza       &lt;dbl&gt; 18, 16, 19, 12, 18, 17, 11, 8, 21, 8, 24, 21, 22, 15, 10…\n$ CPUE          &lt;dbl&gt; 9.22, 28.73, 11.59, 30.76, 5.95, 7.75, 7.51, 4.01, 20.83…\n\n\nIremos transformar esta variável para que o R a interprete como uma variável categórica ordenada.\n\nres &lt;- res |&gt; \n  mutate(Trofia = factor(Trofia, ordered = TRUE, \n                         levels = c(\"Oligotrófico\", \n                                    \"Mesotrófico\", \n                                    \"Eutrófico\")))\n\nApós aplicarmos este comando, vemos que agora o R reconhece esta variável como do tipo &lt;ord&gt;:\n\nglimpse(res)\n\nRows: 31\nColumns: 11\n$ Reservatorio  &lt;chr&gt; \"Cavernoso\", \"Curucaca\", \"Foz do Areia\", \"Irai\", \"JMF\", …\n$ Bacia         &lt;chr&gt; \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguacu\", \"Iguac…\n$ Fechamento    &lt;dbl&gt; 1965, 1982, 1980, 2000, 1970, 1996, 1978, 1979, 1998, 19…\n$ Area          &lt;dbl&gt; 2.90, 2.00, 139.00, 15.00, 0.45, 3.40, 14.00, 3.30, 124.…\n$ Trofia        &lt;ord&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ pH            &lt;dbl&gt; 7.4, 7.0, 7.3, 6.9, 7.3, 7.1, 8.8, 7.1, 7.3, 6.5, 8.6, 9…\n$ Condutividade &lt;dbl&gt; 33.1, 32.4, 35.5, 50.2, 40.2, 23.7, 125.6, 22.8, 39.6, 2…\n$ Alcalinidade  &lt;dbl&gt; 139.80, 125.70, 97.00, 3.30, 3.70, 152.70, 526.00, 50.67…\n$ P.total       &lt;dbl&gt; 7.8, 4.7, 14.3, 53.4, 41.2, 3.3, 15.2, 4.5, 12.1, 11.0, …\n$ Riqueza       &lt;dbl&gt; 18, 16, 19, 12, 18, 17, 11, 8, 21, 8, 24, 21, 22, 15, 10…\n$ CPUE          &lt;dbl&gt; 9.22, 28.73, 11.59, 30.76, 5.95, 7.75, 7.51, 4.01, 20.83…\n\n\nE se fizermos:\n\nres$Trofia\n\n [1] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n[16] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n[31] &lt;NA&gt;\nLevels: Oligotrófico &lt; Mesotrófico &lt; Eutrófico\n\n\nTemos agora a indicação de que há uma ordenação sequencial nos níveis de trofia em que Oligotrófico &lt; Mesotrófico &lt; Eutrófico.\nA partir de agora, se extrairmos uma tabela de frequência relativa, as linhas serão apresentadas na ordem pré-definida.\n\nftrofia &lt;- res |&gt; \n  group_by(Trofia) |&gt; \n  summarise(Frequencia = n())\n\nftrofia |&gt; \n  gt()\n\n\n\n\n\n\n\nTrofia\nFrequencia\n\n\n\n\nNA\n31\n\n\n\n\n\n\n\nNa tabela acima, a última linha aparece vazia, pois há casos sem informação, isto é, com dados faltantes que são representados por NA. Caso você não queira representar os dados faltantes, é possível utilizar a função drop_na() para excluir estas linhas.\n\nftrofia &lt;- res |&gt; \n  drop_na(Trofia) |&gt; \n  group_by(Trofia) |&gt; \n  summarise(Frequencia = n())\n\nftrofia |&gt; \n  gt()\n\n\n\n\n\n\n\nTrofia\nFrequencia\n\n\n\n\n\n\n\n\n\nPodemos adicionar uma coluna de frequência relativa como fizemos anteriormente.\n\nftrofia_rel &lt;- ftrofia |&gt; \n  mutate(Freq_relativa = Frequencia / sum(Frequencia))\n\nftrofia_rel |&gt; \n  gt()\n\n\n\n\n\n\n\nTrofia\nFrequencia\nFreq_relativa"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varqualit.html#representação-gráfica",
    "href": "conteudo/estatistica_descritiva/varqualit.html#representação-gráfica",
    "title": "Descrevendo variáveis qualitativas",
    "section": "3 Representação gráfica",
    "text": "3 Representação gráfica\nVariáveis categóricas não ordenadas ou ordenadas podem ser representadas por gráficos de barras.\n\n\n\n\n\n\nO pacote ggplot2\n\n\n\nUtilizaremos o pacote ggplot2 para representar graficamente as variáveis. O ggplot2 é instalado e habilitado juntamente com o tidyverse, de modo que neste momento você já o tem habilitado em sua sessão do R.\nPara uma rápida explicação do ggplot2, veja aqui. Para uma explicação detalhada, veja o site oficial (ggplot2){target=“_blank”} e o livro ggplot2: Elegant Graphics for Data Analysis.\n\n\n\n3.1 Criando um gráfico no ggplot2\nUm gráfico no ggplot2 é feito em camadas que devem ter minimamente:\n\nA definição da tabela de dados;\nA estética gráfica indicando quais variáveis serão representadas e suas posições no gráfico;\nO formato da representação por meio de geometrias gráficas (ex. gráficos de pontos, linhas, barras, etc.).\n\nEsta abordagem permite que tenhamos um método consistente para construir diferentes tipos de gráficos.\nGráfico de frequência\nUm gráfico de barras da variável Bacia ficaria:\n\nggplot(data = res) + # define tabela de dados\n  aes(x = Bacia) +   # define a estética gráfica\n  geom_bar()         # define a geometria gráfica\n\n\n\n\n\n\n\n\nVamos entender o comando:\n\nggplot(res): define a tabela de dados que será utilizada.\naes(x = Bacia): define que o eixo x deste gráfico deverá conter os níveis da variável Bacia.\ngeom_bar(): define o tipo gráfico, que no ggplot2 é denominado de geometria gráfica.\n\nEstes argumentos devem ser inseridos sequencialmente separados pelo símbolo +.\nO argumento geom_bar() espera como argumento uma variável qualitativa em um dos eixos. Por padrão, a função fará a contagem dos níveis dentro da variável e representará no eixo y.\nPoderíamos ter feito o mesmo gráfico de barras indicando que a variável Bacia seria representada no eixo y, o que resultaria em um gráfico de barras invertido conforme abaixo:\n\nggplot(data = res) +\n  aes(y = Bacia) +\n  geom_bar()\n\n\n\n\n\n\n\n\nA estética gráfica (aes()) não precisa estar em uma linha separada. Também não é obrigatório escrevermos data = res. De fato, é mais comum escrevermos esta sequência de argumentos como:\n\nggplot(res, mapping = aes(x = Bacia)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nou simplesmente:\n\nggplot(res, aes(x = Bacia)) +\n  geom_bar()\n\n\n\n\n\n\n\n\no que irá gerar os mesmos resultados.\nFinalmente, poderíamos organizar as barras em ordem decrescente como fizemos com as tabelas de frequência, utilizando a função fct_infreq():\n\nggplot(res, aes(x = fct_infreq(Bacia))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nou em ordem crescente, revertendo o comando anterior com a função fct_rev().\n\nggplot(res, aes(x = fct_rev(fct_infreq(Bacia)))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nFormatando a figura\nPara tornar a figura mais autoexplicativa, podemos adicionar camadas identificando os eixos e fornecendo título, subtítulo e outras informações:\n\nggplot(res, aes(x = Bacia)) +\n  geom_bar() +\n  labs(\n    title = \"Reservatórios do Estado do Paraná\",\n    subtitle = \"Reservatórios por bacia hidrográfica\",\n    caption = \"Dados obtidos do livro: Biocenoses em Reservatórios\",\n    x = \"Bacia hidrográfica\",\n    y = \"Frequência\"\n  )\n\n\n\n\n\n\n\n\nGráfico de frequência relativa\nUtilizando o ggplot2, é simples construir um gráfico de frequência relativa.\n\nggplot(res, aes(x = Bacia, y = after_stat(prop), group = 1)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nVeja que para isso transformamos as contagens em proporções. Se quisermos transformar em percentuais, então:\n\nggplot(res, aes(x = Bacia, y = after_stat(prop), group = 1)) +\n  geom_bar() +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nOutras opções para construir um gráfico de barras\nAs figuras que acabamos de fazer apresentam, de modo gráfico, as mesmas informações das tabelas de frequência vistas no início do capítulo sem que fosse necessário construir a tabela de frequência, pois o comando geom_bar() já realiza esta contagem.\nEntretanto, caso já tivéssemos a tabela de frequência, também poderíamos utilizá-la diretamente. No início do capítulo, construímos a tabela fbacia_rel, onde tínhamos 3 colunas: Bacia, Frequencia, Freq_relativa.\nPodemos construir gráficos de barras das tabelas Frequencia ou Freq_relativa da seguinte forma:\n\nggplot(fbacia_rel, aes(x = Bacia, y = Frequencia)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\ne\n\nggplot(fbacia_rel, aes(x = Bacia, y = Freq_relativa)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nPara utilizar diretamente uma tabela de frequências, devemos oferecer a variável do eixo x, do eixo y e, no comando geom_bar(), adicionar o argumento stat = \"identity\". Feito isso, o comando utiliza diretamente os números disponíveis em cada linha da coluna Frequencia.\n\nGráfico de frequência para variáveis categóricas ordenadas\nPara variáveis categóricas ordenadas, valem os mesmos comandos apresentados acima. Usamos a função geom_bar() para construir os gráficos de barras. A diferença é que, antes da construção, é necessário que a variável em questão tenha sido transformada para um fator ordenado.\nLembrando o que fizemos no início do capítulo, esta transformação pode ser feita para a variável Trofia com os comandos:\n\nres &lt;- res |&gt; \n  mutate(Trofia = factor(Trofia, ordered = TRUE, \n                         levels = c(\"Oligotrófico\", \n                                    \"Mesotrófico\", \n                                    \"Eutrófico\")))\n\nFeito isso, o comando geom_bar() vai organizar os níveis de acordo com a sequência definida:\n\nggplot(res, aes(x = Trofia)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nE caso seja necessário retirar reservatórios com dados faltantes em Trofia, podemos fazer:\n\nres |&gt; \n  drop_na(Trofia) |&gt; \n  ggplot(aes(x = Trofia)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPré-processamento do data-frame para o ggplot2\n\n\n\nNo comando acima, a tabela de dados não foi inserida dentro do comando ggplot(). Ela foi inicialmente processada para remoção de NAs com a função drop_na() e o operador |&gt; foi utilizado para inserir o resultado do processamento no ggplot(). Esta é outra maneira de combinar capacidade de processamento de dados no R com a representação gráfica do pacote ggplot2."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html",
    "href": "conteudo/estatistica_descritiva/variacao.html",
    "title": "Medidas de variação",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nAs medidas de variação indicam o grau de dispersão das observações. Distribuições com observações muito próximas à média têm baixo grau de dispersão, enquanto aquelas com observações muito distantes da média têm alto grau de dispersão. Vamos apresentar quatro índices que medem o grau de dispersão: a variância, o desvio padrão, o coeficiente de variação e a amplitude de variação."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html#variância-amostral",
    "href": "conteudo/estatistica_descritiva/variacao.html#variância-amostral",
    "title": "Medidas de variação",
    "section": "1 Variância amostral",
    "text": "1 Variância amostral\nA variância amostral, descrita pelo símbolo \\(s^2\\), mede quão distantes as observações em uma variável estão de sua média aritmética.\nPara um conjunto de observações, \\(s^2\\) é dada por:\n\\[s^2=\\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1}\\]\nSeja a variável \\(X\\) abaixo:\n\n\nCódigo\nn &lt;- 5\nset.seed(1)\nX &lt;- sample(x = 1:10, size = n, rep = TRUE)\n\n\n\\(X =\\) {9, 4, 7, 1, 2}\n\\(X\\) tem 5 observações:\nPara calcularmos \\(s^2\\), devemos inicialmente obter a média de \\(X\\), que neste caso é:\n\\(\\overline{X} = 4.6\\)\nE subtrair cada observação da média:\n\n\nCódigo\ndf &lt;- data.frame(X) |&gt; \n  mutate(dif = X - mean(X)) |&gt; \n  as.data.frame() # Garantir que é um data frame simples\n\ndf |&gt; \n  knitr::kable(col.names = c('$X$', '$X - \\\\overline{X}$'))\n\n\n\n\n\n\\(X\\)\n\\(X - \\overline{X}\\)\n\n\n\n\n9\n4.4\n\n\n4\n-0.6\n\n\n7\n2.4\n\n\n1\n-3.6\n\n\n2\n-2.6\n\n\n\n\n\nEm seguida, elevamos cada diferença ao quadrado:\n\n\nCódigo\ndf &lt;- df |&gt; \n  mutate(dif = X - mean(X)) |&gt; \n  mutate(dif2 = dif^2)\n\ndf |&gt; \n  knitr::kable(col.names = c('$X$',\n                             '$X - \\\\overline{X}$',\n                             '${(X - \\\\overline{X})}^{2}$'))\n\n\n\n\n\n\\(X\\)\n\\(X - \\overline{X}\\)\n\\({(X - \\overline{X})}^{2}\\)\n\n\n\n\n9\n4.4\n19.36\n\n\n4\n-0.6\n0.36\n\n\n7\n2.4\n5.76\n\n\n1\n-3.6\n12.96\n\n\n2\n-2.6\n6.76\n\n\n\n\n\nSe somarmos essas quantias e dividirmos por \\(n-1\\), teremos a variância amostral como:\n\\(s^2 = \\frac{19.36 + 0.36 + 5.76 + 12.96 + 6.76}{5 - 1} = \\frac{45.2}{4} = 11.3\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html#desvio-padrão-amostral",
    "href": "conteudo/estatistica_descritiva/variacao.html#desvio-padrão-amostral",
    "title": "Medidas de variação",
    "section": "2 Desvio padrão amostral",
    "text": "2 Desvio padrão amostral\nO desvio padrão amostral (\\(s\\)) é a raiz quadrada da variância amostral.\n\\[s=\\sqrt{\\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1}}\\]\nE em nosso exemplo:\n\\(s = \\sqrt{11.3} = 3.36\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html#coeficiente-de-variação",
    "href": "conteudo/estatistica_descritiva/variacao.html#coeficiente-de-variação",
    "title": "Medidas de variação",
    "section": "3 Coeficiente de variação",
    "text": "3 Coeficiente de variação\nO coeficiente de variação (cv) relaciona o desvio padrão à média, sendo definido por:\n\\[cv = s/\\overline{X}\\] ou \\[cv_{\\%}  = s/\\overline{X}\\cdot 100\\]\nEm nosso exemplo:\n\\(cv = \\frac{3.36}{4.6} \\cdot 100 = 73.08\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html#amplitude-de-variação",
    "href": "conteudo/estatistica_descritiva/variacao.html#amplitude-de-variação",
    "title": "Medidas de variação",
    "section": "4 Amplitude de variação",
    "text": "4 Amplitude de variação\nÉ a diferença entre os pontos máximo e mínimo de um grupo de observações.\nAmplitude de variação = \\(X_{maximo} - X_{minimo}\\)\nque em nosso exemplo é:\nAmplitude de variação = \\(9 - 1 = 8\\)"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/variacao.html#obtendo-medidas-de-variação-de-uma-tabela-de-dados",
    "href": "conteudo/estatistica_descritiva/variacao.html#obtendo-medidas-de-variação-de-uma-tabela-de-dados",
    "title": "Medidas de variação",
    "section": "5 Obtendo medidas de variação de uma tabela de dados",
    "text": "5 Obtendo medidas de variação de uma tabela de dados\nImporte a base de dados Reservatorios_Parana_parcial.csv.\n\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\n\nUsaremos a função summarise para obter descritores de variação para a variável CPUE.\n\nres |&gt; \n  summarise(CPUE_var = var(CPUE),\n            CPUE_dp = sd(CPUE),\n            CPUE_cv = sd(CPUE) / mean(CPUE) * 100,\n            CPUE_amplitude = max(CPUE) - min(CPUE)) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE_var\nCPUE_dp\nCPUE_cv\nCPUE_amplitude\n\n\n\n\n54.31838\n7.3701\n58.02786\n28.71"
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varquant.html",
    "href": "conteudo/estatistica_descritiva/varquant.html",
    "title": "Descrevendo variáveis quantitativas",
    "section": "",
    "text": "Pacotes e funções utilizados\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\nVariáveis quantitativas podem ser discretas ou contínuas. A descrição dos padrões de distribuição para esses tipos de variáveis é feita utilizando tabelas (frequência e frequência acumulada) e gráficos (histogramas ou gráficos de frequência acumulada)."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varquant.html#tabelas-de-frequência-para-variáveis-quantitativas",
    "href": "conteudo/estatistica_descritiva/varquant.html#tabelas-de-frequência-para-variáveis-quantitativas",
    "title": "Descrevendo variáveis quantitativas",
    "section": "1 Tabelas de frequência para variáveis quantitativas",
    "text": "1 Tabelas de frequência para variáveis quantitativas\nA construção de tabelas de frequências para variáveis quantitativas necessita que agrupemos as observações em faixas de valores. Veja as observações abaixo por exemplo:\n\\(X =\\) {2.66, 3.72, 5.73, 9.08, 2.02, 8.98, 9.45, 6.61, 6.29, 0.62}\nPodemos agrupá-las nas seguintes faixas de valores:\n(0,2], (2,4], (4,6], (6,8], (8,10]\nEstas faixas de valores são denominadas de intervalos de classe. Se alocadas nestes intervalos, as observações ficam:\n\nX &lt;- c(2.66, 3.72, 5.73, 9.08, 2.02, 8.98, 9.45, 6.61, 6.29, 0.62)\nClasses &lt;- cut(X, seq(0, 10, by = 2))\n\ndf &lt;- data.frame(X, Classes)\n\ndf |&gt; \n  gt()\n\n\n\n\n\n\n\nX\nClasses\n\n\n\n\n2.66\n(2,4]\n\n\n3.72\n(2,4]\n\n\n5.73\n(4,6]\n\n\n9.08\n(8,10]\n\n\n2.02\n(2,4]\n\n\n8.98\n(8,10]\n\n\n9.45\n(8,10]\n\n\n6.61\n(6,8]\n\n\n6.29\n(6,8]\n\n\n0.62\n(0,2]\n\n\n\n\n\n\n\nUma tabela de frequência para estas observações é construída contando o número de observações por intervalo de classes. Neste caso:\n\ndf |&gt; \n  group_by(Classes) |&gt; \n  summarise(Frequencia = n()) |&gt; \n  gt()\n\n\n\n\n\n\n\nClasses\nFrequencia\n\n\n\n\n(0,2]\n1\n\n\n(2,4]\n3\n\n\n(4,6]\n1\n\n\n(6,8]\n2\n\n\n(8,10]\n3\n\n\n\n\n\n\n\nNa coluna Frequencia, temos o número de observações da variável X para cada um dos intervalos de classe.\n\n1.1 Alterando o tamanho dos intervalos de classe\nNo exemplo anterior, definimos os limites dos intervalos de classe de 2 em 2 unidades. Poderíamos ter escolhido outros tamanhos, por exemplo, de 4 em 4. Neste caso teríamos:\n\nClasses &lt;- cut(X, seq(0, 12, by = 4))\ndata.frame(X, Classes) |&gt; \n  group_by(Classes) |&gt; \n  summarise(Frequencia = n()) |&gt; \n  gt()\n\n\n\n\n\n\n\nClasses\nFrequencia\n\n\n\n\n(0,4]\n4\n\n\n(4,8]\n3\n\n\n(8,12]\n3\n\n\n\n\n\n\n\nNote que ao escolhermos o tamanho dos intervalos de classe, estamos criando a variável qualitativa ordinal Classes, a partir do agrupamento das observações em X. Neste sentido, não há um único tamanho correto para os intervalos de classe. O objetivo é encontrar um tamanho que permita evidenciar os padrões de distribuição da variável sem perdermos muitos detalhes.\nPoderíamos escolher um tamanho muito grande, de 5 em 5. Neste caso, teríamos somente 2 grupos.\n\nClasses &lt;- cut(X, seq(0, 10, by = 5))\ndata.frame(X, Classes) |&gt; \n  group_by(Classes) |&gt; \n  summarise(Frequencia = n()) |&gt; \n  gt()\n\n\n\n\n\n\n\nClasses\nFrequencia\n\n\n\n\n(0,5]\n4\n\n\n(5,10]\n6\n\n\n\n\n\n\n\nPor outro lado, poderíamos escolher um tamanho muito pequeno, por exemplo, de 1 em 1.\n\nClasses &lt;- cut(X, seq(0, 10, by = 1))\ndata.frame(X, Classes) |&gt; \n  group_by(Classes) |&gt; \n  summarise(Frequencia = n()) |&gt; \n  gt()\n\n\n\n\n\n\n\nClasses\nFrequencia\n\n\n\n\n(0,1]\n1\n\n\n(2,3]\n2\n\n\n(3,4]\n1\n\n\n(5,6]\n1\n\n\n(6,7]\n2\n\n\n(8,9]\n1\n\n\n(9,10]\n2\n\n\n\n\n\n\n\nNas duas situações, não é possível evidenciar os padrões de distribuição da variável X. Na primeira, perdemos muita informação agrupando as observações em somente duas faixas e, na última, perdemos a capacidade de visualizar os padrões de distribuição de X.\n\n\n1.2 Tabela de frequência para a CPUE\nImporte a base de dados Reservatorios_Parana_parcial.csv.\n\nres &lt;- read_delim(\n  file = \"https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Reservatorios_Parana_parcial.csv\",\n  delim = \",\",\n  locale = locale(decimal_mark = \".\", encoding = \"latin1\")\n)\n\nNo objeto res, temos 8 variáveis quantitativas: Fechamento, Area, pH, Condutividade, Alcalinidade, P.total, Riqueza, CPUE. Vamos verificar como fica uma tabela de frequências para a variável CPUE, que expressa a captura em \\(kg\\) de peixes em cada reservatório. Inicialmente, vamos selecionar somente esta coluna da tabela e visualizá-la em ordem crescente.\n\nsort(res$CPUE)\n\n [1]  2.05  2.43  4.01  4.71  5.60  5.95  6.29  7.35  7.51  7.75  7.95  9.22\n[13]  9.40 11.59 11.73 11.74 12.55 13.04 13.12 13.67 13.72 13.86 16.10 16.50\n[25] 17.95 20.83 20.92 21.82 24.88 28.73 30.76\n\n\nVemos que o menor valor é 2.05 \\(kg\\) e o maior 30.76 \\(kg\\). Assumindo que temos 31 observações, vamos criar um intervalo de classes de 5 em 5 unidades. Para isso, criaremos a variável cl_cpue, que será uma sequência de \\(0\\) a \\(35\\), com tamanho \\(5\\). Os valores nesta sequência são os limites de classe.\n\ncl_cpue &lt;- seq(from = 0, to = 35, by = 5)\ncl_cpue\n\n[1]  0  5 10 15 20 25 30 35\n\n\nUtilizaremos os limites de classe para gerar uma nova coluna, delimitando os intervalos a que cada observação pertence. Para isso, utilizaremos a função cut.\n\ntab_cpue &lt;- res |&gt; \n  select(CPUE) |&gt; \n  mutate(int_cpue = cut(CPUE, breaks = cl_cpue))\n\nE veremos a tabela em ordem crescente de classes para facilitar a identificação de padrões.\n\ntab_cpue |&gt; \n  arrange(CPUE) |&gt; \n  gt()\n\n\n\n\n\n\n\nCPUE\nint_cpue\n\n\n\n\n2.05\n(0,5]\n\n\n2.43\n(0,5]\n\n\n4.01\n(0,5]\n\n\n4.71\n(0,5]\n\n\n5.60\n(5,10]\n\n\n5.95\n(5,10]\n\n\n6.29\n(5,10]\n\n\n7.35\n(5,10]\n\n\n7.51\n(5,10]\n\n\n7.75\n(5,10]\n\n\n7.95\n(5,10]\n\n\n9.22\n(5,10]\n\n\n9.40\n(5,10]\n\n\n11.59\n(10,15]\n\n\n11.73\n(10,15]\n\n\n11.74\n(10,15]\n\n\n12.55\n(10,15]\n\n\n13.04\n(10,15]\n\n\n13.12\n(10,15]\n\n\n13.67\n(10,15]\n\n\n13.72\n(10,15]\n\n\n13.86\n(10,15]\n\n\n16.10\n(15,20]\n\n\n16.50\n(15,20]\n\n\n17.95\n(15,20]\n\n\n20.83\n(20,25]\n\n\n20.92\n(20,25]\n\n\n21.82\n(20,25]\n\n\n24.88\n(20,25]\n\n\n28.73\n(25,30]\n\n\n30.76\n(30,35]\n\n\n\n\n\n\n\nA nova tabela tab_cpue tem agora duas colunas: os valores numéricos de CPUE e os valores transformados em intervalos de classe, int_cpue. É com esta última que montaremos a tabela de frequência.\n\nfre_cpue &lt;- tab_cpue |&gt; \n  group_by(int_cpue) |&gt; \n  summarise(Frequencia = n())\n\nfre_cpue |&gt; \n  gt()\n\n\n\n\n\n\n\nint_cpue\nFrequencia\n\n\n\n\n(0,5]\n4\n\n\n(5,10]\n9\n\n\n(10,15]\n9\n\n\n(15,20]\n3\n\n\n(20,25]\n4\n\n\n(25,30]\n1\n\n\n(30,35]\n1\n\n\n\n\n\n\n\nE, em seguida, de frequência relativa:\n\nfre_cpue &lt;- fre_cpue |&gt; \n  mutate(Freq_relativa = Frequencia / sum(Frequencia))\n\nfre_cpue |&gt; \n  gt()\n\n\n\n\n\n\n\nint_cpue\nFrequencia\nFreq_relativa\n\n\n\n\n(0,5]\n4\n0.12903226\n\n\n(5,10]\n9\n0.29032258\n\n\n(10,15]\n9\n0.29032258\n\n\n(15,20]\n3\n0.09677419\n\n\n(20,25]\n4\n0.12903226\n\n\n(25,30]\n1\n0.03225806\n\n\n(30,35]\n1\n0.03225806\n\n\n\n\n\n\n\nVeja que os intervalos de (5,10] e (10,15] contêm o maior número de observações, cerca de 29% cada um, e que acima de \\(25\\) \\(kg\\) temos somente duas observações.\n\n\n1.3 Tabela de frequência acumulada\nOutra forma de representar o padrão de distribuição para uma variável quantitativa é apresentá-la em uma tabela de frequência acumulada. Fazemos isso somando de forma cumulativa as observações em cada classe de intervalo e criando duas colunas adicionais de frequência acumulada e de frequência relativa acumulada.\n\nfre_cpue &lt;- fre_cpue |&gt; \n  mutate(F_acumulada = cumsum(Frequencia),\n         FR_acumulada = cumsum(Freq_relativa))\n\nfre_cpue |&gt; \n  gt()\n\n\n\n\n\n\n\nint_cpue\nFrequencia\nFreq_relativa\nF_acumulada\nFR_acumulada\n\n\n\n\n(0,5]\n4\n0.12903226\n4\n0.1290323\n\n\n(5,10]\n9\n0.29032258\n13\n0.4193548\n\n\n(10,15]\n9\n0.29032258\n22\n0.7096774\n\n\n(15,20]\n3\n0.09677419\n25\n0.8064516\n\n\n(20,25]\n4\n0.12903226\n29\n0.9354839\n\n\n(25,30]\n1\n0.03225806\n30\n0.9677419\n\n\n(30,35]\n1\n0.03225806\n31\n1.0000000\n\n\n\n\n\n\n\nVeja agora que a última linha da coluna de frequência acumulada é igual ao número de observações total e que a da frequência relativa acumulada é igual a 1."
  },
  {
    "objectID": "conteudo/estatistica_descritiva/varquant.html#representação-gráfica-histogramas",
    "href": "conteudo/estatistica_descritiva/varquant.html#representação-gráfica-histogramas",
    "title": "Descrevendo variáveis quantitativas",
    "section": "2 Representação gráfica: histogramas",
    "text": "2 Representação gráfica: histogramas\nHistogramas são representações das tabelas de frequência e de frequência relativa. Um histograma da coluna CPUE pode ser feito com o comando:\n\nggplot(res, aes(x = CPUE)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nOs intervalos de classe foram escolhidos automaticamente pela função geom_histogram. Se quisermos ter o controle sobre estes intervalos, podemos adicionar o argumento breaks e a sequência com os limites de classe que criamos anteriormente:\n\nggplot(res, aes(x = CPUE)) +\n  geom_histogram(breaks = cl_cpue)\n\n\n\n\n\n\n\n\nA formatação do histograma acima pode ser melhorada de diversas formas, por exemplo:\n\nggplot(res, aes(x = CPUE, label = after_stat(count))) +\n  geom_histogram(breaks = cl_cpue, \n                 fill = 'darkblue', color = 'white') +\n  labs(x = 'Captura em kg', y = 'Frequência') +\n  geom_text(stat = \"bin\", size = 6, vjust = 1.5, color = 'white',\n            breaks = cl_cpue) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nModificamos a cor do preenchimento (fill = 'darkblue'), e identificamos as barras individualmente traçando uma linha branca entre elas (color = 'white');\nReescrevemos o rótulo dos eixos \\(x\\) e \\(y\\) (labs());\nIdentificamos as frequências em cada barra individualmente com o argumento label = after_stat(count) e a função geom_text;\nModificamos o tema do gráfico para obter uma alteração geral na aparência da figura. Existem diversos outros temas possíveis que podem ser vistos aqui.\n\nUm histograma com a frequência relativa pode ser obtido com:\n\nggplot(res, aes(x = CPUE,\n                y = after_stat(count)/sum(after_stat(count)),\n                label = round(after_stat(count)/sum(after_stat(count)), 2))) +\n  geom_histogram(breaks = cl_cpue, \n                 fill = 'darkblue', color = 'white') +\n  labs(x = 'Captura em kg', y = 'Frequência relativa') +\n  geom_text(stat = \"bin\", size = 6, vjust = 1.5, color = 'white',\n            breaks = cl_cpue) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAqui fizemos duas mudanças: + Inserimos o argumento y = after_stat(count)/sum(after_stat(count)) para dizer que as barras em \\(y\\) devem mostrar a contagem do número de observações em cada intervalo dividido pelo total; + Modificamos o argumento label = round(after_stat(count)/sum(after_stat(count)), 2) de modo que também mostre a frequência relativa, utilizando a função round.\n\n2.1 Representando frequências acumuladas\nA única modificação neste caso será identificarmos o eixo \\(y\\) por sua contagem acumulada: y = cumsum(after_stat(count)).\n\nggplot(res, aes(x = CPUE,\n                y = cumsum(after_stat(count)),\n                label = round(cumsum(after_stat(count)), 2))) +\n  geom_histogram(breaks = cl_cpue, \n                 fill = 'darkblue', color = 'white') +\n  labs(x = 'Captura em kg', y = 'Frequência acumulada') +\n  geom_text(stat = \"bin\", size = 6, vjust = 1.5, color = 'white',\n            breaks = cl_cpue) +\n  theme_classic()\n\n\n\n\n\n\n\n\nPara fazer o mesmo mostrando as frequências relativas, fazemos:\n\nggplot(res, aes(x = CPUE,\n                y = cumsum(after_stat(count)/sum(after_stat(count))),\n                label = round(cumsum(after_stat(count)/sum(after_stat(count))), 2))) +\n  geom_histogram(breaks = cl_cpue, \n                 fill = 'darkblue', color = 'white') +\n  labs(x = 'Captura em kg', y = 'Frequência acumulada relativa') +\n  geom_text(stat = \"bin\", size = 6, vjust = 1.5, color = 'white',\n            breaks = cl_cpue) +\n  theme_classic()"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html",
    "title": "O modelo da distribuição normal",
    "section": "",
    "text": "Bibliotecas utilizadas nesta seção\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as st"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html#simulando-uma-distribuição-de-probabilidade-normal",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html#simulando-uma-distribuição-de-probabilidade-normal",
    "title": "O modelo da distribuição normal",
    "section": "1 Simulando uma distribuição de probabilidade normal",
    "text": "1 Simulando uma distribuição de probabilidade normal\nVamos utilizar nosso modelo teórico de probabilidades (a distribuição normal) para prever o que seria esperado para as frequências relativas de alunos de diferentes alturas. Para isso precisamos calcular a probabilidade abaixo da curva para diferentes faixas de altura.\nEstritamente falando a equação da distribuição normal abaixo:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma} \\right)^2}, \\quad x \\in \\mathbb{R} \\mid -\\infty \\leq x \\leq +\\infty\n\\]\né a Função de Densidade de Probabilidade (PDF) da distribuição normal. Com base nesta equação, as probabilidades para intervalos de \\(X\\) são obtidas por meio da Função de Probabilidade Acumulada (CDF).\n\nmi = 170.94\nsigma = 6.86\n\nx = np.linspace(130, 210, 1000)\npdf = st.norm.pdf(x = x, loc = mi, scale = sigma)\n\ncdf = st.norm.cdf(x = x, loc = mi, scale = sigma)\n\nA distribuição normal com média \\(X = 170.94\\) e \\(\\sigma = 6.86\\) estão representadas abaixo (PDF - Figura 2 (a); CDF - Figura 2 (b)).\nplt.plot(x, pdf)\nplt.xlabel('Alturas (cm)')\nplt.ylabel('PDF')\nplt.show()\n\nplt.plot(x, cdf)\nplt.xlabel('Alturas (cm)')\nplt.ylabel('CDF')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Função de Densidade de Probabilidade (PDF)\n\n\n\n\n\n\n\n\n\n\n\n(b) Função de Probabilidade Acumulada (CDF).\n\n\n\n\n\n\n\nFigura 2: Distribuição normal de probabilidade"
  },
  {
    "objectID": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html#obtendo-probabilidades-de-uma-distribuição-normal",
    "href": "conteudo/distribuicao_normal/distribuicao_normal-probabilidade.html#obtendo-probabilidades-de-uma-distribuição-normal",
    "title": "O modelo da distribuição normal",
    "section": "2 Obtendo probabilidades de uma distribuição normal",
    "text": "2 Obtendo probabilidades de uma distribuição normal\n\n2.1 A probabilidade de \\(X\\) ser menor ou igual a \\(x_1\\): \\(P(X \\le x_1)\\)\nmi = 170.94\nsigma = 6.86\nx1 = 160\n\nx = np.linspace(130, 210, 1000)\n\npdf_y = st.norm.pdf(x = x, loc = mi, scale = sigma)\ncdf_y = st.norm.cdf(x = x, loc = mi, scale = sigma)\np = st.norm.cdf(x = x1, loc=mi, scale=sigma)\n\nplt.plot(x, pdf_y)\nplt.fill_between(x, pdf_y, where = (x &lt;= x1), color='lightblue')\nplt.title(f'$P(X \\leq {x1})$ = {np.round(p, 3)}')\nplt.show()\n\nplt.plot(x, cdf_y)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title(f'$F(X = {x1}$) = {np.round(p, 3)}')\nplt.plot([x1, x1], [0, p], color = 'red', linewidth = 3)\nplt.plot([130, x1], [p, p], color = 'red', linewidth = 3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Função de Densidade de Probabilidade (PDF)\n\n\n\n\n\n\n\n\n\n\n\n(b) Função de Probabilidade Acumulada (CDF).\n\n\n\n\n\n\n\nFigura 3: Distribuição Normal de Probabilidades.\n\n\n\n\n\n2.2 A probabilidade de \\(X\\) ser maior ou igual a \\(x_1\\): \\(P(X \\ge x_1)\\)\nmi = 170.94\nsigma = 6.86\nx1 = 180\n\nx = np.linspace(130, 210, 1000)\n\npdf_y = st.norm.pdf(x = x, loc = mi, scale = sigma)\ncdf_y = st.norm.cdf(x = x, loc = mi, scale = sigma)\np = st.norm.cdf(x = x1, loc=mi, scale=sigma)\n\nplt.plot(x, pdf_y)\nplt.fill_between(x, pdf_y, where = (x &gt;= x1), color='lightblue')\nplt.title(f'$P(X \\geq {x1})$ = {np.round(p, 3)}')\nplt.show()\n\nplt.plot(x, cdf_y)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title(f'$F(X = {x1}$) = {np.round(p, 3)}')\nplt.plot([x1, x1], [0, p], color = 'red', linewidth = 3)\nplt.plot([130, x1], [p, p], color = 'red', linewidth = 3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Função de Densidade de Probabilidade (PDF)\n\n\n\n\n\n\n\n\n\n\n\n(b) Função de Probabilidade Acumulada (CDF).\n\n\n\n\n\n\n\nFigura 4: Distribuição Normal de Probabilidades.\n\n\n\n\n\n2.3 A probabilidade de \\(X\\) estar entre \\(x_1\\) e \\(x_2\\): \\(P(x_1 \\le X \\le x_2)\\)\nmi = 170.94\nsigma = 6.86\nx1 = 160\nx2 = 180\n\nx = np.linspace(130, 210, 1000)\n\npdf_y = st.norm.pdf(x = x, loc = mi, scale = sigma)\ncdf_y = st.norm.cdf(x = x, loc = mi, scale = sigma)\np1 = st.norm.cdf(x = x1, loc=mi, scale=sigma)\np2 = st.norm.cdf(x = x2, loc=mi, scale=sigma)\np = p2 - p1\n\nplt.plot(x, pdf_y)\nplt.fill_between(x, pdf_y, where = ((x &gt;= x1) & (x &lt;= x2)), color='lightblue')\nplt.title(f'$P({x1} \\leq X \\leq {x2})$ = {np.round(p, 3)}')\nplt.show()\n\nplt.plot(x, cdf_y)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title(f'$F(X = {x1}$) = {np.round(p, 3)}')\nplt.title(f'$F(X = {x2}$) = {np.round(p, 3)}')\nplt.plot([x1, x1], [0, p1], color = 'red', linewidth = 3)\nplt.plot([130, x1], [p1, p1], color = 'red', linewidth = 3)\nplt.plot([x2, x2], [0, p2], color = 'red', linewidth = 3)\nplt.plot([130, x2], [p2, p2], color = 'red', linewidth = 3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Função de Densidade de Probabilidade (PDF)\n\n\n\n\n\n\n\n\n\n\n\n(b) Função de Probabilidade Acumulada (CDF).\n\n\n\n\n\n\n\nFigura 5: Distribuição Normal de Probabilidades.\n\n\n\n\n\n2.4 Representando \\(x_1\\) e \\(x_2\\) por \\(\\mu \\pm z\\sigma\\): \\(P(\\mu - z\\sigma \\le X \\le \\mu + z\\sigma)\\)\nObs.: \\(z\\) representa o número de desvios padrões acima ou abaixo de \\(\\mu\\).\n\nmi = 170.94\nsigma = 6.86\nz = 1.96\nx1 = mi - z * sigma\nx2 = mi + z * sigma\n\nx = np.arange(130, 210, 0.001)\ny = st.norm.pdf(x = x, loc = mi, scale = sigma)\n\np1 = st.norm.cdf(x = x1, loc=mi, scale=sigma)\np2 = st.norm.cdf(x = x2, loc=mi, scale=sigma)\np = p2 - p1\n\nplt.plot(x, y)\nplt.fill_between(x, y, where = ((x &gt;= x1) & (x &lt;= x2)), color='lightblue')\nplt.title(f'P($\\mu - {z}\\sigma \\leq X \\leq \\mu + {z}\\sigma$) = {np.round(p, 3)}')\nplt.show()\n\n\n\n\n\n\n\nFigura 6: Distribuição Normal de Probabilidades."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html",
    "title": "Construindo um modelo bayesiano",
    "section": "",
    "text": "Considere um globo representando o planeta Terra, pequeno o suficiente para caber em suas mãos. Seu objetivo é estimar a fração da superfície coberta por água. Para isso, você adota a seguinte estratégia: joga o globo para cima girando e, ao pegá-lo, registra se o ponto tocado pelo seu dedo indicador direito é água (🌊) ou terra (🏜️). Você repete esse procedimento algumas vezes, obtendo uma sequência de \\(n\\) observações.\nVocê faz quatro lançamentos do globo e conta quantos deles resultam em água. Um possível resultado seria \\(🌊🌊🏜️🌊\\), totalizando 3 observações de água e 1 de terra. Outro resultado possível é \\(🏜️🏜️🌊🌊\\), com 2 observações de água e 2 de terra. Para \\(n = 4\\) observações, existem 16 resultados possíveis (Tabela 1).\nObserve que apenas um dos resultados contém 4 observações de terra e somente um contém 4 observações de água. Os demais são variações entre esses extremos.\nPodemos reorganizar a tabela para evidenciar todas as combinações que levam ao mesmo número \\(y_i\\) de pontos em água:\nDefina \\(p\\) como a probabilidade de observar água e \\(1 - p\\) como a probabilidade de observar terra após cada lançamento do globo.\nA última linha da Tabela 2 (🌊🌊🌊🌊) tem probabilidade: \\[P(4) = p \\times p \\times p \\times p.\\]\nEnquanto a primeira linha (🏜️🏜️🏜️🏜️) ocorre com probabilidade: \\[P(0) = (1 - p) \\times (1 - p) \\times (1 - p) \\times (1 - p).\\]\nAs linhas correspondentes a \\(P(1)\\), \\(P(2)\\) e \\(P(3)\\) são combinações de \\(p\\) e \\((1 - p)\\), multiplicadas pelo número de formas pelas quais 1, 2 ou 3 registros de água podem ocorrer em 4 lançamentos."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#o-modelo-binomial",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#o-modelo-binomial",
    "title": "Construindo um modelo bayesiano",
    "section": "1 O modelo Binomial",
    "text": "1 O modelo Binomial\nA partir das expressões para \\(P(y)\\) apresentadas na Tabela 3, obtém-se uma fórmula geral que pode ser escrita como:\n\\[P(y \\mid n, p) = \\binom{n}{y} \\, p^y (1 - p)^{n - y}.\n\\tag{1}\\]\nOnde:\n\n\\(y \\in \\{0, 1, 2, \\dots, n\\}\\) é o número de observações de 🌊;\n\\(n\\) é o número total de observações;\n\\(p\\) é a fração de 🌊 que cobre o globo;\n\\(\\binom{n}{y}\\) é o coeficiente binomial, calculado por \\(\\frac{n!}{y!(n - y)!}\\), indicando de quantas maneiras a combinação \\(p^y (1 - p)^{n - y}\\) pode ocorrer.\n\nA Equação 1 fornece a probabilidade de cada resultado possível (número de observações 🌊) em \\(n\\) tentativas, permitindo calcular a probabilidade de todos os possíveis resultados do experimento."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#verossimilhança-a-plausibilidade-de-uma-hipótese",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#verossimilhança-a-plausibilidade-de-uma-hipótese",
    "title": "Construindo um modelo bayesiano",
    "section": "2 Verossimilhança: a plausibilidade de uma hipótese",
    "text": "2 Verossimilhança: a plausibilidade de uma hipótese\nA partir do modelo binomial, podemos definir a função de verossimilhança para um resultado observado. Imagine que, em \\(n = 4\\) lançamentos, foram observados \\(y = 2\\) pontos sobre a água. Não sabemos a verdadeira proporção \\(p\\) de água que cobre a Terra; portanto, fazemos conjecturas e avaliamos cada uma com base nas observações.\nPor exemplo, se supormos que a proporção verdadeira seja 40% \\((p = 0.4)\\), a distribuição binomial determina que a probabilidade de observar \\(y = 2\\) sucessos em \\(n = 4\\) lançamentos seja:\n\\[P(y = 2 \\mid n = 4, p = 0.4) = \\binom{4}{2} \\, 0.4^2 (1 - 0.4)^{4 - 2} = 0.35\\]\nEssa hipótese é apenas uma das possíveis. Para ilustrar outras conjecturas, considere:\n\nSe \\(p = 0.3\\):\n\\(P(2 \\mid 4, 0.3) = \\binom{4}{2} \\, 0.3^2 (1 - 0.3)^{4 - 2} = 0.26\\)\nSe \\(p = 0.8\\):\n\\(P(2 \\mid 4, 0.8) = \\binom{4}{2} \\, 0.8^2 (1 - 0.8)^{4 - 2} = 0.15\\)\n\nEm cada caso, os dados observados \\((y)\\) e o número total de observações \\((n)\\) estão fixos, enquanto o parâmetro \\(p\\) varia conforme a hipótese considerada. Embora a forma matemática seja idêntica à da função de probabilidade binomial, seu uso é diferente. Na função de probabilidade, lemos a probabilidade de \\(y\\) dado \\(n\\) e \\(p\\), enquanto nos exemplos acima, avaliamos a plausibilidade de diferentes hipóteses sobre \\(p\\) dados valores fixos de \\(y\\) e \\(n\\).\nPara evitar confusões, vamos definir a função de verossimilhança como:\n\\[\n\\mathcal{L}(p \\mid n, y) = \\binom{n}{y} \\, p^y (1 - p)^{n - y}.\n\\tag{2}\\]\nAssim, as verossimilhanças para as três conjecturas específicas sobre a proporção de água na superfície do globo serão:\n\n\\(\\mathcal{L}(p = 0.4 \\mid n = 4, y = 2) = 0.35\\),\n\\(\\mathcal{L}(p = 0.3 \\mid n = 4, y = 2) = 0.26\\),\n\\(\\mathcal{L}(p = 0.8 \\mid n = 4, y = 2) = 0.15\\).\n\nDessa forma, entre as três hipóteses levantadas, aquela em que \\(p = 0.4\\) recebe maior suporte das evidências, por estar associada à maior verossimilhança.\nPodemos quantificar esse suporte por meio da razão de verossimilhanças:\n\\[RV = \\frac{\\mathcal{L}(p = 0.4 \\mid 4, 2)}{\\mathcal{L}(p = 0.3 \\mid 4, 2)} = \\frac{0.35}{0.26} = 1.35,\\]\no que indica que, com base nos dados observados, a hipótese de \\(p = 0.4\\) é aproximadamente \\(1.35\\) vezes mais verossímil do que a hipótese de \\(p = 0.3\\).\n\n\n\n\n\n\nResumo: A Função de Verossimilhança Binomial\n\n\n\n\nA expressão é matematicamente idêntica à função de probabilidade binomial, porém interpretada como uma função de \\(p\\) quando os dados \\(Y\\) e \\(n\\) são fixos.\nA verossimilhança indica a plausibilidade de diferentes valores de \\(p\\) à luz dos dados observados.\nNa distribuição binomial, lemos: probabilidade de \\(Y\\) dado \\(n\\) e \\(p\\).\nNa função de verossimilhança, interpretamos: verossimilhança de \\(p\\) dado \\(n\\) e \\(Y\\).\nA razão de verossimilhanças pode ser utilizada para quantificar o suporte relativo entre diferentes hipóteses.\n\n\n\n\n2.1 O perfil de verossimilhança\nAcima, foram testadas três conjecturas específicas para a proporção de água na superfície da Terra (\\(p = 0.3\\), \\(p = 0.4\\), \\(p = 0.8\\)). Para uma avaliação mais completa, podemos analisar o perfil de verossimilhança para uma série de valores de \\(p\\) entre 0 e 1:\n\n\n\n\n\n\n\n\n\nO perfil de verossimilhança indica que, à luz dos nossos dados \\(y = 2\\), a conjectura mais plausível é que a proporção de água que cobre a Terra esteja próxima de 0.5 (neste caso, a verossimilhança máxima é exatamente para \\(p = 0.5\\))."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#inferência-bayesiana-distribuições-a-priori-e-a-posteriori",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#inferência-bayesiana-distribuições-a-priori-e-a-posteriori",
    "title": "Construindo um modelo bayesiano",
    "section": "3 Inferência Bayesiana: distribuições a priori e a posteriori",
    "text": "3 Inferência Bayesiana: distribuições a priori e a posteriori\nA inferência bayesiana utiliza o Teorema de Bayes para derivar a distribuição a posteriori dos parâmetros de interesse, \\(p(\\theta \\mid Y)\\), a partir da verossimilhança \\(p(Y \\mid \\theta)\\) e da distribuição a priori \\(p(\\theta)\\):\n\\[p(\\theta \\mid Y) = \\frac{p(Y \\mid \\theta) \\times p(\\theta)}{p(Y)} \\tag{3}\\]\nEm que:\n\n\\(p(\\theta \\mid Y)\\): distribuição a posteriori de \\(\\theta\\) dado os dados observados \\(Y\\);\n\\(p(Y \\mid \\theta)\\): verossimilhança dos dados \\(Y\\) dada \\(\\theta\\);\n\\(p(\\theta)\\): distribuição a priori de \\(\\theta\\);\n\\(p(Y)\\): probabilidade marginal dos dados, obtida por\n\n\\[\\int p(Y \\mid \\theta) \\times p(\\theta) \\, d\\theta\\]\nNo contexto bayesiano, é comum substituir \\(p(Y \\mid \\theta)\\) pela função de verossimilhança \\(\\mathcal{L}(\\theta \\mid Y)\\), pois ambas são matematicamente equivalentes. Assim, a fórmula da distribuição a posteriori pode ser reescrita como:\n\\[p(\\theta \\mid Y) = \\frac{\\mathcal{L}(\\theta \\mid Y) \\times p(\\theta)}{p(Y)} \\tag{4}\\]"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#priori-informativa-e-não-informativa",
    "href": "conteudo/intro_bayes/intro-bayes-modelo-bayesiano.html#priori-informativa-e-não-informativa",
    "title": "Construindo um modelo bayesiano",
    "section": "4 Priori informativa e não informativa",
    "text": "4 Priori informativa e não informativa\nDenominamos priori não informativa aquela que não acrescenta informações relevantes à distribuição a posteriori além daquelas já contidas nos dados observados. Nesses casos, a distribuição a posteriori é proporcional apenas à verossimilhança:\n\\[p(\\theta \\mid Y) \\propto \\mathcal{L}(\\theta \\mid Y) \\tag{5}\\]\nPor outro lado, ao adotarmos uma priori informativa, atribuímos diferentes densidades de probabilidade às regiões específicas do espaço de parâmetros, refletindo o conhecimento prévio sobre o fenômeno estudado. A distribuição a posteriori, nesse caso, será proporcional ao produto entre a verossimilhança e a priori, integrando evidências anteriores com a informação contida nos dados:\n\\[p(\\theta \\mid Y) \\propto \\mathcal{L}(\\theta \\mid Y) \\times p(\\theta) \\tag{6}\\]\nNo modelo binomial aplicado à proporção de água na superfície oceânica, o parâmetro \\(\\theta\\) representa a proporção de água \\(p\\), e sua distribuição posterior é condicional ao número de observações \\(n\\) e aos dados observados \\(y\\).\nA distribuição a priori para \\(p\\) pode ser não informativa, como no caso da distribuição uniforme, que não favorece nenhum valor específico de \\(p\\). Alternativamente, pode-se adotar uma priori informativa, como a distribuição Beta, que permite ajustar a forma da densidade de probabilidade por meio dos parâmetros \\(\\alpha\\) e \\(\\beta\\), incorporando conhecimento prévio sobre o fenômeno de interesse.\nPara ilustrar o efeito de prioris informativas e não-informativas sobre a distribuição a posteriori, siga a atividade abaixo:\n\n\n\n\n\n\nAtividades interativas: estimando a proporção da superfície oceânica!\n\n\n\n\nAmostre pontos no globo e faça sua própria inferência bayesiana\nNo app abaixo, gere pontos aleatórios na superfície da Terra e verifique quantos caem em água ou em terra.\n👉 Estimando a Proporção da Superfície Oceânica\n\nEscolha quantos pontos deseja amostrar (1 a 1000).\n\nClique em “Gerar Pontos Aleatórios” e observe quantos ficam sobre a água versus sobre a terra.\n\nRegistre esses valores como \\(k\\) sucessos em \\(N\\) pontos (N observações).\n\nUtilize seus dados na inferência Bayesiana\nEm seguida, abra o app abaixo para visualizar como as observações (sucessos e fracassos) combinadas a diferentes escolhas de parâmetros a priori (\\(\\alpha\\), \\(\\beta\\)) geram a distribuição a posteriori:\n👉 Inferência Bayesiana\nDicas de uso\n\nInsira o mesmo número de observações (N) e sucessos (k) obtidos no primeiro app.\n\nAjuste interativamente a distribuição a priori Beta, modificando os parâmetros \\(\\alpha\\) e \\(\\beta\\).\nObserve como a curva azul (“posteriori”) se altera de acordo com a a priori e com os dados observados, e compare com o perfil de verossimilhança (curva verde).\nNote que ao escolher \\(\\alpha = 1\\) e \\(\\beta = 1\\) a distribuição assume um formato uniforme, tornando-se não-informativa."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-counting.html",
    "href": "conteudo/intro_bayes/intro-bayes-counting.html",
    "title": "Contando possibilidades",
    "section": "",
    "text": "A inferência bayesiana, em essência, é uma forma de contar e comparar as diferentes maneiras pelas quais algo pode acontecer. A seguir, vamos desenvolver os princípios da inferência bayesiana de forma simples e intuitiva utilizando o princípio da contagem.\nImagine que temos uma caixa contendo quatro bolinhas de gude, que podem ser azuis ou brancas. Sabemos que há exatamente quatro bolinhas, mas não conhecemos a distribuição entre as cores. Com base nessa informação, podemos listar cinco configurações possíveis:\nEssas são todas as possibilidades compatíveis com o que sabemos sobre o conteúdo da caixa — o conhecimento a priori. Chamamos essas cinco configurações de hipóteses.\nNosso objetivo será descobrir qual dessas hipóteses é mais plausível à medida que obtivermos evidências sobre o conteúdo da caixa."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-counting.html#um-leque-de-possibilidades",
    "href": "conteudo/intro_bayes/intro-bayes-counting.html#um-leque-de-possibilidades",
    "title": "Contando possibilidades",
    "section": "1 Um leque de possibilidades",
    "text": "1 Um leque de possibilidades\nA caixa possui um orifício pelo qual podemos ver apenas uma bolinha por vez. Assim, a única forma de obter evidências sobre o conteúdo da caixa será fazer uma observação, misturar as bolinhas, fazer outra observação e assim por diante. Antes de iniciar esse processo, vamos entender como cada observação nos ajuda a alcançar nosso objetivo, avaliando-as à luz das hipóteses sobre o conteúdo da caixa.\nVamos começar assumindo que seja verdadeira a situação (2) [🔵⚪⚪⚪]. Nesse caso, teríamos 1 possibilidade de observar a bolinha azul e 3 possibilidades de observar uma bolinha branca (Figura 1).\n\n\n\n\n\n\nFigura 1: As quatro possibilidades, assumindo que existam três bolinhas brancas e uma azul. Extraído de McElreath (2018).\n\n\n\n\n\n\n\n\n\nDica útil\n\n\n\nObserve que, embora as três bolinhas brancas pareçam iguais do ponto de vista dos dados (pois apenas registramos suas cores), elas são eventos diferentes. Isso é importante, pois significa que há três maneiras a mais de observar ⚪ do que 🔵.\n\n\nObservamos agora uma segunda bolinha. Isso expande nosso leque de possibilidades em mais uma camada (Figura 2). Agora existem 16 caminhos possíveis (um para cada par de observações), pois, na segunda observação, cada um dos caminhos anteriores se ramifica em outros quatro caminhos possíveis.\n\n\n\n\n\n\nFigura 2: Os 16 caminhos possíveis, assumindo que existam três bolinhas brancas e uma azul. Extraído de McElreath (2018).\n\n\n\nAo observar uma terceira bolinha da caixa, a terceira camada é construída da mesma forma, e agora temos \\(4^3 = 64\\) caminhos possíveis para uma sequência de observações de cores em uma caixa com 4 bolinhas (Figura 3).\n\n\n\n\n\n\nFigura 3: Os 64 caminhos possíveis, assumindo que existam três bolinhas brancas e uma azul. Extraído de McElreath (2018).\n\n\n\n\n\n\n\n\n\nPressuposto Importante\n\n\n\nAcreditamos que, ao sacudir a caixa, cada bolinha tem a mesma chance de ser observada pelo orifício, independentemente de qual tenha saído anteriormente. Por isso, cada caminho do leque é igualmente provável de ser observado."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-counting.html#avaliando-as-evidências",
    "href": "conteudo/intro_bayes/intro-bayes-counting.html#avaliando-as-evidências",
    "title": "Contando possibilidades",
    "section": "2 Avaliando as evidências",
    "text": "2 Avaliando as evidências\nÀ medida que observamos a cor de uma nova bolinha da caixa, alguns desses caminhos são logicamente eliminados.\nSuponha que a sequência de cores observada tenha sido:\n1ª bolinha: 🔵\n2ª bolinha: ⚪\n3ª bolinha: 🔵\nApós a primeira retirada resultar em 🔵, os três caminhos que levariam à observação de uma bolinha branca na primeira camada são imediatamente eliminados. Na segunda retirada, obtivemos ⚪, de modo que um dos caminhos possíveis na segunda camada foi eliminado, restando os três caminhos que se ramificam a partir do primeiro caminho azul. Após a terceira observação, cada um dos três caminhos restantes na segunda camada segue somente para a bolinha azul na terceira camada. Assim, assumindo que a caixa contenha [🔵⚪⚪⚪], existe um total de três maneiras para a sequência [🔵 → ⚪ → 🔵] aparecer. Todas as outras possibilidades foram descartadas à medida que as evidências surgiam.\nDos caminhos restantes na Figura 4, não podemos ter certeza de qual dos três caminhos os dados reais seguiram, pois não podemos identificar as bolinhas individualmente, apenas por sua cor. Entretanto, considerando a hipótese de que a caixa contenha [🔵⚪⚪⚪], podemos afirmar que os dados seguiram um desses três caminhos, pois são os únicos compatíveis tanto com nosso conhecimento prévio (4 bolinhas, azuis ou brancas) quanto com a sequência de dados observada ([🔵 → ⚪ → 🔵]).\n\n\n\n\n\n\nFigura 4: Após eliminar caminhos inconsistentes com a sequência observada, apenas 3 dos 64 caminhos permanecem. Extraído de McElreath (2018)."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-counting.html#avaliando-as-demais-hipóteses",
    "href": "conteudo/intro_bayes/intro-bayes-counting.html#avaliando-as-demais-hipóteses",
    "title": "Contando possibilidades",
    "section": "3 Avaliando as demais hipóteses",
    "text": "3 Avaliando as demais hipóteses\nConsiderando que a caixa contenha [🔵⚪⚪⚪], verificamos que apenas três dos 64 caminhos possíveis poderiam gerar a sequência [🔵 → ⚪ → 🔵]. Falta agora aplicar a mesma lógica às demais hipóteses. Por exemplo, considere [⚪⚪⚪⚪]. Há zero maneiras de essa hipótese produzir os dados observados, pois uma única 🔵 já é logicamente incompatível com ela. A hipótese [🔵🔵🔵🔵] também não pode produzir a sequência, pois há ao menos uma ⚪ observada. Assim, podemos eliminar essas duas hipóteses, pois nenhuma delas fornece sequer um único caminho consistente com os dados.\nPara as hipóteses restantes, isto é, [🔵🔵⚪⚪] e [🔵🔵🔵⚪], o leque de possibilidades se abre novamente.\nA Figura 5 mostra o leque completo para as três hipóteses compatíveis com os dados observados: [🔵⚪⚪⚪], [🔵🔵⚪⚪] e [🔵🔵🔵⚪].\n\n\n\n\n\n\nFigura 5: Caminhos de composição possível para cada hipótese logicamente compatível com a sequência observada. Extraído de McElreath (2018).\n\n\n\nAgora, contamos todas as maneiras pelas quais cada hipótese poderia produzir os dados observados. Para uma bolinha azul e três brancas, existem três maneiras (como já contamos). Para duas bolinhas azuis e duas brancas, há oito caminhos consistentes com a sequência. Para três bolinhas azuis e uma branca, há nove caminhos que sobrevivem às observações.\nConsideramos, assim, as cinco hipóteses diferentes sobre o conteúdo da caixa, variando de zero bolinhas 🔵 a quatro bolinhas 🔵 e, para cada uma dessas hipóteses, contamos quantas possibilidades (ou “caminhos”) poderiam potencialmente produzir a sequência observada.\n\n\n\nTabela 1: Total de maneiras pelas quais cada hipótese pode gerar a sequência [🔵 → ⚪ → 🔵].\n\n\n\n\n\nHipótese\nManeiras de produzir [🔵 → ⚪ → 🔵]\n\n\n\n\n1. [⚪⚪⚪⚪]\n\\(0 \\times 4 \\times 0 = 0\\)\n\n\n2. [🔵⚪⚪⚪]\n\\(1 \\times 3 \\times 1 = 3\\)\n\n\n3. [🔵🔵⚪⚪]\n\\(2 \\times 2 \\times 2 = 8\\)\n\n\n4. [🔵🔵🔵⚪]\n\\(3 \\times 1 \\times 3 = 9\\)\n\n\n5. [🔵🔵🔵🔵]\n\\(4 \\times 0 \\times 4 = 0\\)\n\n\n\n\n\n\nObserve que o número de maneiras de produzir os dados, para cada hipótese, pode ser obtido contando as ramificações em cada camada do leque de possibilidades e, em seguida, multiplicando esses valores (Tabela 1). Isso é apenas um recurso computacional. Ele nos diz a mesma coisa que a Figura 5, mas sem precisar desenhar todo o diagrama. O fato de multiplicarmos os números não altera o sentido de estarmos apenas contando caminhos logicamente possíveis."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-counting.html#atualizando-o-conhecimento",
    "href": "conteudo/intro_bayes/intro-bayes-counting.html#atualizando-o-conhecimento",
    "title": "Contando possibilidades",
    "section": "4 Atualizando o conhecimento",
    "text": "4 Atualizando o conhecimento\nSuponha que o experimento anterior, sumarizado na Tabela 1, tenha sido finalizado. Isso nos diz que, por ora, temos evidências melhores para as hipóteses 3 e 4, isto é, de que a caixa contenha 2 ou 3 bolinhas azuis. Para ajudar a diferenciar essas duas possibilidades ainda mais, resolvemos continuar o experimento e amostrar outra bolinha, o que resultou na observação de uma bolinha azul. Como se trata de um novo experimento, poderíamos recomeçar todo o processo. No entanto, há uma forma melhor de aproveitar o conhecimento adquirido a priori — para cada hipótese, listamos as maneiras anteriores de produzir as observações (o prior) e multiplicamos pelo número de maneiras de produzir a nova evidência 🔵:\n\n\n\nTabela 2: Total de maneiras pelas quais cada hipótese pode gerar a sequência completa [🔵 → ⚪ → 🔵 → 🔵], combinando a contagem anterior com a nova evidência.\n\n\n\n\n\n\n\n\n\n\n\nHipótese\nContagem anterior (prior)\nManeiras de produzir a nova observação 🔵\nContagem posterior\n\n\n\n\n1. [⚪⚪⚪⚪]\n0\n0\n\\(0 \\times 0 = 0\\)\n\n\n2. [🔵⚪⚪⚪]\n3\n1\n\\(3 \\times 1 = 3\\)\n\n\n3. [🔵🔵⚪⚪]\n8\n2\n\\(8 \\times 2 = 16\\)\n\n\n4. [🔵🔵🔵⚪]\n9\n3\n\\(9 \\times 3 = 27\\)\n\n\n5. [🔵🔵🔵🔵]\n0\n4\n\\(0 \\times 4 = 0\\)\n\n\n\n\n\n\nA nova contagem na coluna da direita da Tabela 2 resume as evidências a favor de cada hipótese, de modo que sejam compatíveis tanto com as observações anteriores quanto com a nova observação. Portanto, à medida que novos dados chegam e, desde que sejam independentes dos anteriores, o total de caminhos logicamente possíveis para explicar tanto as observações antigas quanto as novas pode ser calculado pela multiplicação das contagens anteriores pelas novas.\nEm outras palavras, sempre que temos \\(W_\\text{prior}\\) maneiras de uma hipótese produzir observações anteriores (\\(D_\\text{prior}\\)) e, em seguida, obtemos novas observações (\\(D_\\text{novo}\\)) que essa mesma hipótese pode produzir de \\(W_\\text{novo}\\) maneiras, a quantidade total de formas possíveis para essa hipótese explicar tanto os dados antigos quanto os novos é dada simplesmente por \\(W_\\text{prior} \\times W_\\text{novo}\\). Por exemplo, na Tabela 2, a hipótese [🔵🔵⚪⚪] apresenta \\(W_\\text{prior} = 8\\) maneiras de gerar as observações anteriores [🔵 → ⚪ → 🔵] e \\(W_\\text{novo} = 2\\) maneiras de gerar a nova observação [🔵]. Logo, \\(8 \\times 2 = 16\\) caminhos possíveis para explicar tanto os dados antigos quanto os novos.\n\n\n\n\n\n\nCombinando evidências\n\n\n\nNo exemplo acima, os dados antigos e os novos são do mesmo tipo (bolinhas observadas na caixa). Entretanto, não há motivo para excluir a situação em que os dados antigos e os novos tenham sido obtidos de forma diferente. Suponha, por exemplo, que alguém da fábrica de bolinhas informe que as azuis são raras. Para cada caixa contendo [🔵🔵🔵⚪], a fábrica produz duas caixas contendo [🔵🔵⚪⚪] e três caixas contendo [🔵⚪⚪⚪]. Também garante que cada caixa contenha pelo menos uma bolinha azul e uma bolinha branca. Com essa nova informação, podemos atualizar nossas contagens novamente (Tabela 3).\n\n\n\nTabela 3: Contagens atualizadas após incorporar a nova observação 🔵 e as informações externas sobre a frequência das hipóteses.\n\n\n\n\n\n\n\n\n\n\n\nHipótese\nContagem anterior (prior)\nManeiras de produzir 🔵 informadas pela fábrica\nNova contagem\n\n\n\n\n1. [⚪⚪⚪⚪]\n0\n0\n\\(0 \\times 0 = 0\\)\n\n\n2. [🔵⚪⚪⚪]\n3\n3\n\\(3 \\times 3 = 9\\)\n\n\n3. [🔵🔵⚪⚪]\n16\n2\n\\(16 \\times 2 = 32\\)\n\n\n4. [🔵🔵🔵⚪]\n27\n1\n\\(27 \\times 1 = 27\\)\n\n\n5. [🔵🔵🔵🔵]\n0\n0\n\\(0 \\times 0 = 0\\)\n\n\n\n\n\n\nAgora, à luz dessa informação adicional, a hipótese [🔵🔵⚪⚪] torna-se ligeiramente mais plausível do que [🔵🔵🔵⚪]."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-grid.html",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-grid.html",
    "title": "Inferência Bayesiana Binomial",
    "section": "",
    "text": "A estratégia de inferência via grid consiste em discretizar o parâmetro \\(p\\) em pequenos intervalos, avaliando a distribuição a priori e a verossimilhança em cada ponto da grade. Em seguida, multiplica-se esses valores e normaliza-se o resultado para obter a distribuição a posteriori."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#aproximação-bayesiana-via-grid",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#aproximação-bayesiana-via-grid",
    "title": "Inferência Bayesiana Binomial",
    "section": "1 Aproximação Bayesiana via Grid",
    "text": "1 Aproximação Bayesiana via Grid\n\nDefinir os dados\n\n\\(N\\): número total de observações (ensaios Bernoulli).\n\n\\(k\\): número de sucessos observados.\n\nCriar a malha (grid) de valores para \\(p\\)\n\nDivida o intervalo \\([0, 1]\\) em muitos pontos (ex. 1000 pontos).\n\nCada ponto será uma hipótese para o valor de \\(p\\).\n\nAvaliar a priori\n\nEscolha uma forma para a distribuição a priori de \\(p\\).\n\nExemplos: uma Beta(\\(\\alpha, \\beta\\)) ou mesmo uma priori uniforme.\n\nCalcule a densidade da priori em cada ponto do grid.\n\nCalcular a verossimilhança\n\nPara cada valor de \\(p\\) no grid, calcule \\(P(Y = k \\mid p, N)\\) usando a distribuição Binomial:\n\\[\\mathcal{L}(p) = \\binom{N}{k}\\, p^k (1-p)^{N-k}.\\]\nUse por exempo o método binom.pmf(k, N, p) do módulo SciPy ou escreva a fórmula manualmente.\n\nCombinar priori e verossimilhança\n\nA posteriori não normalizada em cada ponto do grid é:\n\\[\\text{posterior}_{\\text{unnorm}}(p) = \\text{prior}(p) \\times \\mathcal{L}(p).\\]\n\nNormalizar a posteriori\n\nSome os valores de \\(\\text{posterior}_{\\text{unnorm}}(p)\\) sobre todos os pontos \\(p\\).\n\nDivida cada valor pela soma total (use integração, como scipy.integrate.simpson para maior precisão).\n\nO resultado é a distribuição a posteriori discreta (aproximada).\n\nCalcular probabilidades de intervalo\n\nPara calcular \\(P(x_1 \\leq p \\leq x_2)\\), some (ou integre) os valores da posteriori nos pontos entre \\(x_1\\) e \\(x_2\\).\n\nVisualizar os resultados\n\nFaça gráficos do perfil da priori, da verossimilhança e da posteriori ao longo do grid de \\(p\\).\n\nDestaque intervalos de interesse (\\(x_1, x_2\\)) e use os valores de probabilidade para estimar o valor de \\(p\\)."
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#exemplo-em-python",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#exemplo-em-python",
    "title": "Inferência Bayesiana Binomial",
    "section": "2 Exemplo em Python",
    "text": "2 Exemplo em Python\nA seguir, um exemplo completo usando numpy e matplotlib para ilustrar cada etapa. Ajuste os valores de \\(N\\), \\(k\\), \\(\\alpha\\) e \\(\\beta\\) conforme necessário.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, beta\nfrom scipy.integrate import simpson\n\n# Parâmetros do experimento\nN = 10   # número total de ensaios\nk = 6    # número de sucessos observados\n\n# Parâmetros da priori Beta\nalpha_param = 1\nbeta_param = 1\n\n# Grid de p (1000 pontos entre 0 e 1)\np_grid = np.linspace(0, 1, 1000)\n\n# 1) Prior: densidade Beta em cada ponto do grid\nprior = beta.pdf(p_grid, a=alpha_param, b=beta_param)\n\n# 2) Verossimilhança: Binomial(k | N, p)\nlikelihood = binom.pmf(k, N, p_grid)\n\n# 3) Posterior não normalizada\nposterior_unnorm = prior * likelihood\n\n# 4) Normaliza para obter a posteriori propriamente dita\narea = simpson(y=posterior_unnorm, x=p_grid)  # integra usando Simpson\nposterior = posterior_unnorm / area\n\n# 5) (Opcional) Calcular probabilidade de um intervalo [x1, x2]\nx1, x2 = 0.4, 0.7\nmask_interval = (p_grid &gt;= x1) & (p_grid &lt;= x2)\nprob_interval = simpson(y=posterior[mask_interval], x=p_grid[mask_interval])\n\n# Visualizar\nfig, axs = plt.subplots(3, 1, figsize=(6, 8))\n\n# Plot da Prior\naxs[0].plot(p_grid, prior, color='red')\naxs[0].set_title(\"Priori Beta\")\naxs[0].set_ylabel(\"Densidade\")\n\n# Plot da Verossimilhança\naxs[1].plot(p_grid, likelihood, color='green')\naxs[1].set_title(f\"Verossimilhança Binomial (k={k}, N={N})\")\naxs[1].set_ylabel(\"PMF\")\n\n# Plot da Posterior\naxs[2].plot(p_grid, posterior, color='blue')\naxs[2].set_title(f\"Posteriori - Prob({x1:.2f} ≤ p ≤ {x2:.2f}) = {prob_interval:.3f}\")\naxs[2].set_xlabel(\"p\")\naxs[2].set_ylabel(\"Densidade\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#interpretação",
    "href": "conteudo/intro_bayes/intro-bayes-binomial-grid.html#interpretação",
    "title": "Inferência Bayesiana Binomial",
    "section": "3 Interpretação",
    "text": "3 Interpretação\n\nObserve como a forma da posteriori (curva azul) é proporcional ao produto da priori (vermelho) pela verossimilhança (verde).\nSe a priori for \\(Beta(1,1)\\) (uniforme), a posteriori fica essencialmente guiada pelos dados.\nAlterar \\(\\alpha\\) e \\(\\beta\\) faz a priori pesar mais (ou menos) no resultado final, dependendo de quão informativa ela é e do tamanho amostral \\(N\\)."
  },
  {
    "objectID": "distribuicao_normal.html",
    "href": "distribuicao_normal.html",
    "title": "A Distribuição Normal",
    "section": "",
    "text": "O modelo de distribuição normal\n\n\nIntrodução à distribuição normal, suas características e aplicações no contexto da inferência estatística.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO modelo da distribuição normal\n\n\nApresenta o modelo matemático da distribuição normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO modelo da distribuição normal\n\n\nExplora a distribuição normal para extrair probabilidades\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "",
    "text": "Nenhum item correspondente"
  },
  {
    "objectID": "index.html#fundamentos-de-programação-e-análise-de-dados",
    "href": "index.html#fundamentos-de-programação-e-análise-de-dados",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "",
    "text": "Nenhum item correspondente"
  },
  {
    "objectID": "index.html#estatística-descritiva-e-probabilidade",
    "href": "index.html#estatística-descritiva-e-probabilidade",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "Estatística Descritiva e Probabilidade",
    "text": "Estatística Descritiva e Probabilidade\n\n\n\n\n\n\n\n\n\n\nEstrutura de Dados\n\n\n\n\n\n\n\n\n\n\n\n\nEstatística Descritiva\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas de Associação\n\n\n\n\n\n\n\n\n\n\n\n\nAmostragem\n\n\n\n\n\n\n\n\n\n\n\n\nA Distribuição Normal\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos de Probabilidades\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html#inferência-estatística-e-teste-de-hipóteses",
    "href": "index.html#inferência-estatística-e-teste-de-hipóteses",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "Inferência Estatística e Teste de hipóteses",
    "text": "Inferência Estatística e Teste de hipóteses\n\n\n\n\n\n\n\n\n\n\nInferência Estatística\n\n\n\n\n\n\n\n\n\n\n\n\nTeste de Hipóteses\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html#modelos-lineares",
    "href": "index.html#modelos-lineares",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "Modelos Lineares",
    "text": "Modelos Lineares\n\n\n\n\n\n\n\n\n\n\nAnálise de variância\n\n\n\n\n\n\n\n\n\n\n\n\nModelos de Regressão\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html#modelos-bayesianos",
    "href": "index.html#modelos-bayesianos",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "Modelos Bayesianos",
    "text": "Modelos Bayesianos\n\n\n\n\n\n\n\n\n\n\nIntrodução à Inferência Bayesiana\n\n\n\n\n\n\n\n\n\n\n\n\nModelos de Regressão Bayesianos\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html#ecologia-numérica",
    "href": "index.html#ecologia-numérica",
    "title": "Modelagem Estatística e Análise de Dados",
    "section": "Ecologia Numérica",
    "text": "Ecologia Numérica\n\n\n\n\n\n\n\n\n\n\nIntrodução à Ecologia Numérica\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "ecologia-numerica.html",
    "href": "ecologia-numerica.html",
    "title": "Introdução à Ecologia Numérica",
    "section": "",
    "text": "Introdução à Álgebra de Matrizes\n\n\nOperações básicas e álgebra matricial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcologia Funcional: aplicação da álgebra matricial\n\n\n\nProduto escalar\n\nÂngulo entre vetores\n\nMultiplicação matricial\n\nMatriz transposta\n\nMatriz simétrica\n\n\n\nCálculo da similaridade funcional entre espécies de peixes usando álgebra matricial.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "medidas_associacao.html",
    "href": "medidas_associacao.html",
    "title": "Medidas de Associação",
    "section": "",
    "text": "Associação entre duas variáveis qualitativas\n\n\n\nEstatística\n\nAnálise qualitativa\n\nTabelas de contingência\n\nMedidas de associação\n\n\n\nAnálise da associação entre variáveis qualitativas, uso de tabelas de contingência e estatísticas de associação.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociação entre duas variáveis quantitativas\n\n\n\nEstatística\n\nAnálise quantitativa\n\nCovariância\n\nCorrelação\n\nMedidas de associação\n\n\n\nAnálise da associação entre variáveis quantitativas, com destaque para covariância e correlação de Pearson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociação entre variáveis quantitativas e qualitativas\n\n\n\nEstatística\n\nANOVA\n\nAnálise quantitativa\n\nAnálise qualitativa\n\nMedidas de associação\n\n\n\nAnálise da relação entre variáveis quantitativas e categóricas, considerando partição da soma de quadrados e coeficientes de determinação.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "estatistica_descritiva.html",
    "href": "estatistica_descritiva.html",
    "title": "Estatística Descritiva",
    "section": "",
    "text": "Descrevendo variáveis qualitativas\n\n\n\nR\n\nAnálise de dados\n\nEstatística descritiva\n\nVariáveis qualitativas\n\nVisualização de dados\n\n\n\nDescrição de variáveis qualitativas, incluindo tabelas de frequência, gráficos de barras e considerações sobre variáveis ordinais.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescrevendo variáveis quantitativas\n\n\n\nR\n\nCiência de dados\n\nAnálise de dados\n\nEstatística descritiva\n\nVariáveis quantitativas\n\nDistribuição de frequência\n\n\n\nExploração de variáveis quantitativas por meio de tabelas de frequência, histogramas e gráficos de frequência acumulada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas de tendência central\n\n\n\nR\n\nAnálise de dados\n\nEstatística descritiva\n\nTendência central\n\nDistribuição de dados\n\n\n\nDiscussão das principais medidas de tendência central (média, mediana, moda) e sua interpretação em diferentes distribuições.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas de variação\n\n\n\nR\n\nAnálise de dados\n\nEstatística descritiva\n\nVariabilidade de dados\n\nAnálise de dispersão\n\n\n\nApresentação das medidas de variação, como variância, desvio padrão, coeficiente de variação e amplitude, com exemplos práticos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas de posição: quartis\n\n\n\nR\n\nAnálise de dados\n\nEstatística descritiva\n\nVariabilidade de dados\n\nAnálise de dispersão\n\n\n\nCálculo e interpretação de quartis para análise de distribuição, ressaltando faixas de variação e valores atípicos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas de posição: transformação Z\n\n\n\nR\n\nAnálise de dados\n\nEstatística descritiva\n\nPadronização de dados\n\nÍndice Z\n\n\n\nTransformação Z para padronizar distribuições, facilitando comparações entre diferentes escalas de medida.\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  }
]