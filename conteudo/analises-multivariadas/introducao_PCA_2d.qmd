---
title: "Introdução à Análise de Componentes Principais (PCA)"
execute:
  echo: true
  warning: false
  include: true
  message: false
  eval: true
format: 
  html:
    css: styles.css
---

```{python}
#| code-fold: true
# Importação de bibliotecas essenciais
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from statsmodels.nonparametric.smoothers_lowess import lowess
from scipy.stats import pearsonr
from numpy.linalg import norm
from math import acos, degrees
from IPython.display import Markdown
from tabulate import tabulate
```

A Análise de Componentes Principais (PCA) é uma técnica multivariada utilizada para organizar e representar objetos (como locais de amostragem, estações, indivíduos, etc.) em um espaço de dimensões reduzidas. Seu objetivo é simplificar conjuntos de dados complexos, com muitas variáveis correlacionadas, transformando-os em um número menor de variáveis não correlacionadas, chamadas **componentes principais**. Essas componentes são combinações lineares das variáveis originais, ortogonais entre si e ordenadas pela quantidade de variância que explicam. A primeira componente captura a maior parte da variação, enquanto as seguintes representam a variância remanescente, em ordem decrescente. Assim, cada nova componente contribui com frações progressivamente menores da variabilidade total, até que as últimas retêm apenas pequenas parcelas da informação, muitas vezes associadas a ruído ou a variações de baixa relevância, que podem ser descartadas sem comprometer a identificação dos principais padrões de ordenação nos dados.

Ao identificar as direções de máxima variabilidade, a PCA projeta os dados nesses novos eixos, permitindo eliminar redundâncias, reduzir ruídos e destacar padrões relevantes. Isso é especialmente útil quando os objetos em estudo são descritos por um grande número de variáveis que podem ser correlacionadas entre si. Dessa forma, em vez de analisar separadamente gráficos de dispersão para todos os pares de variáveis, a PCA organiza os objetos em um espaço multidimensional e os projeta em gráficos bidimensionais ou tridimensionais, cujos eixos concentram grande parte da variabilidade total e facilitam a interpretação das relações e agrupamentos presentes nos dados.

::: {.callout-tip title="Código em Python"}

Acompanha este tutorial o [Código em Python sobre Introdução à PCA](pca_python.py){target="_blank" title="Código em Python"}

:::

**Propriedades Fundamentais da PCA:**

1. Os eixos principais são **ortogonais** entre si, representando direções linearmente independentes.
2. Os autovalores ($\lambda$), que representam a variância ao longo de cada eixo, são sempre positivos ou nulos.
3. A técnica permite resumir, em poucas dimensões, a maior parte da variabilidade de uma matriz de dados com muitos descritores, além de medir a quantidade de variância explicada por esses eixos.

A PCA preserva a **distância euclidiana** entre os objetos, o que significa que a posição relativa entre eles não muda após a rotação dos eixos.

## A Matemática da PCA

A PCA é definida como a análise de autovalores e autovetores de uma matriz de dispersão (covariância ou correlação).

### A Matriz de Dados

Considere um conjunto de dados organizado em uma matriz $\mathbf{Y}$, na qual as linhas representam $n$ objetos (observações) e as colunas representam $p$ descritores (variáveis). Para aplicar a PCA, o primeiro passo é centralizar os dados, subtraindo de cada valor a média de sua respectiva coluna. O resultado é a matriz $\mathbf{Y_c}$.

Para ilustrar, vamos usar um exemplo didático com apenas duas variáveis, o que facilita compreender como a PCA transforma os dados em um novo espaço multidimensional. Nesse caso, cada aluno é descrito por duas informações: o tempo de estudo dedicado a uma determinada disciplina e a frequência em sala de aula, ambos expressos em horas.

A matriz de dados brutos $\mathbf{Y}$ e a matriz centralizada $\mathbf{Y}$ teriam o seguinte formato:

- **Matriz $\mathbf{Y}$ ($n \times p$):**

```{python}
#| code-fold: true
Y = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Notas.csv')
n = Y.shape[0]
p = Y.shape[1]
```

::: {.table-narrow}
```{python}
#| code-fold: true
#| label: tbl-alunos2d
#| tbl-cap: Nome, frequência em sala de aula e tempo dedicado ao estudo.
colunas = ['Nome', 'Frequencia', 'Estudo_horas']
Y_lista = Y[colunas].values.tolist()

Markdown(tabulate(
  Y_lista, 
  headers=colunas
))

```
:::

Onde $n = `{python} n`$ é o número de alunos e $p = `{python} p`$ o número de descritores.

- **Matriz Yc (Dados Centralizados):**
  
  Cada valor $y_{ij}$ é transformado em $y_{ij} - \bar{y}_j$, onde $\bar{y}_j$ é a média da coluna $j$.
  
  $$Y_c = Y - \bar{y}$$

```{python}
#| code-fold: true
variaveis = ["Frequencia", "Estudo_horas"]
y_bar = Y[variaveis].mean()
Freq_avg = float(y_bar['Frequencia'])
E_avg = float(y_bar['Estudo_horas'])
```

Neste exemplo, a  Frequência média em sala de aula é `{python} Freq_avg`, enquanto o tempo de estudo médio por estudante é `{python} E_avg`. Desta modo a matriz centralizada é expressa por:

::: {.table-narrow}
```{python}
#| code-fold: true
#| label: tbl-alunos_centralizado2d
#| tbl-cap: Matriz de daods centralizados.
Y_c = pd.DataFrame(Y['Nome'])
Y_c[variaveis] = Y[colunas[1:]] - y_bar
Y_c_lista = Y_c.values.tolist()

Markdown(tabulate(
  Y_c_lista, 
  headers=colunas
))

```
:::

### Matriz de Dispersão (Covariância)

A PCA opera sobre uma matriz de dispersão $\mathbf{S}$, que pode ser uma matriz de covariância ou correlação. A matriz de covariância $\mathbf{S}$ é calculada a partir dos dados centralizados $\mathbf{Y_c}$:

$$\mathbf{S} = \frac{1}{n-1} \mathbf{Y_c'}\mathbf{Y_c}$$

Onde $\mathbf{Y_c'}$ é a transposta de $\mathbf{Y_c}$.

A matriz de covariância dos dados deste exemplo é dada por:

```{python}
#| code-fold: true
S = Y_c[variaveis].cov(ddof=0)
S
```

Nesta matriz, as diagonais representam as variâncias da frequência ($s_{freq} = `{python} float(S.iloc[0,0])`$) e tempo de estudo ($s_{estudo} = `{python} float(S.iloc[1,1])`$), enquando a covariância entre as variáveis é $`{python} float(S.iloc[1,0])`$

### Autovalores e Autovetores da matris de covariância

Os eixos principais da matriz de dispersão $\mathbf{S}$ são encontrados resolvendo a seguinte equação para os autovalores ($\lambda$) e autovetores ($u$):

$$(S - \lambda_k I)u_k = 0$$

Onde:

- $\lambda_k$ é o $k$-ésimo **autovalor**. Ele representa a quantidade de variância dos dados ao longo do $k$-ésimo eixo principal.
- $u_k$ é o $k$-ésimo **autovetor** associado a $\lambda_k$. Ele define a direção do $k$-ésimo eixo principal.
- $I$ é uma matriz identidade.

Os autovalores são calculados a partir da equação característica:

$$|S - \lambda_k I| = 0$$

E neste exemplo os autovetores são:

```{python}
#| code-fold: true
Ycv = Y_c[variaveis]
pca_2d = PCA()
pca_2d.fit(Ycv)
autovalores = pca_2d.explained_variance_
Lambda_list = autovalores.tolist()
Lambda_formatada = [float(np.round(num,2)) for num in Lambda_list]
R2 = Lambda_list / np.sum(Lambda_list)
R2_formatada = [float(np.round(num,4)) for num in R2]
R2_perc = [float(np.round(num*100,2)) for num in R2]
```

$\Lambda = `{python} Lambda_formatada`$

A soma de todos os autovalores ($\lambda_k$) é igual à variância total dos dados, que é a soma dos elementos da diagonal da matriz $\mathbf{S}$ (traço da matriz). A proporção da variância total explicada por um conjunto de $m$ componentes principais é dada por:

$$R^2 = \frac{\sum_{k=1}^{m}\lambda_k}{\sum_{k=1}^{p}\lambda_k} = `{python} R2_formatada`$$

No exemplo, o PC1 explica a maior parte da variância ($\lambda_1 = `{python} R2_perc[0]`$%) e o PC2 explica o restante ($\lambda_2 = `{python} R2_perc[1]`$%), somando 100%.

Os **autovetores** são então calculados e normalizados para terem comprimento unitário ($u'u = 1$). Eles formam as colunas da matriz $\mathbf{U}$. 

::: {.table-narrow}
```{python}
#| code-fold: true
U = pca_2d.components_
pd.DataFrame(U, columns=['u1', 'u2'])
```
:::

Os elementos dos autovetores são também chamados de **loadings** (pesos) e indicam como cada variável original contribui para a formação de cada componente principal.

### Componentes Principais (Scores)

Os componentes principais, também conhecidos como *scores*, são as coordenadas dos objetos no novo sistema de eixos. Eles são calculados projetando os dados centralizados nos autovetores. A posição de um objeto $i$ no primeiro eixo principal é dada por:

$$f_{i1} = (y_{i1}-\bar{y}_1)u_{11} + ... + (y_{ip}-\bar{y}_p)u_{p1}$$

Matricialmente, a matriz **F** dos componentes principais para todos os objetos é calculada como:

$$F = Y_c U$$

```{python}
#| code-fold: true
F_2d_lista = pca_2d.transform(Ycv)
F_2d = pd.DataFrame(Y['Nome'])
F_2d[['PCA1', 'PCA2']] = pd.DataFrame(F_2d_lista)
```

::: {.table-narrow}
```{python}
#| code-fold: true
#| label: tbl-pca_2d_loadings
#| tbl-cap: Matriz **F** contendo os scores dos componentes principais 1 e 2 para cada objeto da matriz original.
Markdown(tabulate(
  F_2d.values.tolist(), 
  headers=['PCA1', 'PCA2']
))

```
:::

As colunas da matriz **F** contêm os *scores* dos objetos (nas linhas) para cada componente principal e podem ser usadas para criar gráficos de ordenação, como os biplots.


## Interpretando os Resultados da PCA: Biplots

Um **biplot** é um gráfico que exibe simultaneamente os objetos (scores, da matriz $\mathbf{F}$) e os descritores (loadings, da matriz $\mathbf{U}$). Existem duas formas principais de escalonamento (scaling) para biplots, cada uma com uma interpretação diferente.

### Scaling 1 (Biplot de Distância)

- **Objetivo:** Preservar a distância Euclidiana entre os objetos.
- **Matrizes:** Usa os scores da matriz $\mathbf{F}$ e os autovetores da matriz $\mathbf{U}$ (comprimento 1).
- **Interpretação:**
  - A distância entre os pontos dos objetos no gráfico aproxima a sua distância Euclidiana no espaço multidimensional.
  - A projeção ortogonal de um objeto sobre um eixo de descritor aproxima o valor daquele objeto para aquele descritor.
  - Os ângulos entre os vetores dos descritores **não** presenvam os ângulos entre os vetores originais.

```{python}
#| code-fold: true
#| label: fig-pca2d-biplot-scaling1
#| fig-cap: "Gráfico de um biplot de distância (Scaling 1). A distância entre os objetos presenva suas distâncias eucildianas originais."
# Criar o biplot
plt.figure(figsize=(8, 6))
sns.set_style("whitegrid")

# Plotar os objetos (alunos) usando o dataframe F_2d
scatter = sns.scatterplot(data=F_2d, x='PCA1', y='PCA2', 
                         s=100, alpha=0.8, color='steelblue')

# Adicionar labels para os alunos
for i, row in F_2d.iterrows():
    plt.annotate(row['Nome'], (row['PCA1'], row['PCA2']), 
                xytext=(5, 5), textcoords='offset points', 
                fontsize=9, alpha=0.8, fontweight='medium')

# Plotar os vetores das variáveis (loadings)
scale_factor = 2.5  # Fator de escala para melhor visualização

for i, var in enumerate(variaveis):
    plt.arrow(0, 0, U[0, i] * scale_factor, U[1, i] * scale_factor, 
              head_width=0.15, head_length=0.15, fc='red', ec='red', 
              linewidth=2.5, alpha=0.9)
    plt.text(U[0, i] * scale_factor * 1.15, U[1, i] * scale_factor * 1.15, 
             var, color='red', fontsize=13, fontweight='bold',
             ha='center', va='center', bbox=dict(boxstyle="round,pad=0.3", 
                                               facecolor='white', 
                                               alpha=0.8))

# Configurações do gráfico
plt.axhline(y=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)
plt.axvline(x=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)

plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variância explicada)',
          fontsize=12, fontweight='bold')
plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variância explicada)',
          fontsize=12, fontweight='bold')
plt.title('Biplot - Análise de Componentes Principais\nDistâncias entre alunos e correlações entre variáveis',
         fontsize=14, fontweight='bold', pad=20)

# Adicionar grid
plt.grid(True, alpha=0.3, linestyle='--')

# Ajustar limites para melhor visualização
plt.xlim(-10, 10)
plt.ylim(-3, 3)

plt.tight_layout()
plt.show()

```

### Scaling 2 (Biplot de Correlação)

- **Objetivo:** Representar as correlações (ou covariâncias) entre os descritores.
- **Matrizes:** Usa os autovetores escalados pela raiz quadrada dos autovalores ($U_{sc2} = U\Lambda^{1/2}$) para os descritores e scores reescalados ($G = F\Lambda^{-1/2}$) para os objetos.
- **Interpretação:**
  - Os ângulos entre os vetores dos descritores refletem suas correlações: vetores próximos indicam alta correlação positiva; vetores em direções opostas indicam correlação negativa; vetores em 90° indicam ausência de correlação.
  - O comprimento do vetor de um descritor é uma aproximação de seu desvio padrão.
  - A distância entre os objetos no gráfico aproxima sua **distância de Mahalanobis**, não a distância Euclidiana.

```{python}
#| code-fold: true
#| label: fig-pca2d-biplot-scaling2
#| fig-cap: "Gráfico de um biplot de correlação (Scaling 2). Os ângulos entre os vetores dos descritores (setas vermelhas) indicam a correlação entre as variáveis originais."
# Calcular os componentes para Scaling 2
autovalores = pca_2d.explained_variance_
lambda_sqrt = np.sqrt(autovalores)  # Λ^(1/2)
lambda_inv_sqrt = 1 / lambda_sqrt   # Λ^(-1/2)

# Reescalar os loadings: U_sc2 = U * Λ^(1/2)
U_sc2 = U * lambda_sqrt.reshape(-1, 1)

# Reescalar os scores: G = F * Λ^(-1/2)
G_sc2 = F_2d_lista * lambda_inv_sqrt

# Criar dataframe com scores reescalados
G_2d = Y[['Nome']].copy()
G_2d[['PC1_sc2', 'PC2_sc2']] = pd.DataFrame(G_sc2)

# Criar o biplot de correlação
plt.figure(figsize=(8, 6))
sns.set_style("whitegrid")

# Plotar os objetos (alunos) reescalados
scatter = sns.scatterplot(data=G_2d, x='PC1_sc2', y='PC2_sc2', 
                         s=100, alpha=0.8, color='steelblue')

# Adicionar labels para os alunos
for i, row in G_2d.iterrows():
    plt.annotate(row['Nome'], (row['PC1_sc2'], row['PC2_sc2']), 
                xytext=(5, 5), textcoords='offset points', 
                fontsize=9, alpha=0.8, fontweight='medium')

# Plotar os vetores das variáveis (loadings reescalados)
scale_factor = .5  # Fator de escala para melhor visualização

for i, var in enumerate(variaveis):
    plt.arrow(0, 0, U_sc2[0, i] * scale_factor, U_sc2[1, i] * scale_factor, 
              head_width=0.15, head_length=0.15, fc='red', ec='red', 
              linewidth=2.5, alpha=0.9)
    plt.text(U_sc2[0, i] * scale_factor * 1.15, U_sc2[1, i] * scale_factor * 1.15, 
             var, color='red', fontsize=13, fontweight='bold',
             ha='center', va='center', bbox=dict(boxstyle="round,pad=0.3", 
                                               facecolor='white', 
                                               alpha=0.8))

# Configurações do gráfico
plt.axhline(y=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)
plt.axvline(x=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)

plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variância explicada)',
          fontsize=12, fontweight='bold')
plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variância explicada)',
          fontsize=12, fontweight='bold')
plt.title('Biplot de Correlação (Scaling 2) - Análise de Componentes Principais\nÂngulos entre vetores representam correlações entre variáveis',
         fontsize=14, fontweight='bold', pad=20)

# Adicionar grid
plt.grid(True, alpha=0.3, linestyle='--')

# Ajustar limites para melhor visualização
plt.xlim(-2, 3)
plt.ylim(-2, 2)

plt.tight_layout()
plt.show()
```


### Conclusão

```{python}
#| code-fold: true
def angle_between_vectors(v1, v2):
    """Calcula o ângulo entre dois vetores em graus"""
    cos_angle = np.dot(v1, v2) / (norm(v1) * norm(v2))
    return degrees(acos(np.clip(cos_angle, -1.0, 1.0)))

corr_matrix = Y[variaveis].corr()
corr_coef = f"{float(corr_matrix.iloc[0, 1]):.2f}"

angle_vars = angle_between_vectors(U_sc2[:, 0], U_sc2[:, 1])
angle = f"{angle_vars:.2f}"

```

A primeira componente principal (PC1) explicou `{python} R2_perc[0]`% da variância total, enquanto a segunda (PC2) respondeu por apenas `{python} R2_perc[1]`%. Observamos que as variáveis originais, *Frequência* e *Estudo_horas*, apresentam uma forte correlação positiva (coeficiente de correlação = `{python} corr_coef`), o que é visualmente confirmado no biplot de correlação (Scaling 2): seus vetores apontam praticamente na mesma direção, formando um ângulo de `{python} angle`°. Isso indica que estudantes que dedicam mais tempo de estudo tendem também a ter maior frequência em sala de aula.

No biplot de distância (Scaling 1), nota-se a proximidade entre os alunos com melhor desempenho, como Fernanda e Eduardo, que obtiveram os maiores valores em ambas as variáveis. Assim, a ordenação dos alunos a partir das variáveis originais pode ser bem representada por um único componente, o PC1. Nesse exemplo, o PC1 reflete a associação conjunta entre *Frequência* e *Estudo_horas*, podendo ser interpretado como um indicador de *nível de engajamento* do estudante na disciplina.

## A Escolha da Matriz de Dispersão na PCA: Covariância vs. Correlação

A Análise de Componentes Principais (PCA) opera sobre uma matriz de dispersão, que pode ser uma **matriz de covariância** ($\mathbf{S}$) ou uma **matriz de correlação** ($\mathbf{R}$). A escolha entre essas duas matrizes depende diretamente da natureza das variáveis originais.

1. **PCA baseada na Matriz de Covariância ($\mathbf{S}$):** A análise é realizada diretamente sobre os dados centralizados ($Y_c$), em que cada valor teve a média de sua coluna subtraída. Essa abordagem é adequada quando as variáveis são **dimensionalmente homogêneas**, ou seja, possuem as mesmas unidades físicas e ordens de magnitude e variabilidade semelhantes. Nesse caso, as diferenças de variância entre as variáveis são consideradas informações relevantes, e a PCA buscará os eixos que maximizam a covariância total.

2. **PCA baseada na Matriz de Correlação ($\mathbf{R}$):** Quando as variáveis não são dimensionalmente homogêneas — por exemplo, ao analisar dados que combinam temperatura (°C), peso (kg) e concentração (mg/L) — ou quando suas variâncias são muito discrepantes, a padronização dos dados se torna essencial. A padronização consiste em centralizar os dados (subtrair a média) e, adicionalmente, dividir cada valor pelo desvio padrão da respectiva variável. Esse processo transforma os dados para que tenham média 0 e desvio padrão 1. **Uma propriedade deste processo é que a matriz de covariância das variáveis padronizadas é idêntica à matriz de correlação das variáveis originais.** Portanto, realizar a PCA sobre dados padronizados equivale a realizá-la sobre a matriz de correlação ($\mathbf{R}$). Essa abordagem garante que todas as variáveis contribuam igualmente para a análise, evitando que descritores com maior variância dominem a definição dos componentes principais.

### Exemplo: Frequência versus Tempo de Estudo

No exemplo dos alunos, com as variáveis `Frequencia` e `Estudo_horas`, a PCA foi conduzida sobre a matriz de covariância. Isso foi possível porque as variâncias das duas variáveis são relativamente parecidas, como mostrado na @tbl-alunos2d. Neste cenário, a padronização não foi estritamente necessária, pois nenhuma das variáveis tinha variância suficientemente grande para ofuscar a outra, permitindo que a análise capturasse a estrutura de covariância e refletisse a forte associação positiva entre as duas variáveis.

Em casos mais gerais, especialmente quando se mede uma ampla gama de descritores com unidades e escalas distintas, **a padronização é fortemente recomendada**, pois, se a PCA for aplicada a uma matriz de covariância de variáveis com variâncias muito díspares, os primeiros componentes principais seriam predominantemente determinados pelas variáveis de maior variância, e a estrutura de correlação entre as demais variáveis não seria descrita adequadamente.

A decisão de usar a matriz $\mathbf{S}$ ou $\mathbf{R}$ se resume a determinar se a **magnitude da variância das variáveis originais deve influenciar a análise ou se todas as variáveis devem ter o mesmo peso**. Se for importante manter as diferenças de magnitude, utiliza-se a matriz de covariância. Caso contrário, para garantir que a PCA revele a estrutura de correlação subjacente entre os descritores, a padronização dos dados e, consequentemente, o uso da matriz de correlação é o caminho mais seguro.