---
title: "Construindo um modelo Bayesiano"  
subtitle: "VerossimilhanÃ§a e distribuiÃ§Ã£o *a priori*"  
description: "ConteÃºdo baseado em @mcelreath2018statistical"
Categories: ["InferÃªncia bayesiana", "modelo Binomial", "distribuiÃ§Ãµes de probabilidade"]
image: "images/intro-bayes-modelo-bayesiano.png"  
execute:
  echo: false
  warning: false
  include: true
  message: false
---

---

Considere um globo representando o planeta Terra, pequeno o suficiente para caber em suas mÃ£os. Seu objetivo Ã© estimar a fraÃ§Ã£o da superfÃ­cie coberta por Ã¡gua. Para isso, vocÃª adota a seguinte estratÃ©gia: joga o globo para cima girando e, ao pegÃ¡-lo, registra se o ponto tocado pelo seu dedo indicador direito Ã© Ã¡gua (ğŸŒŠ) ou terra (ğŸœï¸). VocÃª repete esse procedimento algumas vezes, obtendo uma sequÃªncia de $n$ observaÃ§Ãµes.

VocÃª faz quatro lanÃ§amentos do globo e conta quantos deles resultam em Ã¡gua. Um possÃ­vel resultado seria $ğŸŒŠğŸŒŠğŸœï¸ğŸŒŠ$, totalizando 3 observaÃ§Ãµes de Ã¡gua e 1 de terra. Outro resultado possÃ­vel Ã© $ğŸœï¸ğŸœï¸ğŸŒŠğŸŒŠ$, com 2 observaÃ§Ãµes de Ã¡gua e 2 de terra. Para $n = 4$ observaÃ§Ãµes, existem 16 resultados possÃ­veis (@tbl-proporcao-globo).

| NÂº |       Dados       | NÂº pontos na Ã¡gua | ProporÃ§Ã£o de pontos na Ã¡gua |
|:--:|:-----------------:|:-----------------:|:----------------------------:|
|  1 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸      | 0                 | 0/4 = 0.00                   |
|  2 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸŒŠ      | 1                 | 1/4 = 0.25                   |
|  3 | ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸœï¸      | 1                 | 1/4 = 0.25                   |
|  4 | ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸœï¸      | 1                 | 1/4 = 0.25                   |
|  5 | ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸœï¸      | 1                 | 1/4 = 0.25                   |
|  6 | ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸŒŠ      | 2                 | 2/4 = 0.50                   |
|  7 | ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸŒŠ      | 2                 | 2/4 = 0.50                   |
|  8 | ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸœï¸      | 2                 | 2/4 = 0.50                   |
|  9 | ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸŒŠ      | 2                 | 2/4 = 0.50                   |
| 10 | ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸœï¸      | 2                 | 2/4 = 0.50                   |
| 11 | ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸœï¸      | 2                 | 2/4 = 0.50                   |
| 12 | ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸŒŠ      | 3                 | 3/4 = 0.75                   |
| 13 | ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸŒŠ      | 3                 | 3/4 = 0.75                   |
| 14 | ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸŒŠ      | 3                 | 3/4 = 0.75                   |
| 15 | ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸœï¸      | 3                 | 3/4 = 0.75                   |
| 16 | ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸŒŠ      | 4                 | 4/4 = 1.00                   |

: **Resultados possÃ­veis de 4 observaÃ§Ãµes (terra ou Ã¡gua) com o respectivo nÃºmero e proporÃ§Ã£o de observaÃ§Ãµes em Ã¡gua.** {#tbl-proporcao-globo .striped .hover}

Observe que apenas um dos resultados contÃ©m 4 observaÃ§Ãµes de terra e somente um contÃ©m 4 observaÃ§Ãµes de Ã¡gua. Os demais sÃ£o variaÃ§Ãµes entre esses extremos.

Podemos reorganizar a tabela para evidenciar todas as combinaÃ§Ãµes que levam ao mesmo nÃºmero $y_i$ de pontos em Ã¡gua:

| NÂº pontos na Ã¡gua ($y_i$) | Dados | NÂº de combinaÃ§Ãµes |
|:---------------------------:|:------|:-----------------|
| 0 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸ | 1 |
| 1 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸœï¸ <br> ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸœï¸ | 4 |
| 2 | ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸŒŠ <br> ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸœï¸ | 6 |
| 3 | ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸŒŠ <br> ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸŒŠ <br> ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸŒŠ <br> ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸœï¸ | 4 |
| 4 | ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸŒŠ | 1 |

: **CombinaÃ§Ãµes que resultam em $y_i$ pontos na Ã¡gua.** {#tbl-combinacao-globo tbl-colwidths="[25,50,25]" .striped .hover}

<br>

Defina $p$ como a probabilidade de observar Ã¡gua e $1 - p$ como a probabilidade de observar terra apÃ³s cada lanÃ§amento do globo.

A Ãºltima linha da @tbl-combinacao-globo (ğŸŒŠğŸŒŠğŸŒŠğŸŒŠ) tem probabilidade:
$$P(4) = p \times p \times p \times p.$$

Enquanto a primeira linha (ğŸœï¸ğŸœï¸ğŸœï¸ğŸœï¸) ocorre com probabilidade:
$$P(0) = (1 - p) \times (1 - p) \times (1 - p) \times (1 - p).$$

As linhas correspondentes a $P(1)$, $P(2)$ e $P(3)$ sÃ£o combinaÃ§Ãµes de $p$ e $(1 - p)$, multiplicadas pelo nÃºmero de formas pelas quais 1, 2 ou 3 registros de Ã¡gua podem ocorrer em 4 lanÃ§amentos.

<br>

| NÂº pontos na Ã¡gua ($y_i$) | Dados | NÂº de combinaÃ§Ãµes | $P(Y)$ |
|:---------------------------:|:------|:-----------------:|:--------|
| 0 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸ | 1 | $1 \times (1-p) \times (1-p) \times (1-p) \times (1-p)$ |
| 1 | ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸœï¸ <br> ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸœï¸ | 4 | $4 \times p \times (1-p) \times (1-p) \times (1-p)$ |
| 2 | ğŸœï¸ ğŸœï¸ ğŸŒŠ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸœï¸ ğŸŒŠ <br> ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸœï¸ ğŸœï¸ ğŸŒŠ <br> ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸœï¸ <br> ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸœï¸ | 6 | $6 \times p \times p \times (1-p) \times (1-p)$ |
| 3 | ğŸœï¸ ğŸŒŠ ğŸŒŠ ğŸŒŠ <br> ğŸŒŠ ğŸœï¸ ğŸŒŠ ğŸŒŠ <br> ğŸŒŠ ğŸŒŠ ğŸœï¸ ğŸŒŠ <br> ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸœï¸ | 4 | $4 \times p \times p \times p  \times (1-p)$ |
| 4 | ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸŒŠ | 1 | $(p) \times p \times p \times p$ |

: **Probabilidade $P(Y)$ de observar diferentes nÃºmeros ($y_i$) de pontos sobre a Ã¡gua.** {#tbl-probabilidade-globo tbl-colwidths="[25,30,15,30]" .striped .hover}

<br>

## O modelo Binomial

A partir das expressÃµes para $P(Y)$ apresentadas na @tbl-probabilidade-globo, obtÃ©m-se uma fÃ³rmula geral que pode ser escrita como:

$$P(Y \mid n, p) = \binom{n}{Y} \, p^Y (1 - p)^{n - Y}.
$${#eq-binomial}

**Onde:**

- $Y \in \{0, 1, 2, \dots, n\}$ Ã© o nÃºmero de observaÃ§Ãµes de ğŸŒŠ;
- $n$ Ã© o nÃºmero total de observaÃ§Ãµes;
- $p$ Ã© a fraÃ§Ã£o de ğŸŒŠ que cobre o globo;
- $\binom{n}{Y}$ Ã© o coeficiente binomial, calculado por $\frac{n!}{Y!(n - Y)!}$, indicando de quantas maneiras a combinaÃ§Ã£o $p^Y (1 - p)^{n - Y}$ pode ocorrer.

A @eq-binomial fornece a probabilidade de cada resultado possÃ­vel (nÃºmero de observaÃ§Ãµes ğŸŒŠ) em $n$ tentativas, permitindo calcular a probabilidade de **todos** os possÃ­veis resultados do experimento.

## VerossimilhanÃ§a: a plausibilidade de uma hipÃ³tese

```{python}
#| echo: false
from math import comb
import numpy as np

# DefiniÃ§Ãµes de n, y e da lista de hipÃ³teses p
n = 4
y = 2
p_list = [0.4, 0.30, 0.80]

# CÃ¡lculo das verossimilhanÃ§as (ou probabilidades) para cada hipÃ³tese
prob = [round(comb(n, y)*(p**y)*((1 - p)**(n - y)), 2) for p in p_list]

# IdentificaÃ§Ã£o da maior verossimilhanÃ§a e Ã­ndice
max_prob = max(prob)
indice_max = prob.index(max_prob)
```


A partir do modelo binomial, podemos definir a **funÃ§Ã£o de verossimilhanÃ§a** para um resultado observado. Imagine que, em $n = `{python} n`$ lanÃ§amentos, foram observados $y = `{python} y`$ pontos sobre a Ã¡gua. NÃ£o sabemos a verdadeira proporÃ§Ã£o $p$ de Ã¡gua que cobre a Terra; portanto, fazemos conjecturas e avaliamos cada uma com base nas observaÃ§Ãµes.

Por exemplo, se supormos que a proporÃ§Ã£o verdadeira seja 40% $(p = 0.4)$, a distribuiÃ§Ã£o binomial determina que a probabilidade de observar $y = `{python} y`$ sucessos em $n = `{python} n`$ lanÃ§amentos seja:

$$P(Y=`{python} y` \mid `{python} n`, `{python} p_list[0]`) = \binom{`{python} n`}{`{python} y`} \, `{python} p_list[0]`^`{python} y` (1 - `{python} p_list[0]`)^{`{python} n` - `{python} y`} = `{python} prob[0]`$$

Essa hipÃ³tese Ã© apenas uma das possÃ­veis. Para ilustrar outras conjecturas, considere:

- Se $p = `{python} p_list[1]`$:

  $P(Y=`{python} y` \mid `{python} n`, `{python} p_list[1]`) = \binom{`{python} n`}{`{python} y`} \, `{python} p_list[1]`^`{python} y` (1 - `{python} p_list[1]`)^{`{python} n` - `{python} y`} = `{python} prob[1]`$

- Se $p = `{python} p_list[2]`$:

  $P(Y=`{python} y` \mid `{python} n`, `{python} p_list[2]`) = \binom{`{python} n`}{`{python} y`} \, `{python} p_list[2]`^`{python} y` (1 - `{python} p_list[2]`)^{`{python} n` - `{python} y`} = `{python} prob[2]`$

Em cada caso, os dados observados $(Y)$ e o nÃºmero total de observaÃ§Ãµes $(n)$ estÃ£o fixos, enquanto o parÃ¢metro $p$ varia conforme a hipÃ³tese considerada. Embora a forma matemÃ¡tica seja idÃªntica Ã  da funÃ§Ã£o de probabilidade binomial, seu uso Ã© diferente. Na funÃ§Ã£o de probabilidade, lemos a probabilidade de $Y$ dado $n$ e $p$, enquanto nos exemplos acima, avaliamos a plausibilidade de diferentes valores de $p$ diante dos dados fixos $Y$ e $n$. 

Para evitar confusÃµes, vamos definir a funÃ§Ã£o de verossimilhanÃ§a como:

$$
\mathcal{L}(p \mid n, Y) = \binom{n}{Y} \, p^Y (1 - p)^{n - Y}.
$$ {#eq-likelihood-binomial}

Assim, as verossimilhanÃ§as para as trÃªs conjecturas especÃ­ficas sobre a proporÃ§Ã£o de Ã¡gua na superfÃ­cie do globo serÃ£o:

- $\mathcal{L}(p = `{python} p_list[1]` \mid `{python} n`, `{python} y`) = `{python} prob[1]`$,
- $\mathcal{L}(p = `{python} p_list[0]` \mid `{python} n`, `{python} y`) = `{python} prob[0]`$,
- $\mathcal{L}(p = `{python} p_list[2]` \mid `{python} n`, `{python} y`) = `{python} prob[2]`$.

Dessa forma, entre as trÃªs hipÃ³teses levantadas, aquela em que $p = `{python} p_list[indice_max]`$ recebe **maior suporte das evidÃªncias**, por estar associada Ã  maior verossimilhanÃ§a.

Podemos quantificar esse suporte por meio da **razÃ£o de verossimilhanÃ§as**:

$$RV = \frac{\mathcal{L}(p = `{python} p_list[indice_max]` \mid `{python} n`, `{python} y`)}{\mathcal{L}(p = `{python} p_list[1]` \mid `{python} n`, `{python} y`)} = \frac{`{python} prob[indice_max]`}{`{python} prob[1]`} = `{python} round(prob[indice_max]/prob[1],2)`,$$

o que indica que a hipÃ³tese $p = `{python} p_list[indice_max]`$ Ã© aproximadamente $`{python} round(prob[indice_max]/prob[1],2)`$ **vezes mais verossÃ­mil** do que a hipÃ³tese $p = `{python} p_list[1]`$ com base nos dados observados.

::: {.callout-warning title="Resumo: A FunÃ§Ã£o de VerossimilhanÃ§a Binomial"}

- A expressÃ£o Ã© **formalmente idÃªntica** Ã  da distribuiÃ§Ã£o binomial, porÃ©m interpretada como uma funÃ§Ã£o de $p$ quando os dados $Y$ e $n$ sÃ£o fixos.
- A verossimilhanÃ§a indica a plausibilidade de diferentes valores de $p$ Ã  luz dos dados observados.
- Na distribuiÃ§Ã£o binomial, lemos: **probabilidade de $Y$ dado $n$ e $p$**.
- Na funÃ§Ã£o de verossimilhanÃ§a, interpretamos: **verossimilhanÃ§a de $p$ dado $n$ e $Y$**.
- A razÃ£o de verossimilhanÃ§as pode ser utilizada para quantificar o suporte relativo entre diferentes hipÃ³teses.
:::

### O perfil de verossimilhanÃ§a

Acima, foram testadas trÃªs conjecturas especÃ­ficas para a proporÃ§Ã£o de Ã¡gua na superfÃ­cie da Terra ($p = `{python} p_list[1]`$, $p = `{python} p_list[0]`$, $p = `{python} p_list[2]`$). Para uma avaliaÃ§Ã£o mais completa, podemos analisar o **perfil de verossimilhanÃ§a** para uma sÃ©rie de valores de $p$ entre 0 e 1:

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from math import comb

# ParÃ¢metros do experimento: n e y (reutilizamos do chunk anterior)
# n = 4; y = 2

# GeraÃ§Ã£o de valores de p no intervalo [0, 1]
p_vals = np.linspace(0, 1, 100)

# CÃ¡lculo da verossimilhanÃ§a para cada valor de p
likelihood = [comb(n, y)*(p**y)*((1 - p)**(n - y)) for p in p_vals]

# Plot do grÃ¡fico
plt.plot(p_vals, likelihood)
plt.xticks(np.arange(0, 1.01, 0.1))
plt.grid(True, which='both', axis='x', linestyle='--', alpha=0.6)
plt.xlabel("p")
plt.ylabel("VerossimilhanÃ§a")
plt.title(f"FunÃ§Ã£o de verossimilhanÃ§a Binomial (n={n}, y={y})")
plt.show()
```

O perfil de verossimilhanÃ§a indica que, Ã  luz dos nossos dados $y = `{python} y`$, a conjectura mais plausÃ­vel Ã© que a proporÃ§Ã£o de Ã¡gua que cobre a Terra esteja prÃ³xima de 0.5 (neste caso, a verossimilhanÃ§a mÃ¡xima Ã© *exatamente* para $p = 0.5$).

## InferÃªncia Bayesiana: distribuiÃ§Ãµes *a priori* e *a posteriori*

No contexto bayesiano, a funÃ§Ã£o de verossimilhanÃ§a Ã© combinada com uma **distribuiÃ§Ã£o *a priori*** para obter a **distribuiÃ§Ã£o *a posteriori*** do parÃ¢metro $p$.  

Para ilustrar esse processo, retomamos as trÃªs conjecturas especÃ­ficas sobre a proporÃ§Ã£o de Ã¡gua na superfÃ­cie do globo e consideramos um caso simples, em que $p$ pode assumir **apenas trÃªs valores**: $p = `{python} p_list[1]`$, $p = `{python} p_list[0]`$ e $p = `{python} p_list[2]`$.  

### HipÃ³teses ($H$) e distribuiÃ§Ã£o *a priori* uniforme

Definimos trÃªs hipÃ³teses sobre o parÃ¢metro $p$, cada uma correspondendo a um desses valores:

- $H_1: p = 0.3$
- $H_2: p = 0.5$
- $H_3: p = 0.8$

Para comeÃ§ar, vamos assumir que nossa distribuiÃ§Ã£o *a priori* Ã© **uniforme** entre essas trÃªs hipÃ³teses, ou seja, nÃ£o temos motivos para preferir uma Ã  outra. Neste caso:

$$P(p = `{python} p_list[1]`) = P(p = `{python} p_list[0]`) = P(p = `{python} p_list[2]`) = \frac{1}{3}.$$

Suponha que realizamos $n = `{python} n`$ lanÃ§amentos do globo e observamos $Y = `{python} y`$ pontos de Ã¡gua. As verossimilhanÃ§as para cada hipÃ³tese podem ser calculadas a partir da distribuiÃ§Ã£o binomial:

$$\mathcal{L}(p \mid n=4, Y=2)
= \binom{4}{2}\, p^2 (1 - p)^{2}.$$

Resultando em:

- $\mathcal{L}(p = `{python} p_list[1]` \mid `{python} n`, `{python} y`) = `{python} prob[1]`$
- $\mathcal{L}(p = `{python} p_list[0]` \mid `{python} n`, `{python} y`) = `{python} prob[0]`$
- $\mathcal{L}(p = `{python} p_list[2]` \mid `{python} n`, `{python} y`) = `{python} prob[2]`$

### DistribuiÃ§Ã£o *a posteriori*

```{python}
#| echo: false
import numpy as np

# 'prob' foi calculado no chunk binom_likelihood_calc
lD_Hi = np.array(prob)  # verossimilhanÃ§as
priori_hip = np.array([1/3, 1/3, 1/3])

# Numerador: L(D|Hi) * P(Hi)
P_Hi = lD_Hi * priori_hip
# Denominador (normalizador): soma de L(D|Hi)*P(Hi) sobre i
P_H = sum(P_Hi)

# Probabilidades a posteriori
P_HiD = (lD_Hi * priori_hip) / P_H

# Armazenando para exibir
lD_Hi_list = lD_Hi.tolist()
P_H_list = round(P_H.tolist(),2)
P_Hround = np.round(P_H, 2)
P_HiD_rounded = np.round(P_HiD, 2)

P_Hi_list = np.round(P_Hi, 2).tolist()
P_HiD_list = P_HiD_rounded.tolist()
```

Vamos utilizar uma expressÃ£o anÃ¡loga ao Teorema de Bayes para obter a probabilidade *a posteriori* para cada hipÃ³tese $H_i$:

$$P(H_i \mid D)
= \frac{\mathcal{L}(D \mid H_i) \,\cdot\, P(H_i)}{P(H)},$$

Onde:

- $P(H_i \mid D)$ Ã© a probabilidade *a posteriori* da hipÃ³tese $H_i$ atualizada pelas observaÃ§Ãµes.
- $\mathcal{L}(D \mid H_i)$ Ã© a verossimilhanÃ§a dos dados $D$ (observaÃ§Ãµes) dada a hipÃ³tese $H_i$.
- $P(H_i)$ Ã© a probabilidade *a priori* da hipÃ³tese $H_i$.
- $P(H)$ a soma ponderada de todas as verossimilhanÃ§as pelas suas *priors* (fator de normalizaÃ§Ã£o), garantindo que as probabilidades *a posteriori* somem 1.

Em nosso exemplo:

$P(H) = \mathcal{L}(D \mid H_1) \,\cdot\, P(H_1) + \mathcal{L}(D \mid H_2) \,\cdot\, P(H_2) + \mathcal{L}(D \mid H_3) \,\cdot\, P(H_3)$

$P(H) = \sum_{i=1}^{3} \mathcal{L}(D \mid H_i) \,\cdot\, P(H_i)$


Uma vez tendo assumindo uma distribuiÃ§Ã£o *a priori* **uniforme** em que $P(H_i) = \frac{1}{3}$, podemos obter as probabilidades *a posteriori* para cada hipÃ³tese. Assim temos:

$P(H) = \mathcal{L}(D \mid H_1) \,\cdot\, P(H_1) + \mathcal{L}(D \mid H_2) \,\cdot\, P(H_2) + \mathcal{L}(D \mid H_3) \,\cdot\, P(H_3)$

$P(H) = `{python} lD_Hi_list[1]` \times \frac{1}{3} + `{python} lD_Hi_list[0]` \times \frac{1}{3} + `{python} lD_Hi_list[2]` \times \frac{1}{3}$

$P(H) = `{python} P_Hi_list[1]` + `{python} P_Hi_list[0]` + `{python} P_Hi_list[2]` = `{python} P_H_list`$

De modo que:

$P(`{python} p_list[1]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[1]` \times \frac{1}{3}}{`{python} P_H_list`} \approx `{python} P_HiD_list[1]`$

$P(`{python} p_list[0]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[0]` \times \frac{1}{3}}{`{python} P_H_list`} \approx `{python} P_HiD_list[0]`$

$P(`{python} p_list[2]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[2]` \times \frac{1}{3}}{`{python} P_H_list`} \approx `{python} P_HiD_list[2]`$

Observe que o valor mais alto da probabilidade *a posteriori* estÃ¡ associado a $p = `{python} p_list[indice_max]`$. AlÃ©m disso, ao calcularmos a razÃ£o entre as probabilidades *a posteriori* das hipÃ³teses $P(H_2)$ ($p = `{python} p_list[0]`$) e $P(H_1)$ ($p = `{python} p_list[1]`$):

$$\frac{P(`{python} p_list[indice_max]` \mid `{python} n`, `{python} y`)}{P(`{python} p_list[1]` \mid `{python} n`, `{python} y`)} = \frac{`{python} P_HiD_list[indice_max]`}{`{python} P_HiD_list[1]`} = `{python} round(P_HiD_list[indice_max]/P_HiD_list[1],2)`,$$

obtemos exatamente o mesmo resultado encontrado anteriormente pela razÃ£o de verossimilhanÃ§as. Isso ocorre porque ao assumimos uma distribuiÃ§Ã£o *a priori* uniforme, **toda a informaÃ§Ã£o que diferencia as hipÃ³teses vem exclusivamente dos dados observados**, refletida nas verossimilhanÃ§as.

### DistribuiÃ§Ã£o *a priori* informativa

Suponha agora que hÃ¡ informaÃ§Ãµes prÃ©vias indicando que a proporÃ§Ã£o de Ã¡gua sobre o globo Ã© frequentemente **acima de 0.5**. Especificamente, vamos supor que:

- $P(H_2)$ ($p = `{python} p_list[0]`$) seja **2 vezes maior** que $P(H_1)$ $p = `{python} p_list[1]`$,
- $P(H_3)$ ($p = `{python} p_list[2]`$) seja **5 vezes maior** que $P(H_1)$ $p = `{python} p_list[1]`$.  

Dessa forma, teremos uma distribuiÃ§Ã£o *a priori* definida como:

$$P(p = `{python} p_list[1]`) = \frac{1}{8}, \quad
P(p = `{python} p_list[0]`) = \frac{2}{8}, \quad
P(p = `{python} p_list[2]`) = \frac{5}{8}.$$


```{python}
#| echo: false
import numpy as np

# 'prob' foi calculado no chunk binom_likelihood_calc
lD_Hi = np.array(prob)  # verossimilhanÃ§as
priori_hip_inform = np.array([2/8, 1/8, 5/8])#np.array([2/6, 1/6, 3/6])

# Numerador: L(D|Hi) * P(Hi)
P_Hi_inform = lD_Hi * priori_hip_inform
# Denominador (normalizador): soma de L(D|Hi)*P(Hi) sobre i
P_H_inform = sum(P_Hi_inform)
lD_Hi_list = lD_Hi.tolist()
P_H_inform_list = round(P_H_inform.tolist(),2)

# Probabilidades a posteriori
P_HiD_inform = (lD_Hi * priori_hip_inform) / P_H_inform

# Armazenando para exibir
P_H_inform_round = np.round(P_H_inform, 2)
P_HiD_inform_rounded = np.round(P_HiD_inform, 2)

P_Hi_inform_list = np.round(P_Hi_inform, 2).tolist()
P_HiD_inform_list = P_HiD_inform_rounded.tolist()

# IdentificaÃ§Ã£o da maior verossimilhanÃ§a e Ã­ndice
max_P_HiD_inform_list = max(P_HiD_inform_list)
indice_max_P_HiD_inform_list = P_HiD_inform_list.index(max_P_HiD_inform_list)
```

$P(H) = \mathcal{L}(D \mid H_1) \,\cdot\, P(H_1) + \mathcal{L}(D \mid H_2) \,\cdot\, P(H_2) + \mathcal{L}(D \mid H_3) \,\cdot\, P(H_3)$

$P(H) = `{python} lD_Hi_list[1]` \times \frac{1}{8} + `{python} lD_Hi_list[0]` \times \frac{2}{8} + `{python} lD_Hi_list[2]` \times \frac{5}{8}$

$P(H) = `{python} P_Hi_inform_list[1]` + `{python} P_Hi_inform_list[0]` + `{python} P_Hi_inform_list[2]` = `{python} P_H_inform_list`$

De modo que:

$P(`{python} p_list[1]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[1]` \times \frac{1}{8}}{`{python} P_H_inform_list`} \approx `{python} P_HiD_inform_list[1]`$

$P(`{python} p_list[0]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[0]` \times \frac{2}{8}}{`{python} P_H_inform_list`} \approx `{python} P_HiD_inform_list[0]`$

$P(`{python} p_list[2]` \mid `{python} n`, `{python} y`) = \frac{`{python} lD_Hi_list[2]` \times \frac{5}{8}}{`{python} P_H_inform_list`} \approx `{python} P_HiD_inform_list[2]`$


Como resultado, observamos que a maior probabilidade *a posteriori* recai sobre a hipÃ³tese $p = `{python} p_list[indice_max_P_HiD_inform_list]`$, ainda que exista um suporte similar para $p = `{python} p_list[0]`$. Esse resultado se deve, sobretudo, Ã  **influÃªncia da distribuiÃ§Ã£o *a priori***, que atribuiu maior peso Ã  hipÃ³tese $H_3$.


Portanto, na inferÃªncia bayesiana, a distribuiÃ§Ã£o *a posteriori* de uma hipÃ³tese Ã© **proporcional ao produto entre sua verossimilhanÃ§a e sua probabilidade *a priori*** (Eq. @eq-posterior_proporcional):

$$P(H_i \mid D) \;\propto\; \mathcal{L}(H_i \mid D) \cdot P(H_i)$${#eq-posterior_proporcional}

No caso em que a distribuiÃ§Ã£o *a priori* seja uniforme em todo o espaÃ§o de hipÃ³teses, a distribuiÃ§Ã£o *a posteriori* serÃ¡ **proporcional apenas Ã  verossimilhanÃ§a** (Eq. @eq-posterior_proporcional_uniforme):

$$P(H_i \mid D) \;\propto\; \mathcal{L}(H_i \mid D)$${#eq-posterior_proporcional_uniforme}

