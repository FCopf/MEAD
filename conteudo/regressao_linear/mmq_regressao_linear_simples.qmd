---
title: "M√©todo dos M√≠nimos Quadrados na Regress√£o Linear Simples"
subtitle: "Implementa√ß√£o em Python usando √Ålgebra Matricial"
description: "Tutorial pr√°tico para implementar o m√©todo dos m√≠nimos quadrados em Python, aplicando os conceitos de √°lgebra linear e estat√≠stica b√°sica."
Categories: [
          "Regress√£o linear",
          "M√©todo dos M√≠nimos Quadrados",
          "√Ålgebra Matricial",
          "Python"
        ]

image: "images/mmq_regressao_linear_simples.png"
execute:
  echo: true
  warning: false
  include: true
  message: false
---

## üìö Introdu√ß√£o

::: {.callout-tip title="Objetivos"}

Neste tutorial, vamos implementar o **M√©todo dos M√≠nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regress√£o linear simples**.

**Objetivo**: Encontrar os coeficientes $\beta_0$ e $\beta_1$ da equa√ß√£o $\hat{y} = \beta_0 + \beta_1 x$ que melhor se ajustam aos nossos dados.

:::

## üõ†Ô∏è Importando as Bibliotecas

Primeiro, vamos importar as bibliotecas que usaremos:

```{python}
import matplotlib.pyplot as plt  # Para cria√ß√£o e manipula√ß√£o gr√°fica
import numpy as np           # Para opera√ß√µes matem√°ticas e matriciais
```

**üí° Dica**: No Google Colab, essas bibliotecas j√° v√™m instaladas!

## üìä Inserindo os Dados

Vamos trabalhar um exemplo simples em que $x$ e $y$ s√£o inseridos como listas em Python:

```{python}
# Nossos dados de exemplo
x = [0, 1, 2, 3, 4]  # Vari√°vel independente (preditora)
y = [0, 1, 1, 4, 4]  # Vari√°vel dependente (resposta)
```

Caso deseje visualizar se os objetos `x` e `y` foram criados corretamente podemos utilizar a fun√ß√£o `print()`.
```{python}
print("Valores de x:", x)
print("Valores de y:", y)
```

## üìà Visualizando os Dados

Antes de ajustar o modelo, vamos visualizar nossos dados em um gr√°figo de dispers√£o utilizando a fun√ß√£o `scatter()` da biblioteca [Matplotlib](https://matplotlib.org/):

```{python}
# Criando o gr√°fico de dispers√£o
plt.figure(figsize=(8, 6))
plt.scatter(x, y, color = '#0072B2', s=120, label='Dados observados')

# Configurando o layout gr√°fico
plt.title('Gr√°fico de Dispers√£o dos Dados', fontsize=14, fontweight='bold')
plt.xlabel('Vari√°vel X', fontsize=12)
plt.ylabel('Vari√°vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

**üìù Observa√ß√£o**: O gr√°fico sugere uma rela√ß√£o linear entre as vari√°veis, o que justifica o uso da regress√£o linear simples, que iremos ajustra por meio do MMQ.

## üßÆ Implementando o MMQ - Passo a Passo

### Criando os Vetores Base

Lembre-se da teoria: inicialmente precisamos caracterizar os vetores $\vec{f}_0$, $\vec{f}_1$ e $\vec{y}$:

$$\vec{f}_0 = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \quad \text{,} \quad \vec{f}_1 = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \quad \text{e} \quad \vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# N√∫mero de observa√ß√µes
n = len(x)

# Vetor f0: vetor de 1's (para o intercepto Œ≤‚ÇÄ)
f0 = [1] * n  # Cria uma lista com n elementos iguais a 1

# Vetor f1: nossos valores de x (para o coeficiente Œ≤‚ÇÅ)
f1 = x.copy()  # Copia os valores de x para um novo objeto denominado f1
```

Visualizando os vetores $\vec{f}_0$ e $\vec{f}_1$.
```{python}
print("Vetor f0 (intercepto):", f0)
print("Vetor f1 (coeficiente):", f1)
```

### Construindo as Matrizes X e Y

Agora vamos montar as matrizes do sistema:

$$X = \begin{bmatrix} \vec{f}_0 & \vec{f}_1 \end{bmatrix} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix} \quad \text{e} \quad Y = \begin{bmatrix} \vec{y} \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# Matriz X: combinando f0 e f1 em colunas
X = np.column_stack((f0, f1))

# Matriz Y: transformando y em matriz com n linhas e 1 coluna 
Y = np.array(y).reshape(n, 1)
```

Visualizando as matrizes $X$ e $Y$.
```{python}
print("Matriz X:")
print(X)
print("\nMatriz Y:")
print(Y)
print(f"\nDimens√µes - X: {X.shape}, Y: {Y.shape}")
```

Note agora que a matriz $X$ tem `{python} X.shape[0]` linhas e `{python} X.shape[1]` colunas, enquanto a matriz $Y$ tem `{python} Y.shape[0]` linhas e `{python} Y.shape[1]` colunas.

### Resolvendo o Sistema Normal

Agora vamos calcular os coeficientes usando as opera√ß√µes matriciais: 
$$B = (X^T X)^{-1} X^T Y$$

```{python}
# Calculando X transposta vezes X
XTX = np.dot(X.T, X)  # X.T √© a transposta de X
# Calculando X transposta vezes Y
XTY = np.dot(X.T, Y)
# Calculando a matriz inversa (X^T X)^(-1)
XTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X
# Coeficientes de regress√£o
B = np.dot(XTX_inv, XTY)
```

Visualizando os objetos
```{python}
# Calculando X transposta vezes X
print("X^T X:")
print(XTX)

print("\nX^T Y:")
print(XTY)

print("\nB:")
print(B)

```

::: {.callout-note}

1. A fun√ß√£o ¬¥np.dot()¬¥ em Python pode ser substitu√≠da pelo s√≠mbolo `@`. Teste os c√≥digos abaixo e verifique que os resultados coincidem:

```{python}
print("Usando np.dot()")
print(np.dot(X.T, X))
```

```{python}
print("Usando '@'")
print(X.T @ X)

```

2. Os elementos internos de uma matriz podem ser acessados destacando suas posi√ß√µes na linha e coluna. Considere a matrtiz `B`. O coeficiente $\beta_0$ pode ser acessado na linha 1 e coluna 1, enquanto $\beta_1$ pode ser acessado na linha 2 e coluna 1.

```{python}
print(B[0,0]) # Beta 0
```

```{python}
print(B[1,0]) # Beta 1
```
:::

### Obtendo os Valores Ajustados de y

Tendo obtido os coeficientes de regress√£o, os valores ajustados de y ($\hat{y}$) podem ser obtido pela multiplica√ß√£o matricial:

$$F = X B = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix} \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{bmatrix}$$

**Obs.**: denominamos $F$ a matriz de valores ajustados de $y$.

```{python}
# Valores ajustados (preditos)
F = np.dot(X, B)
```

```{python}
# Valores ajustados (preditos)
print(F)
```

### Avaliando a Qualidade do Ajuste

#### Calculando a Soma dos Quadrados dos Res√≠duos ($SQ_{res}$)

$SQ_{res}$ pode ser obtida pela multiplica√ß√£o matricial:

$$SQ_{res} = \boldsymbol{e}^T \boldsymbol{e}$$

Em que $\boldsymbol{e}$ √© a matriz coluna dos **res√≠duos** obtida pela diferen√ßa entre os valores observados e ajustados de $y$:

$$\boldsymbol{e} = Y - F = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} - \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_n \end{bmatrix} = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix}$$


```{python}
# Res√≠duos: diferen√ßa entre valores observados e ajustados
e = Y - F

# Soma dos Quadrados dos Res√≠duos
SQres = np.dot(e.T, e)[0, 0]
```

#### Calculando a Soma dos Quadrados Totais ($SQ_{tot}$)

$SQ_{tot}$ pode ser obtida pela multiplica√ß√£o matricial:

$$SQ_{tot} = \boldsymbol{D}^T \boldsymbol{D}$$

Em que $\boldsymbol{D}$ √© a matriz coluna dos **desvios da m√©dis** obtida pela diferen√ßa entre os valores observados de $y$ e a m√©dia de $\overline{y}$:

$$\boldsymbol{D} = Y - \overline{Y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} - \begin{bmatrix} \overline{y} \\ \overline{y} \\ \vdots \\ \overline{y} \end{bmatrix} = \begin{bmatrix} d_1 \\ d_2 \\ \vdots \\ d_n \end{bmatrix}$$

```{python}
# Soma dos Quadrados Total
Y_medio = np.mean(Y)
D = Y - Y_medio
SQtot = np.dot(D.T, D)[0, 0]
```

#### Calculando o coeficiente de determina√ß√£o $R^2$:

O $R^2$ √© dado pela express√£o:

$$R^2 = 1 - \frac{SQ_{res}}{SQ_{tot}}$$

```{python}
# Coeficiente de Determina√ß√£o R¬≤
R2 = 1 - (SQres / SQtot)
```

---

Visualizando os resultados:

```{python}
print("üìä Medidas de Qualidade do Ajuste:")
print(f"Soma dos Quadrados dos Res√≠duos (SQres): {SQres:.4f}")
print(f"Soma dos Quadrados Total (SQtot): {SQtot:.4f}")
print(f"Coeficiente de Determina√ß√£o (R¬≤): {R2:.4f}")
print(f"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%")
```


**üìù Interpreta√ß√£o do $R^2$**:

- Varia de 0 a 1
- Quanto mais pr√≥ximo de 1, melhor o ajuste
- Representa a propor√ß√£o da varia√ß√£o em $y$ explicada pelo modelo

---

## üìä Visualizando o Resultado Final

Vamos plotar os dados originais junto com a reta ajustada:

```{python}
# Criando o gr√°fico final
plt.figure(figsize=(8, 6))

# Pontos observados
plt.scatter(x, y, 
            color = '#0072B2', s=120,
            label=f'Dados observados (n={n})')

# Valores ajustados
plt.scatter(x, F[:,0],
            color='#000000', marker='*', s=120, 
            label='Valores ajustados')

# Reta ajustada
plt.plot(x, F[:,0], color='#D55E00', 
         label=fr'Reta de regress√£o: $\hat{{y}} = {B[0,0]:.3f} + {B[1,0]:.3f}x$')

# Configura√ß√µes do gr√°fico
plt.title(f'Regress√£o Linear Simples - MMQ\nR¬≤ = {R2:.4f}', 
          fontsize=14, fontweight='bold')
plt.xlabel('Vari√°vel X', fontsize=12)
plt.ylabel('Vari√°vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

## üéØ Resumo dos Resultados

```{python}
print("="*50)
print("         RESUMO DA REGRESS√ÉO LINEAR")
print("="*50)
print(f"Equa√ß√£o ajustada: y = {B[0,0]:.4f} + {B[1,0]:.4f}x")
print(f"Coeficiente de determina√ß√£o (R¬≤): {R2:.4f}")
print(f"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%")
print("="*50)
```

## üßæ Resumo do C√≥digo

1. Inser√ß√£o dos Dados
```{python}
x = [0, 1, 2, 3, 4]
y = [0, 1, 1, 4, 4]
```

2. Defini√ß√£o das matrizes do sistema
```{python}
n = len(x)
f0 = [1] * n
f1 = x.copy()

X = np.column_stack((f0, f1))
Y = np.array(y).reshape(n, 1)
```

3. C√°lculo dos coeficientes
```{python}
XTX = X.T @ X
XTY = X.T @ Y
XTX_inv = np.linalg.inv(XTX)
B = XTX_inv @ XTY
```

4. Qualidade do ajuste

```{python}
F = X @ B
e = Y - F
SQres = (e.T @ e)[0, 0]

Y_medio = np.mean(Y)
D = Y - Y_medio
SQtot = (D.T @ D)[0, 0]

R2 = 1 - (SQres / SQtot)
```

## üöÄ Exerc√≠cio Pr√°tico

Implemente o MMQ com novos dados:

```{python}
# Experimente com estes dados:
x_novo = [1, 2, 3, 4, 5, 6]
y_novo = [2, 4, 5, 4, 5, 7]

# Dica: voc√™ pode copiar e adaptar o c√≥digo acima!
```