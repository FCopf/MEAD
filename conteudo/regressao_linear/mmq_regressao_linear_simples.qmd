---
title: "M√©todo dos M√≠nimos Quadrados na Regress√£o Linear Simples"
subtitle: "Implementa√ß√£o em Python usando √Ålgebra Matricial"
description: "Tutorial pr√°tico para implementar o m√©todo dos m√≠nimos quadrados em Python, aplicando os conceitos de √°lgebra linear e estat√≠stica b√°sica."
Categories: [
          "Regress√£o linear",
          "M√©todo dos M√≠nimos Quadrados",
          "√Ålgebra Matricial",
          "Python"
        ]

image: "images/mmq_regressao_linear_simples.png"
execute:
  echo: true
  warning: false
  include: true
  message: false
---

## üìö Introdu√ß√£o

::: {.callout-tip title="Objetivos"}

Neste tutorial, vamos implementar o **M√©todo dos M√≠nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regress√£o linear simples**.

**Objetivo**: Encontrar os coeficientes $\beta_0$ e $\beta_1$ da equa√ß√£o $\hat{y} = \beta_0 + \beta_1 x$ que melhor se ajustam aos nossos dados.

:::



## üõ†Ô∏è Importando as Bibliotecas

Primeiro, vamos importar as bibliotecas que usaremos:

```{python}
import pandas as pd           # Para manipula√ß√£o de dados
import matplotlib.pyplot as plt  # Para criar gr√°ficos
import seaborn as sns        # Para gr√°ficos mais bonitos
import numpy as np           # Para opera√ß√µes matem√°ticas e matriciais
```

**üí° Dica**: No Google Colab, essas bibliotecas j√° v√™m instaladas!

## üìä Definindo os Dados

Vamos trabalhar um exemplo simples em que $x$ e $y$ s√£o inseridos como listas em Python:

```{python}
# Nossos dados de exemplo
x = [0, 1, 2, 3, 4]  # Vari√°vel independente (preditora)
y = [0, 1, 1, 4, 4]  # Vari√°vel dependente (resposta)

print("Valores de x:", x)
print("Valores de y:", y)
print("N√∫mero de observa√ß√µes:", len(x))
```

## üìà Visualizando os Dados

Antes de ajustar o modelo, vamos visualizar nossos dados:

```{python}
# Criando o gr√°fico de dispers√£o
plt.figure(figsize=(8, 6))
plt.scatter(x, y, color='blue', marker='o', s=80, alpha=0.7, label='Dados observados')

# Configurando o gr√°fico
plt.title('Gr√°fico de Dispers√£o dos Dados', fontsize=14, fontweight='bold')
plt.xlabel('Vari√°vel X', fontsize=12)
plt.ylabel('Vari√°vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

**üìù Observa√ß√£o**: O gr√°fico sugere uma rela√ß√£o linear entre as vari√°veis, o que justifica o uso da regress√£o linear simples.

## üßÆ Implementando o MMQ - Passo a Passo

### Criando os Vetores Base

Lembre-se da teoria: precisamos dos vetores $\vec{f}_0$, $\vec{f}_1$ e $\vec{y}$:

$$\vec{f}_0 = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \quad \text{,} \quad \vec{f}_1 = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \quad \text{e} \quad \vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# N√∫mero de observa√ß√µes
n = len(x)

# Vetor f0: vetor de 1's (para o intercepto Œ≤‚ÇÄ)
f0 = [1] * n  # Cria uma lista com n elementos iguais a 1

# Vetor f1: nossos valores de x (para o coeficiente Œ≤‚ÇÅ)
f1 = x.copy()  # Copia os valores de x

print("Vetor f0 (intercepto):", f0)
print("Vetor f1 (coeficiente):", f1)
```

### Construindo as Matrizes X e Y

Agora vamos montar as matrizes do sistema:

$$X = \begin{bmatrix} \vec{f}_0 & \vec{f}_1 \end{bmatrix} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix} \quad \text{e} \quad Y = \begin{bmatrix} \hat{y} \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

```{python}
# Matriz X: combinando f0 e f1 em colunas
X = np.column_stack((f0, f1))

# Matriz Y: transformando y em matriz com n linhas e 1 coluna 
Y = np.array(y).reshape(n, 1)

print("Matriz X:")
print(X)
print("\nMatriz Y:")
print(Y)
print(f"\nDimens√µes - X: {X.shape}, Y: {Y.shape}")
```

### Resolvendo o Sistema Normal

Agora vamos calcular os coeficientes usando a f√≥rmula: 

$$\boldsymbol{\hat{\beta}} = (X^T X)^{-1} X^T Y$$

```{python}
# Calculando X transposta vezes X
XTX = np.dot(X.T, X)  # X.T √© a transposta de X
print("X^T X:")
print(XTX)

# Calculando X transposta vezes Y
XTY = np.dot(X.T, Y)
print("\nX^T Y:")
print(XTY)

# Calculando os coeficientes: B = (X^T X)^(-1) (X^T Y)
XTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X
B = np.dot(XTX_inv, XTY)

print("\nüéØ Coeficientes estimados:")
print(f"Œ≤‚ÇÄ (intercepto) = {B[0, 0]:.4f}")
print(f"Œ≤‚ÇÅ (inclina√ß√£o) = {B[1, 0]:.4f}")
```

**üìö Interpreta√ß√£o**:

- $\beta_0$: valor de y quando x = 0
- $\beta_1$: o quanto y aumenta para o aumento de **uma unidade** em x

::: {.callout-note}

A fun√ß√£o ¬¥np.dot()¬¥ em Python tamb√©m pode ser substitu√≠da pelo s√≠mbolo `@`. Teste os c√≥digos abaixo e verifique que os resultados coincidem:

```{python}
print("Usando np.dot()")
print(np.dot(X.T, X))
```

```{python}
print("Usando '@'")
print(X.T @ X)

```

:::

### Avaliando a Qualidade do Ajuste

Vamos calcular o coeficiente de determina√ß√£o $R^2$:

```{python}
# Valores ajustados (preditos)
Y_ajustado = np.dot(X, B)

# Res√≠duos: diferen√ßa entre valores observados e ajustados
residuos = Y - Y_ajustado

# Soma dos Quadrados dos Res√≠duos
SQres = np.dot(residuos.T, residuos)[0, 0]

# Soma dos Quadrados Total
Y_medio = np.mean(Y)
desvios_media = Y - Y_medio
SQtot = np.dot(desvios_media.T, desvios_media)[0, 0]

# Coeficiente de Determina√ß√£o R¬≤
R2 = 1 - (SQres / SQtot)

print("üìä Medidas de Qualidade do Ajuste:")
print(f"Soma dos Quadrados dos Res√≠duos (SQres): {SQres:.4f}")
print(f"Soma dos Quadrados Total (SQtot): {SQtot:.4f}")
print(f"Coeficiente de Determina√ß√£o (R¬≤): {R2:.4f}")
print(f"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%")
```


**üìù Interpreta√ß√£o do $R^2$**:

- Varia de 0 a 1
- Quanto mais pr√≥ximo de 1, melhor o ajuste
- Representa a propor√ß√£o da varia√ß√£o em $y$ explicada pelo modelo

## üìä Visualizando o Resultado Final

Vamos plotar os dados originais junto com a reta ajustada:

```{python}
# Criando pontos para desenhar a reta
y_ajustados = B[0, 0] + B[1, 0] * np.array(x)  # Equa√ß√£o da reta: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx

print("y_ajustados:")
print(y_ajustados)
```

```{python}
# Criando o gr√°fico final
plt.figure(figsize=(8, 6))

# Pontos observados
plt.scatter(x, y, color='blue', marker='o', s=100, alpha=0.7, 
           label=f'Dados observados (n={n})', zorder=3)

# Valores ajustados
Y_pontos = B[0, 0] + B[1, 0] * np.array(x)
plt.scatter(x, Y_pontos, color='red', marker='x', s=80, 
           label='y ajustado', zorder=3)

# Reta ajustada
plt.plot(x, y_ajustados, color='red', linewidth=2, 
         label=fr'Reta de regress√£o: $\hat{{y}} = {B[0,0]:.3f} + {B[1,0]:.3f}x$')

# Linhas dos res√≠duos
for i in range(len(x)):
    plt.plot([x[i], x[i]], [y[i], Y_pontos[i]], 'gray', linestyle='--', alpha=0.5, label = 'Res√≠duos' if i == 0 else '')

# Configura√ß√µes do gr√°fico
plt.title(f'Regress√£o Linear Simples - MMQ\nR¬≤ = {R2:.4f}', 
          fontsize=14, fontweight='bold')
plt.xlabel('Vari√°vel X', fontsize=12)
plt.ylabel('Vari√°vel Y', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()
```

## üéØ Resumo dos Resultados

```{python}
print("="*50)
print("         RESUMO DA REGRESS√ÉO LINEAR")
print("="*50)
print(f"Equa√ß√£o ajustada: y = {B[0,0]:.4f} + {B[1,0]:.4f}x")
print(f"Coeficiente de determina√ß√£o (R¬≤): {R2:.4f}")
print(f"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%")
print("="*50)
```

## üöÄ Exerc√≠cio Pr√°tico

Teste o c√≥digo com novos dados:

```{python}
# Experimente com estes dados:
x_novo = [1, 2, 3, 4, 5, 6]
y_novo = [2, 4, 5, 4, 5, 7]

# Implemente todo o processo do MMQ com os novos dados
# Dica: voc√™ pode copiar e adaptar o c√≥digo acima!
```