---
title: "Explorando Modelos de Regressão Bayesiana"
subtitle: "Dos modelos lineares a respostas generalizadas e estruturas hierárquicas"
author: "Prof. Fabio Cop"
image: "images/regressao-glm-hierarquico-apresentacao.png"
format:
  revealjs: 
    institute: Laboratório de Ecologia e Dinâmica de Comunidades (LaEDCom)
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/logo_white.png 
    footer: |
      Instituto do Mar - Unifesp · <a href="https://fcopf.github.io/MEAD/" target="_blank">Métodos em Estatística e Análise de Dados</a>
    theme: [default, custom.scss]
title-slide-attributes:
   data-background: "linear-gradient(135deg, #d0dee4, #b8c0ce, #d6e0e4)"

execute:
    eval: true
---

## O que aprendemos até aqui: prioris e posterioris

<iframe src="https://fcopf-binomial-bayesiana.share.connect.posit.cloud/" style="width: 100%; height: 80%; zoom: 0.6;" frameborder="0"></iframe>

---

## O que aprendemos até aqui: distribuição Normal de Probabilidade

$$
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2} \left(\frac{y - \mu}{\sigma} \right)^2} \longrightarrow \quad y \sim \mathcal{N}(\mu, \sigma)
$$

![](images/normal-distributions.png){width=100% fig-align="center"}


---

## O que aprendemos até aqui: o modelo de Regressão linear

::: columns

::: {.column width="50%"}


---

**Variável aleatória resposta**

---

$$
y \sim \mathcal{N}(\mu, \sigma)
$$

$$
\mu = \beta_0 + \beta_1 x
$$

---

**Prioris**

---

$$
\beta_0 \sim \mathcal{N}(\mu_{\beta_0}, \sigma_{\beta_0})
$$

$$
\beta_1 \sim \mathcal{N}(\mu_{\beta_1}, \sigma_{\beta_1})
$$

$$
\sigma \sim \text{Lognormal}(\mu_{\log \sigma}, \sigma_{\log \sigma})
$$


:::

::: {.column width="50%"}

![](images/Regre2.png){width=150%}

:::

:::

---

## O que aprendemos até aqui: Programação Probabilística

::: columns

::: {.column width="50%"}


---

**PyMC**

---

```{.python code-line-numbers="|1|2-5|7-10|12-13"}
with pm.Model() as modelo:
    # Definição das prioris
    Intercept = pm.Normal("Intercept", mu=60, sigma=5)
    calcado = pm.Normal("calcado", mu=2.8, sigma=0.1)
    sigma = pm.HalfNormal("sigma", sigma=10)
    
    # Definição do modelo
    mu = beta_0 + beta_1 * X
    altura = pm.Normal("altura", mu=mu, sigma=sigma, 
                        observed=Y)
    
    # Amostra a distribuição posterior
    resultados = pm.sample()
```



:::

::: {.column width="50%"}

---

**Bambi**

---

```{.python code-line-numbers="|1-6|8-10|12-13"}
# Definição das prioris
custom_priors = {
    "Intercept": bmb.Prior("Normal", mu=60, sigma=5),
    "calcado": bmb.Prior("Normal", mu=2.8, sigma=0.1),
    "sigma": bmb.Prior("HalfNormal", sigma=10)
}

# Definição do modelo
modelo = bmb.Model("altura ~ calcado", df, 
                    priors=custom_priors)

# Amostra a distribuição posterior
resultados = modelo.fit()
```

:::

:::

---

## O que aprendemos até aqui: ajuste da posteriori

::: columns

::: {.column width="50%"}

![](images/trace-plots.png)


:::

::: {.column width="50%"}

![](images/regressao_linear_retas.png)

:::

:::

---

## Daqui para frente: uma variedade de modelos e estruturas

1.  Extenção da Regressão Linear para:
    -   Múltiplos preditores.
    -   Diferentes tipos de variáveis resposta (GLMs).
    -   Dados com estrutura de agrupamento (Modelos Hierárquicos).


---

## Regressão linear múltipla


::: columns


::: {.column width="50%"}

---

**Variável aleatória resposta**

---

$$
y \sim \mathcal{N}(\mu, \sigma)
$$

$$
\mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k
$$

---

**Prioris**

---

$$
\beta_0 \sim \mathcal{N}(\mu_{\beta_0}, \sigma_{\beta_0})
$$

$$
\beta_j \sim \mathcal{N}(\mu_{\beta_j}, \sigma_{\beta_j}) \quad \text{para } j = 1, \dots, k
$$

$$
\sigma \sim \text{Lognormal}(\mu_{\log \sigma}, \sigma_{\log \sigma})
$$


:::


::: {.column width="50%"}

<!-- ![](images/RegreMultipla.png){width=150%} -->

```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. Simulação dos dados
np.random.seed(42)
n = 100
X1 = np.random.uniform(0, 10, n)
X2 = np.random.uniform(0, 10, n)

# Coeficientes da regressão
beta_0 = 3
beta_1 = 2
beta_2 = -1

# Função de ligação
mu = beta_0 + beta_1 * X1 + beta_2 * X2
sigma = 3
# Variável aleatória resposta
y = np.random.normal(mu, sigma, n)


# 2. Gráfico 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Dispersão dos pontos simulados
ax.scatter(X1, X2, y, color='blue', alpha=0.6, label='Dados simulados')

# Geração do plano
x1_grid, x2_grid = np.meshgrid(np.linspace(0, 10, 30), np.linspace(0, 10, 30))
y_grid = beta_0 + beta_1 * x1_grid + beta_2 * x2_grid

ax.plot_surface(x1_grid, x2_grid, y_grid, alpha=0.4, color='orange', label='Plano de regressão')

# Ajusta a visualização
ax.view_init(elev=30, azim=45)

# Rótulos e título
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.show()

```

:::


:::

---

## Programação Probabilística


::: columns


::: {.column width="50%"}

---

**PyMC**

---

```{.python}
with pm.Model() as modelo:
    # Definição das prioris
    Intercept = pm.Normal("Intercept", mu=60, sigma=5)
    beta_1 = pm.Normal("beta_1", mu=2.8, sigma=0.1)
    beta_2 = pm.Normal("beta_2", mu=1.5, sigma=0.1)
    sigma = pm.HalfNormal("sigma", sigma=10)

    # Definição do modelo
    mu = Intercept + beta_1 * X1 + beta_2 * X2
    altura = pm.Normal("altura", mu=mu, sigma=sigma, 
                       observed=Y)

    # Amostra a distribuição posterior
    resultados = pm.sample()
```


:::


::: {.column width="50%"}

---

**Bambi**

---

```{.python}
# Definição das prioris
custom_priors = {
    "Intercept": bmb.Prior("Normal", mu=60, sigma=5),
    "X1": bmb.Prior("Normal", mu=2.8, sigma=0.1),
    "X2": bmb.Prior("Normal", mu=1.5, sigma=0.1),
    "sigma": bmb.Prior("HalfNormal", sigma=10)
}

# Definição do modelo
modelo = bmb.Model("altura ~ X1 + X2", df, 
                   priors=custom_priors)

# Amostra a distribuição posterior
resultados = modelo.fit()

```


:::


:::

---

## Regressão de Poisson: dados de contagem

::: columns

::: {.column width="30%"}
$$
y \sim \text{Poisson}(\lambda)
$$

$$
\log(\lambda) = \mu = \beta_0 + \beta_1 x
$$

:::

::: {.column width="70%"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

# 1. Simulação dos dados
np.random.seed(42)
n = 100
X1 = np.random.uniform(0, 10, n)

# Coeficientes
beta_0 = 2.0
beta_1 = -0.3

# Valor esperado (função de ligação log)
mu = np.exp(beta_0 + beta_1 * X1)

# Variável resposta de contagem
y = np.random.poisson(mu)

# 2. Gráfico 2D com scatter e linha média
plt.figure(figsize=(10, 6))

# Pontos simulados
plt.scatter(X1, y, color='blue', alpha=0.6, label='Dados observados')

# Linha média esperada
x1_sorted = np.linspace(0, 10, 200)
mu_pred = np.exp(beta_0 + beta_1 * x1_sorted)
plt.plot(x1_sorted, mu_pred, color='red', linewidth=2, label='Valor ajustado')

# Rótulos e 
plt.tick_params(axis='both', labelsize=20)
plt.xlabel('X1', fontsize = 18)
plt.ylabel('y (contagem)', fontsize = 18)
plt.title('Regressão de Poisson: y ~ Poisson(exp(β₀ + β₁·X₁))', fontsize = 18)
plt.legend(fontsize = 13)
plt.grid(True)
plt.tight_layout()
plt.show()

```

::: 

::: 

---

## Regressão de Poisson: dados de contagem

::: columns

::: {.column width="50%"}

$$
f(y) = \frac{e^{-\lambda} \lambda^y}{y!} \longrightarrow \quad y \sim \text{Poisson}(\lambda)
$$

$$
\log(\lambda) = \mu
$$

:::

::: {.column width="50%"}

---

**Variável aleatória resposta**

---

$$
y \sim \text{Poisson}(\lambda)
$$

$$
\log(\lambda) = \mu = \beta_0 + \beta_1 x
$$

---

**Prioris**

---

$$
\beta_0 \sim \mathcal{N}(\mu_{\beta_0}, \sigma_{\beta_0})
$$

$$
\beta_1 \sim \mathcal{N}(\mu_{\beta_1}, \sigma_{\beta_1})
$$

:::

:::

---

## Programação Probabilística

::: columns

::: {.column width="50%"}

---

**PyMC**

---

```{.python}
with pm.Model() as modelo:
    # Definição das prioris
    Intercept = pm.Normal("Intercept", mu=0, sigma=5)
    beta = pm.Normal("beta", mu=0, sigma=2)
    
    # Função de ligação log: log(λ) = μ = Intercept + beta * X
    mu = Intercept + beta * X
    lambda_ = pm.math.exp(mu)
    
    # Modelo de verossimilhança
    contagem = pm.Poisson("contagem", mu=lambda_, 
                          observed=Y)
    
    # Amostragem
    resultados = pm.sample()
```

:::

::: {.column width="50%"}

---

**Bambi**

---

```{.python}
# Definição das prioris
custom_priors = {
    "Intercept": bmb.Prior("Normal", mu=0, sigma=5),
    "x": bmb.Prior("Normal", mu=0, sigma=2),
}

# Modelo com função de ligação log (default da família Poisson)
modelo = bmb.Model("contagem ~ x", df, 
                   family="poisson", priors=custom_priors)

# Amostragem
resultados = modelo.fit()
```

:::

:::

---

## Regressão Logística: dados dicotômicos

::: columns

::: {.column width="40%"}

$$
y \sim \text{Bernoulli}(p)
$$

$$
\log\left(\frac{p}{1-p}\right) = \mu = \beta_0 + \beta_1 x
$$

$$
p = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}
$$

:::

::: {.column width="60%"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

# 1. Simulação dos dados
np.random.seed(42)
n = 100
X1 = np.random.uniform(0, 10, n)

# Coeficientes
beta_0 = -6.0
beta_1 = 1.0

# Probabilidade de sucesso (função logística)
logits = beta_0 + beta_1 * X1
p = 1 / (1 + np.exp(-logits))

# Variável resposta binária
y = np.random.binomial(1, p)

# 2. Gráfico 2D com scatter e curva logística
plt.figure(figsize=(10, 6))

# Pontos simulados
plt.scatter(X1, y, color='blue', alpha=0.65, label='Dados observados', s = 100)

# Linha da probabilidade estimada
x1_sorted = np.linspace(0, 10, 300)
p_pred = 1 / (1 + np.exp(-(beta_0 + beta_1 * x1_sorted)))
plt.plot(x1_sorted, p_pred, color='red', linewidth=2, label='Probabilidade ajustada')

# Rótulos e título
plt.xlabel('X1', fontsize=18)
plt.ylabel('y (classe)', fontsize=18)
plt.title('Regressão Logística: y ~ Bernoulli(p), p = logistic(β₀ + β₁·X₁)', fontsize=18)
plt.legend(loc = 'center left', fontsize=14)
plt.grid(True)
plt.tight_layout()
plt.ylim(-0.1, 1.1)
plt.show()
```

:::

:::

---

## Regressão Logística: dados dicotômicos

::: columns

::: {.column width="50%"}

---

**Distribuição da variável resposta**

---

$$
f(y) = p^y (1 - p)^{1 - y}
$$

$$
y \sim \text{Bernoulli}(p)
$$

---

**Função de ligação**

---

$$
\text{logit}(p) = \mu = \beta_0 + \beta_1 x
$$

$$
\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)
$$

$$
\log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x
$$

:::

::: {.column width="50%"}

$$
\frac{p}{1 - p} = e^{\beta_0 + \beta_1 x}
$$


$$
p = (1 - p) \cdot e^{\beta_0 + \beta_1 x}
$$


$$
p = e^{\beta_0 + \beta_1 x} - p \cdot e^{\beta_0 + \beta_1 x}
$$

$$
p \left(1 + e^{\beta_0 + \beta_1 x}\right) = e^{\beta_0 + \beta_1 x}
$$

$$
p = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}
$$

:::

:::

---

## Programação Probabilística

::: columns

::: {.column width="50%"}

---

**PyMC**

---

```{.python}
with pm.Model() as modelo:
    # Definição das prioris
    Intercept = pm.Normal("Intercept", mu=0, sigma=5)
    beta = pm.Normal("beta", mu=0, sigma=2)
    
    # Preditor linear e função de ligação logit
    mu = Intercept + beta * X
    p = pm.math.sigmoid(mu)
    
    # Verossimilhança
    y_obs = pm.Bernoulli("y_obs", p=p, observed=Y)
    
    # Amostragem
    resultados = pm.sample()
```

:::

::: {.column width="50%"}

---

**Bambi**

---

```{.python}
# Definição das prioris
custom_priors = {
    "Intercept": bmb.Prior("Normal", mu=0, sigma=5),
    "x": bmb.Prior("Normal", mu=0, sigma=2),
}

# Modelo logístico (ligação logit é padrão para bernoulli)
modelo = bmb.Model("y ~ x", df, 
                   family="bernoulli", priors=custom_priors)

# Amostragem
resultados = modelo.fit()
```

:::

:::

---

## Modelo Hierárquico Normal com Intercepto e Inclinação Variáveis

::: columns

::: {.column width="30%"}

$$
y_{ij} \sim \mathcal{N}(\mu_{ij}, \sigma^2)
$$

$$
\mu_{ij} = \beta_{0j} + \beta_{1j} x_{ij}
$$

$$
\beta_{0j} \sim \mathcal{N}(\gamma_0, \tau_0^2) \\
\beta_{1j} \sim \mathcal{N}(\gamma_1, \tau_1^2)
$$

:::

::: {.column width="70%"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

# 1. Simulação de dados hierárquicos
np.random.seed(42)
n_groups = 5
n_per_group = 30

# Hiperparâmetros
gamma_0 = 2.0
gamma_1 = 1.0
tau_0 = 1.0
tau_1 = 0.5
sigma = 1.0

X = []
Y = []
group_labels = []

for j in range(n_groups):
    # Sorteia coeficientes específicos por grupo
    beta_0j = np.random.normal(gamma_0, tau_0)
    beta_1j = np.random.normal(gamma_1, tau_1)
    
    # Simula preditor e resposta para o grupo j
    x_j = np.random.uniform(0, 10, n_per_group)
    mu_j = beta_0j + beta_1j * x_j
    y_j = np.random.normal(mu_j, sigma)
    
    X.append(x_j)
    Y.append(y_j)
    group_labels.extend([j] * n_per_group)

# Empilha todos os grupos
X = np.concatenate(X)
Y = np.concatenate(Y)
group_labels = np.array(group_labels)

# 2. Gráfico com uma linha de tendência por grupo
plt.figure(figsize=(10, 6))

colors = plt.cm.tab10(np.linspace(0, 1, n_groups))
for j in range(n_groups):
    idx = group_labels == j
    plt.scatter(X[idx], Y[idx], alpha=0.6, label=f'Grupo {j}', color=colors[j])
    
    # Ajuste linear visual (para ilustração apenas)
    x_seq = np.linspace(0, 10, 100)
    beta_0j = np.polyfit(X[idx], Y[idx], 1)[1]
    beta_1j = np.polyfit(X[idx], Y[idx], 1)[0]
    plt.plot(x_seq, beta_0j + beta_1j * x_seq, color=colors[j])

# Rótulos e título
plt.xlabel('X', fontsize=18)
plt.ylabel('y', fontsize=18)
plt.title('Modelo Hierárquico: y ~ N(β₀ⱼ + β₁ⱼ·X, σ²)', fontsize=18)
plt.legend(fontsize=18)
plt.grid(True)
plt.tight_layout()
plt.show()
```

:::

:::

---

## Variação entre grupos: coeficientes do modelo hierárquico

::: columns

::: {.column width="30%"}

Coeficientes específicos por grupo:

$$
\beta_{0j} \sim \mathcal{N}(\gamma_0, \tau_0^2) \\
\beta_{1j} \sim \mathcal{N}(\gamma_1, \tau_1^2)
$$

Visualizando a dispersão dos parâmetros em relação às médias populacionais $\gamma_0, \gamma_1$.

:::

::: {.column width="70%"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Reutilizando os parâmetros e grupos da simulação anterior
np.random.seed(42)
n_groups = 5
gamma_0 = 2.0
gamma_1 = 1.0
tau_0 = 1.0
tau_1 = 0.5

# Amostragem dos coeficientes por grupo
beta_0 = np.random.normal(gamma_0, tau_0, n_groups)
beta_1 = np.random.normal(gamma_1, tau_1, n_groups)

# Gráfico: dispersão dos coeficientes por grupo
plt.figure(figsize=(8, 6))
plt.scatter(beta_0, beta_1, color='purple', s=80)

colors = plt.cm.tab10(np.linspace(0, 1, n_groups))
for j in range(n_groups):
    plt.scatter(beta_0[j], beta_1[j], color=colors[j], s=80)
    plt.text(beta_0[j] + 0.05, beta_1[j], f'Grupo {j}', fontsize=14)


# Médias populacionais (hiperparâmetros)
plt.axvline(gamma_0, color='gray', linestyle='--', label=r'$\gamma_0$')
plt.axhline(gamma_1, color='black', linestyle='--', label=r'$\gamma_1$')

plt.xlabel(r'Intercepto $\beta_{0j}$', fontsize=18)
plt.ylabel(r'Inclinação $\beta_{1j}$', fontsize=18)
plt.title('Distribuição dos coeficientes por grupo', fontsize=18)
plt.grid(True)
plt.legend(fontsize=13)
plt.tight_layout()
plt.show()
```

:::

:::
