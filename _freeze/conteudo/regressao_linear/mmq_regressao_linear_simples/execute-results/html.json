{
  "hash": "954a1a1d24a568a048f4ad39cab698d8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"M√©todo dos M√≠nimos Quadrados na Regress√£o Linear Simples\"\nsubtitle: \"Implementa√ß√£o em Python usando √Ålgebra Matricial\"\ndescription: \"Tutorial pr√°tico para implementar o m√©todo dos m√≠nimos quadrados em Python, aplicando os conceitos de √°lgebra linear e estat√≠stica b√°sica.\"\nCategories: [\n          \"Regress√£o linear\",\n          \"M√©todo dos M√≠nimos Quadrados\",\n          \"√Ålgebra Matricial\",\n          \"Python\"\n        ]\n\nimage: \"images/mmq_regressao_linear_simples.png\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n---\n\n\n## üìö Introdu√ß√£o\n\n::: {.callout-tip title=\"Objetivos\"}\n\nNeste tutorial, vamos implementar o **M√©todo dos M√≠nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regress√£o linear simples**.\n\n**Objetivo**: Encontrar os coeficientes $\\beta_0$ e $\\beta_1$ da equa√ß√£o $\\hat{y} = \\beta_0 + \\beta_1 x$ que melhor se ajustam aos nossos dados.\n\n:::\n\n\n\n## üõ†Ô∏è Importando as Bibliotecas\n\nPrimeiro, vamos importar as bibliotecas que usaremos:\n\n::: {#5facf568 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd           # Para manipula√ß√£o de dados\nimport matplotlib.pyplot as plt  # Para criar gr√°ficos\nimport seaborn as sns        # Para gr√°ficos mais bonitos\nimport numpy as np           # Para opera√ß√µes matem√°ticas e matriciais\n```\n:::\n\n\n**üí° Dica**: No Google Colab, essas bibliotecas j√° v√™m instaladas!\n\n## üìä Definindo os Dados\n\nVamos trabalhar um exemplo simples em que $x$ e $y$ s√£o inseridos como listas em Python:\n\n::: {#c7e3d582 .cell execution_count=2}\n``` {.python .cell-code}\n# Nossos dados de exemplo\nx = [0, 1, 2, 3, 4]  # Vari√°vel independente (preditora)\ny = [0, 1, 1, 4, 4]  # Vari√°vel dependente (resposta)\n\nprint(\"Valores de x:\", x)\nprint(\"Valores de y:\", y)\nprint(\"N√∫mero de observa√ß√µes:\", len(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores de x: [0, 1, 2, 3, 4]\nValores de y: [0, 1, 1, 4, 4]\nN√∫mero de observa√ß√µes: 5\n```\n:::\n:::\n\n\n## üìà Visualizando os Dados\n\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n::: {#550361ed .cell execution_count=3}\n``` {.python .cell-code}\n# Criando o gr√°fico de dispers√£o\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, color='blue', marker='o', s=80, alpha=0.7, label='Dados observados')\n\n# Configurando o gr√°fico\nplt.title('Gr√°fico de Dispers√£o dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('Vari√°vel X', fontsize=12)\nplt.ylabel('Vari√°vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_linear_simples_files/figure-html/cell-4-output-1.png){width=666 height=529}\n:::\n:::\n\n\n**üìù Observa√ß√£o**: O gr√°fico sugere uma rela√ß√£o linear entre as vari√°veis, o que justifica o uso da regress√£o linear simples.\n\n## üßÆ Implementando o MMQ - Passo a Passo\n\n### Criando os Vetores Base\n\nLembre-se da teoria: precisamos dos vetores $\\vec{f}_0$, $\\vec{f}_1$ e $\\vec{y}$:\n\n$$\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#766f9c65 .cell execution_count=4}\n``` {.python .cell-code}\n# N√∫mero de observa√ß√µes\nn = len(x)\n\n# Vetor f0: vetor de 1's (para o intercepto Œ≤‚ÇÄ)\nf0 = [1] * n  # Cria uma lista com n elementos iguais a 1\n\n# Vetor f1: nossos valores de x (para o coeficiente Œ≤‚ÇÅ)\nf1 = x.copy()  # Copia os valores de x\n\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (coeficiente):\", f1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVetor f0 (intercepto): [1, 1, 1, 1, 1]\nVetor f1 (coeficiente): [0, 1, 2, 3, 4]\n```\n:::\n:::\n\n\n### Construindo as Matrizes X e Y\n\nAgora vamos montar as matrizes do sistema:\n\n$$X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#edf03198 .cell execution_count=5}\n``` {.python .cell-code}\n# Matriz X: combinando f0 e f1 em colunas\nX = np.column_stack((f0, f1))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna \nY = np.array(y).reshape(n, 1)\n\nprint(\"Matriz X:\")\nprint(X)\nprint(\"\\nMatriz Y:\")\nprint(Y)\nprint(f\"\\nDimens√µes - X: {X.shape}, Y: {Y.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatriz X:\n[[1 0]\n [1 1]\n [1 2]\n [1 3]\n [1 4]]\n\nMatriz Y:\n[[0]\n [1]\n [1]\n [4]\n [4]]\n\nDimens√µes - X: (5, 2), Y: (5, 1)\n```\n:::\n:::\n\n\n### Resolvendo o Sistema Normal\n\nAgora vamos calcular os coeficientes usando a f√≥rmula: \n\n$$\\boldsymbol{\\hat{\\beta}} = (X^T X)^{-1} X^T Y$$\n\n::: {#4cdfaadf .cell execution_count=6}\n``` {.python .cell-code}\n# Calculando X transposta vezes X\nXTX = np.dot(X.T, X)  # X.T √© a transposta de X\nprint(\"X^T X:\")\nprint(XTX)\n\n# Calculando X transposta vezes Y\nXTY = np.dot(X.T, Y)\nprint(\"\\nX^T Y:\")\nprint(XTY)\n\n# Calculando os coeficientes: B = (X^T X)^(-1) (X^T Y)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\nB = np.dot(XTX_inv, XTY)\n\nprint(\"\\nüéØ Coeficientes estimados:\")\nprint(f\"Œ≤‚ÇÄ (intercepto) = {B[0, 0]:.4f}\")\nprint(f\"Œ≤‚ÇÅ (inclina√ß√£o) = {B[1, 0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX^T X:\n[[ 5 10]\n [10 30]]\n\nX^T Y:\n[[10]\n [31]]\n\nüéØ Coeficientes estimados:\nŒ≤‚ÇÄ (intercepto) = -0.2000\nŒ≤‚ÇÅ (inclina√ß√£o) = 1.1000\n```\n:::\n:::\n\n\n**üìö Interpreta√ß√£o**:\n\n- $\\beta_0$: valor de y quando x = 0\n- $\\beta_1$: o quanto y aumenta para o aumento de **uma unidade** em x\n\n::: {.callout-note}\n\nA fun√ß√£o ¬¥np.dot()¬¥ em Python tamb√©m pode ser substitu√≠da pelo s√≠mbolo `@`. Teste os c√≥digos abaixo e verifique que os resultados coincidem:\n\n::: {#64279951 .cell execution_count=7}\n``` {.python .cell-code}\nprint(\"Usando np.dot()\")\nprint(np.dot(X.T, X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsando np.dot()\n[[ 5 10]\n [10 30]]\n```\n:::\n:::\n\n\n::: {#6559018a .cell execution_count=8}\n``` {.python .cell-code}\nprint(\"Usando '@'\")\nprint(X.T @ X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsando '@'\n[[ 5 10]\n [10 30]]\n```\n:::\n:::\n\n\n:::\n\n### Avaliando a Qualidade do Ajuste\n\nVamos calcular o coeficiente de determina√ß√£o $R^2$:\n\n::: {#aa1a3cf6 .cell execution_count=9}\n``` {.python .cell-code}\n# Valores ajustados (preditos)\nY_ajustado = np.dot(X, B)\n\n# Res√≠duos: diferen√ßa entre valores observados e ajustados\nresiduos = Y - Y_ajustado\n\n# Soma dos Quadrados dos Res√≠duos\nSQres = np.dot(residuos.T, residuos)[0, 0]\n\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\ndesvios_media = Y - Y_medio\nSQtot = np.dot(desvios_media.T, desvios_media)[0, 0]\n\n# Coeficiente de Determina√ß√£o R¬≤\nR2 = 1 - (SQres / SQtot)\n\nprint(\"üìä Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos Res√≠duos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de Determina√ß√£o (R¬≤): {R2:.4f}\")\nprint(f\"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüìä Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos Res√≠duos (SQres): 1.9000\nSoma dos Quadrados Total (SQtot): 14.0000\nCoeficiente de Determina√ß√£o (R¬≤): 0.8643\nPorcentagem da varia√ß√£o explicada: 86.43%\n```\n:::\n:::\n\n\n**üìù Interpreta√ß√£o do $R^2$**:\n\n- Varia de 0 a 1\n- Quanto mais pr√≥ximo de 1, melhor o ajuste\n- Representa a propor√ß√£o da varia√ß√£o em $y$ explicada pelo modelo\n\n## üìä Visualizando o Resultado Final\n\nVamos plotar os dados originais junto com a reta ajustada:\n\n::: {#c0731c59 .cell execution_count=10}\n``` {.python .cell-code}\n# Criando pontos para desenhar a reta\ny_ajustados = B[0, 0] + B[1, 0] * np.array(x)  # Equa√ß√£o da reta: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx\n\nprint(\"y_ajustados:\")\nprint(y_ajustados)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny_ajustados:\n[-0.2  0.9  2.   3.1  4.2]\n```\n:::\n:::\n\n\n::: {#80cbf935 .cell execution_count=11}\n``` {.python .cell-code}\n# Criando o gr√°fico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nplt.scatter(x, y, color='blue', marker='o', s=100, alpha=0.7, \n           label=f'Dados observados (n={n})', zorder=3)\n\n# Valores ajustados\nY_pontos = B[0, 0] + B[1, 0] * np.array(x)\nplt.scatter(x, Y_pontos, color='red', marker='x', s=80, \n           label='y ajustado', zorder=3)\n\n# Reta ajustada\nplt.plot(x, y_ajustados, color='red', linewidth=2, \n         label=fr'Reta de regress√£o: $\\hat{{y}} = {B[0,0]:.3f} + {B[1,0]:.3f}x$')\n\n# Linhas dos res√≠duos\nfor i in range(len(x)):\n    plt.plot([x[i], x[i]], [y[i], Y_pontos[i]], 'gray', linestyle='--', alpha=0.5, label = 'Res√≠duos' if i == 0 else '')\n\n# Configura√ß√µes do gr√°fico\nplt.title(f'Regress√£o Linear Simples - MMQ\\nR¬≤ = {R2:.4f}', \n          fontsize=14, fontweight='bold')\nplt.xlabel('Vari√°vel X', fontsize=12)\nplt.ylabel('Vari√°vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_linear_simples_files/figure-html/cell-12-output-1.png){width=759 height=566}\n:::\n:::\n\n\n## üéØ Resumo dos Resultados\n\n::: {#c823d5cb .cell execution_count=12}\n``` {.python .cell-code}\nprint(\"=\"*50)\nprint(\"         RESUMO DA REGRESS√ÉO LINEAR\")\nprint(\"=\"*50)\nprint(f\"Equa√ß√£o ajustada: y = {B[0,0]:.4f} + {B[1,0]:.4f}x\")\nprint(f\"Coeficiente de determina√ß√£o (R¬≤): {R2:.4f}\")\nprint(f\"Porcentagem da varia√ß√£o explicada: {R2*100:.2f}%\")\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\n         RESUMO DA REGRESS√ÉO LINEAR\n==================================================\nEqua√ß√£o ajustada: y = -0.2000 + 1.1000x\nCoeficiente de determina√ß√£o (R¬≤): 0.8643\nPorcentagem da varia√ß√£o explicada: 86.43%\n==================================================\n```\n:::\n:::\n\n\n## üöÄ Exerc√≠cio Pr√°tico\n\nTeste o c√≥digo com novos dados:\n\n::: {#16668ed2 .cell execution_count=13}\n``` {.python .cell-code}\n# Experimente com estes dados:\nx_novo = [1, 2, 3, 4, 5, 6]\ny_novo = [2, 4, 5, 4, 5, 7]\n\n# Implemente todo o processo do MMQ com os novos dados\n# Dica: voc√™ pode copiar e adaptar o c√≥digo acima!\n```\n:::\n\n\n",
    "supporting": [
      "mmq_regressao_linear_simples_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}