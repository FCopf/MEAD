{
  "hash": "231f2271d47fba09428f49ca7652c357",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Método dos Mínimos Quadrados na Regressão Polinomial\"\nsubtitle: \"Implementação em Python usando Álgebra Matricial\"\ndescription: \"Tutorial prático para implementar o método dos mínimos quadrados em Python para modelos polinomiais, aplicando os conceitos de álgebra linear e estatística básica.\"\nCategories: [\n          \"Regressão polinomial\",\n          \"Método dos Mínimos Quadrados\",\n          \"Álgebra Matricial\",\n          \"Python\"\n        ]\n\nimage: \"images/mmq_regressao_polinomial.png\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n---\n\n\n## 📚 Introdução\n\n::: {.callout-tip title=\"Objetivos\"}\n\nNeste tutorial, vamos implementar o **Método dos Mínimos Quadrados (MMQ)** em Python para ajustar um modelo de **regressão polinomial** de segundo grau.\n\n**Objetivo**: Encontrar os coeficientes $\\beta_0$, $\\beta_1$ e $\\beta_2$ da equação $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ que melhor se ajustam aos nossos dados.\n\n:::\n\n## 🛠️ Importando as Bibliotecas\n\nVamos começar importando as bibliotecas necessárias:\n\n::: {#0706a4ad .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd           # Para manipulação de dados\nimport matplotlib.pyplot as plt  # Para criação e manipulação gráfica\nimport seaborn as sns        # Para criação e manipulação gráfica\nimport numpy as np           # Para operações matemáticas e matriciais\n```\n:::\n\n\n## 📊 Inserindo os Dados\n\nVamos trabalhar com dados que apresentam uma relação quadrática. Ao invés de digitarmos os dados diretamente $y$ e $x$ como listas, iremos ler os dados a partir de um arquivo que está disponível no link [regressao_polinomial_exemplo](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv). O arquivo esta no formato `.csv` em que cada coluna é separada por uma vírgula, um tipo de formatação muito comum.\n\n::: {#d5f574bf .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n```\n:::\n\n\nUtilizando a função `read_csv()` da bilbioteca [Pandas](https://pandas.pydata.org/), os dados foram importados no formato de **data frame**, basicamento uma estrutura de dados em linhas e colunas, em que as colunas são denominadas de **atributos**.\n\n::: {#5f92b8ff .cell execution_count=3}\n``` {.python .cell-code}\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x   y\n0  0   5\n1  1   2\n2  2  10\n3  3   8\n4  4  15\n5  5  35\n```\n:::\n:::\n\n\n## 📈 Visualizando os Dados\n\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n::: {#cabbee77 .cell execution_count=4}\n``` {.python .cell-code}\n# Criando the gráfico de dispersão\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = df, x = 'x', y = 'y', color = '#0072B2', s=120, label='Dados observados')\n\n# Configurando o gráfico\nplt.title('Gráfico de Dispersão dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-5-output-1.png){width=662 height=529}\n:::\n:::\n\n\n**📝 Observação 1**: Aparentemente, um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressão linear simples. Nosso objetivo será explorar esse modelo e, ao final, compará-lo com o modelo linear.\n\n**📝 Observação 2**: Como importamos os dados diretamente de um arquivo `.csv` para o objeto `df`, utilizamos a função `scatterplot` da biblioteca [Seaborn](https://seaborn.pydata.org/) para plotar o gráfico de dispersão entre as variáveis $y$ e $x$.\n\n## 🧮 Implementando o MMQ Polinomial - Passo a Passo\n\n### Criando os Vetores Base\n\nPara o modelo polinomial $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, precisamos dos vetores:\n\n$$\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#e2498e97 .cell execution_count=5}\n``` {.python .cell-code}\n# Número de observações\nn = len(df['x'])\n\n# Vetor f0: vetor de 1's (para o intercepto β₀)\nf0 = [1] * n\n\n# Vetor f1: valores de x (para o coeficiente linear β₁)\nf1 = df['x'].copy()\n\n# Vetor f2: valores de x² (para o coeficiente quadrático β₂)\nf2 = np.array(df['x'])**2  # Eleva cada elemento de x ao quadrado\n```\n:::\n\n\nVisualizando os vetores $\\vec{f}_0$, $\\vec{f}_1$ e $\\vec{f}_2$.\n\n::: {#154ba24a .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (termo linear):\", f1)\nprint(\"Vetor f2 (termo quadrático):\", f2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVetor f0 (intercepto): [1, 1, 1, 1, 1, 1]\nVetor f1 (termo linear): 0    0\n1    1\n2    2\n3    3\n4    4\n5    5\nName: x, dtype: int64\nVetor f2 (termo quadrático): [ 0  1  4  9 16 25]\n```\n:::\n:::\n\n\n### Construindo as Matrizes X e Y\n\nAgora vamos montar as matrizes do sistema polinomial:\n\n$$X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 & \\vec{f}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#dfb6c161 .cell execution_count=7}\n``` {.python .cell-code}\n# Matriz X: combinando f0, f1 e f2 em colunas\nX = np.column_stack((f0, f1, f2))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna\nY = np.array(df['y']).reshape(n, 1)\n```\n:::\n\n\n### Resolvendo o Sistema Normal\n\nCalculamos os coeficientes usando a mesma fórmula: \n\n$$B = (X^T X)^{-1} X^T Y$$\n\n::: {#d1591043 .cell execution_count=8}\n``` {.python .cell-code}\n# Calculando X transposta vezes X\nXTX = X.T @ X  # X.T é a transposta de X\n# Calculando X transposta vezes Y\nXTY = X.T @ Y\n# Calculando a matriz inversa (X^T X)^(-1)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\n# Coeficientes de regressão\nB = XTX_inv @ XTY\n```\n:::\n\n\n::: {.callout-note title=\"Interpretação\"}\n\n- $\\beta_0$ (intercepto): valor de y quando x = 0\n- $\\beta_1$ (coeficiente linear): relacionado à taxa de variação linear\n- $\\beta_2$ (coeficiente quadrático): relacionado à curvatura da parábola\n  - Se $\\beta_2 > 0$: parábola com concavidade para cima\n  - Se $\\beta_2 < 0$: parábola com concavidade para baixo\n\n:::\n\n### Obtendo os Valores Ajustados de y\n\nTendo obtido os coeficientes de regressão, os valores ajustados de y ($\\hat{y}$) podem ser obtido pela multiplicação matricial:\n\n$$F = XB = \\begin{bmatrix} 1 & x_1 & x^2_1 \\\\ 1 & x_2 & x^2_2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x^2_n \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}$$\n\n**Obs.**: denominamos $F$ a matriz de valores ajustados de $y$.\n\n::: {#f38a422e .cell execution_count=9}\n``` {.python .cell-code}\n# Valores ajustados (preditos)\nF = X @ B\n```\n:::\n\n\n### Avaliando a Qualidade do Ajuste\n\n#### Calculando a Soma dos Quadrados dos Resíduos ($SQ_{res}$)\n\n$SQ_{res}$ pode ser obtida pela multiplicação matricial:\n\n$$SQ_{res} = \\boldsymbol{e}^T \\boldsymbol{e}$$\n\nEm que $\\boldsymbol{e}$ é a matriz coluna dos **resíduos** obtida pela diferença entre os valores observados e ajustados de $y$:\n\n$$\\boldsymbol{e} = Y - F = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$$\n\n::: {#5bf1fc09 .cell execution_count=10}\n``` {.python .cell-code}\n# Resíduos: diferença entre valores observados e ajustados\ne = Y - F\n\n# Soma dos Quadrados dos Resíduos\nSQres = (e.T @ e)[0, 0]\n```\n:::\n\n\n#### Calculando a Soma dos Quadrados Totais ($SQ_{tot}$)\n\n$SQ_{tot}$ pode ser obtida pela multiplicação matricial:\n\n$$SQ_{tot} = \\boldsymbol{D}^T \\boldsymbol{D}$$\n\nEm que $\\boldsymbol{D}$ é a matriz coluna dos **desvios da médis** obtida pela diferença entre os valores observados de $y$ e a média de $\\overline{y}$:\n\n$$\\boldsymbol{D} = Y - \\overline{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\overline{y} \\\\ \\overline{y} \\\\ \\vdots \\\\ \\overline{y} \\end{bmatrix} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}$$\n\n::: {#0287a0e3 .cell execution_count=11}\n``` {.python .cell-code}\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n```\n:::\n\n\n#### Calculando o coeficiente de determinação $R^2$:\n\nO $R^2$ é dado pela expressão:\n\n$$R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}$$\n\n::: {#1bd51abd .cell execution_count=12}\n``` {.python .cell-code}\n# Coeficiente de Determinação R²\nR2 = 1 - (SQres / SQtot)\n```\n:::\n\n\n---\n\nVisualizando os resultados:\n\n::: {#ae4e1e67 .cell execution_count=13}\n``` {.python .cell-code}\nprint(\"📊 Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos Resíduos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de Determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos Resíduos (SQres): 59.2643\nSoma dos Quadrados Total (SQtot): 705.5000\nCoeficiente de Determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%\n```\n:::\n:::\n\n\n## 📊 Visualizando o Resultado Final\n\nVamos plotar os dados originais junto com a curva ajustada:\n\nCriando uma linha `contínua` para $\\hat{y}$\n\n::: {#b7966772 .cell execution_count=14}\n``` {.python .cell-code}\n# Criando pontos para desenhar a curva suave\nx_curva = np.linspace(min(df['x']), max(df['x']), 100)\ny_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2\n```\n:::\n\n\n::: {#f4d84c97 .cell execution_count=15}\n``` {.python .cell-code}\n# Criando o gráfico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nsns.scatterplot(data = df, x = 'x', y = 'y', \n                color = '#0072B2', s=120,\n                label=f'Dados observados (n={n})')\n\n# Valores ajustados\nplt.scatter(df['x'], F[:,0], \n           color='#000000', marker='*', s=120, \n           label='Valores ajustados')\n\n# Curva ajustada\nplt.plot(x_curva, y_curva, \n         color='#D55E00', \n         label=fr'Curva ajustada: $\\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')\n\n# Configurações do gráfico\nplt.title(f'Regressão Polinomial (2º grau) - MMQ\\nR² = {R2:.4f}', \n          fontsize=15, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-16-output-1.png){width=759 height=569}\n:::\n:::\n\n\n## 🎯 Resumo dos Resultados\n\n::: {#f040cf5a .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"=\"*60)\nprint(\"         RESUMO DA REGRESSÃO POLINOMIAL\")\nprint(\"=\"*60)\nprint(f\"Equação ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}x²\")\nprint(f\"Coeficiente de determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\nprint(\"=\"*60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\n         RESUMO DA REGRESSÃO POLINOMIAL\n============================================================\nEquação ajustada: y = 5.7500 -4.5679x + 1.9821x²\nCoeficiente de determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%\n============================================================\n```\n:::\n:::\n\n\n## 🔍 Comparação: Linear vs Polinomial\n\nVamos comparar o ajuste linear e polinomial para os mesmos dados:\n\n::: {#adae8e5e .cell execution_count=17}\n``` {.python .cell-code}\n# Ajuste LINEAR para comparação\nX_linear = np.column_stack((f0, f1))  # Apenas f0 e f1\nB_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)\n\n# R² do modelo linear\nF_linear = X_linear @ B_linear\nresiduos_linear = Y - F_linear\nSQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]\nR2_linear = 1 - (SQres_linear / SQtot)\n\nprint(\"📊 Comparação dos Modelos:\")\nprint(\"-\" * 40)\nprint(f\"Modelo Linear:     R² = {R2_linear:.4f}\")\nprint(f\"Modelo Polinomial: R² = {R2:.4f}\")\nprint(f\"Melhoria no R²:    {R2 - R2_linear:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 Comparação dos Modelos:\n----------------------------------------\nModelo Linear:     R² = 0.7081\nModelo Polinomial: R² = 0.9160\nMelhoria no R²:    0.2079\n```\n:::\n:::\n\n\nGráficos de dispersão\n\n::: {#28c40764 .cell execution_count=18}\n``` {.python .cell-code}\ny_linear = B_linear[0, 0] + B_linear[1, 0] * np.array(df['x'])\n\n# Gráfico comparativo\nplt.figure(figsize=(8, 6))\n\n# plt.subplot(1, 2, 1)\nsns.scatterplot(data = df, x = 'x', y = 'y', s=100, color = '#0072B2', label='Dados observados')\nplt.plot(df['x'], y_linear, color='#D55E00', label=f'Modelo Linear\\nR² = {R2_linear:.4f}')\nplt.plot(x_curva, y_curva, color='#009E73', label=f'Modelo Polinomial\\nR² = {R2:.4f}')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# plt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-19-output-1.png){width=659 height=503}\n:::\n:::\n\n\n## 🧾 Resumo do Código (modelo polinomial)\n\n1. Inserção dos Dados\n\n::: {#babfe78f .cell execution_count=19}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\n```\n:::\n\n\n2. Definição das matrizes do sistema\n\n::: {#0e4f63d4 .cell execution_count=20}\n``` {.python .cell-code}\nn = len(df['x'])\nf0 = [1] * n\nf1 = df['x'].copy()\n\nX = np.column_stack((f0, f1, f2))\nY = np.array(df['y']).reshape(n, 1)\n```\n:::\n\n\n3. Cálculo dos coeficientes\n\n::: {#da78e2f3 .cell execution_count=21}\n``` {.python .cell-code}\nXTX = X.T @ X\nXTY = X.T @ Y\nXTX_inv = np.linalg.inv(XTX)\nB = XTX_inv @ XTY\n```\n:::\n\n\n4. Qualidade do ajuste\n\n::: {#6332bd92 .cell execution_count=22}\n``` {.python .cell-code}\nY_ajustado = X @ B\ne = Y - Y_ajustado\nSQres = (e.T @ e)[0, 0]\n\nY_medio = np.mean(Y)\nD = Y - Y_medio\nSQtot = (D.T @ D)[0, 0]\n\nR2 = 1 - (SQres / SQtot)\n```\n:::\n\n\n## 🚀 Exercício Prático\n\nTeste o código com novos dados:\n\n::: {#d1a9c490 .cell execution_count=23}\n``` {.python .cell-code}\n# Experimente com estes dados (padrão quadrático diferente):\ndf_novo = pd.DataFrame({\n  'x_novo': [1, 2, 3, 4, 5, 6, 7],\n  'y_novo': [30, 12, 18, 9, 7, 8, 6]\n})\n\nprint(df_novo)\n\n# Questões para investigar:\n# 1. Qual é o R² do modelo polinomial para estes dados?\n# 2. O coeficiente β₂ é positivo ou negativo? O que isso significa?\n# 3. Compare com o modelo linear - qual é a diferença no R²?\n\n# Implemente todo o processo do MMQ polinomial com os novos dados\n# Dica: você pode copiar e adaptar o código acima!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x_novo  y_novo\n0       1      30\n1       2      12\n2       3      18\n3       4       9\n4       5       7\n5       6       8\n6       7       6\n```\n:::\n:::\n\n\n## 💡 Conceitos Importantes Revisados\n\n1. **Regressão Polinomial**: Extensão da regressão linear para relações curvas\n2. **Matriz de Design**: Agora com três colunas: $[1, x, x^2]$\n3. **Interpretação dos Coeficientes**: Cada coeficiente tem significado específico\n4. **Comparação de Modelos**: Uso do $R^2$ para avaliar qual modelo é melhor\n\n## 🔗 Próximos Passos\n\n- Experimente com polinômios de grau maior ($x^3$, $x^4$, etc.)\n- Investigue o conceito de **overfitting** com graus muito altos\n\n",
    "supporting": [
      "mmq_regressao_polinomial_files"
    ],
    "filters": [],
    "includes": {}
  }
}