{
  "hash": "50a37e7bed860f1a86db05480033bc66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Método dos Mínimos Quadrados na Regressão Polinomial\"\nsubtitle: \"Implementação em Python usando Álgebra Matricial\"\ndescription: \"Tutorial prático para implementar o método dos mínimos quadrados em Python para modelos polinomiais, aplicando os conceitos de álgebra linear e estatística básica.\"\nCategories: [\n          \"Regressão polinomial\",\n          \"Método dos Mínimos Quadrados\",\n          \"Álgebra Matricial\",\n          \"Python\"\n        ]\n\nimage: \"images/mmq_regressao_polinomial.png\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n---\n\n\n## 📚 Introdução\n\n::: {.callout-tip title=\"Objetivos\"}\n\nNeste tutorial, vamos implementar o **Método dos Mínimos Quadrados (MMQ)** em Python para ajustar um modelo de **regressão polinomial** de segundo grau.\n\n**Objetivo**: Encontrar os coeficientes $\\beta_0$, $\\beta_1$ e $\\beta_2$ da equação $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ que melhor se ajustam aos nossos dados.\n\n:::\n\n## 🛠️ Importando as Bibliotecas\n\nVamos começar importando as bibliotecas necessárias:\n\n::: {#af30b33d .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd           # Para manipulação de dados\nimport matplotlib.pyplot as plt  # Para criar gráficos\nimport seaborn as sns        # Para gráficos mais bonitos\nimport numpy as np           # Para operações matemáticas e matriciais\n```\n:::\n\n\n**💡 Dica**: Estas são as mesmas bibliotecas do tutorial anterior!\n\n## 📊 Definindo os Dados\n\nVamos trabalhar com dados que apresentam uma relação quadrática. Ao invés de digitarmos os dados diretamente como listas, iremos ler os dados a partir de um arquivo. O arquivo está disponível no link [regressao_polinomial_exemplo](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv).\n\n::: {#e220f0c0 .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x   y\n0  0   5\n1  1   2\n2  2  10\n3  3   8\n4  4  15\n5  5  35\n```\n:::\n:::\n\n\n## 📈 Visualizando os Dados\n\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n::: {#8dcec6a2 .cell execution_count=3}\n``` {.python .cell-code}\n# Criando the gráfico de dispersão\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\n\n# Configurando o gráfico\nplt.title('Gráfico de Dispersão dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-4-output-1.png){width=662 height=529}\n:::\n:::\n\n\n**📝 Observação 1**: Um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressão linear simples. Nosso objetivo será explorar esse modelo e, ao final, compará-lo com o modelo linear.\n\n**📝 Observação 2**: Como importamos os dados diretamente de um arquivo `.csv` para o objeto `df`, utilizamos a função `scatterplot` da biblioteca [Seaborn](https://seaborn.pydata.org/) para plotar o gráfico de dispersão entre as variáveis $y$ e $x$.\n\n## 🧮 Implementando o MMQ Polinomial - Passo a Passo\n\n### Criando os Vetores Base\n\nPara o modelo polinomial $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, precisamos dos vetores:\n\n$$\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#2f221b2d .cell execution_count=4}\n``` {.python .cell-code}\n# Passando as variáveis do data frame para listas\nx = df['x'].to_list()\ny = df['y'].to_list()\n```\n:::\n\n\n::: {#e522655b .cell execution_count=5}\n``` {.python .cell-code}\n# Número de observações\nn = len(x)\n\n# Vetor f0: vetor de 1's (para o intercepto β₀)\nf0 = [1] * n\n\n# Vetor f1: valores de x (para o coeficiente linear β₁)\nf1 = x.copy()\n\n# Vetor f2: valores de x² (para o coeficiente quadrático β₂)\nf2 = np.array(x)**2  # Eleva cada elemento de x ao quadrado\n\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (termo linear):\", f1)\nprint(\"Vetor f2 (termo quadrático):\", f2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVetor f0 (intercepto): [1, 1, 1, 1, 1, 1]\nVetor f1 (termo linear): [0, 1, 2, 3, 4, 5]\nVetor f2 (termo quadrático): [ 0  1  4  9 16 25]\n```\n:::\n:::\n\n\n### Construindo as Matrizes X e Y\n\nAgora vamos montar as matrizes do sistema polinomial:\n\n$$X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 & \\vec{f}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#3b1ee968 .cell execution_count=6}\n``` {.python .cell-code}\n# Matriz X: combinando f0, f1 e f2 em colunas\nX = np.column_stack((f0, f1, f2))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna\nY = np.array(y).reshape(n, 1)\n\nprint(\"Matriz X:\")\nprint(X)\nprint(\"\\nMatriz Y:\")\nprint(Y)\nprint(f\"\\nDimensões - X: {X.shape}, Y: {Y.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatriz X:\n[[ 1  0  0]\n [ 1  1  1]\n [ 1  2  4]\n [ 1  3  9]\n [ 1  4 16]\n [ 1  5 25]]\n\nMatriz Y:\n[[ 5]\n [ 2]\n [10]\n [ 8]\n [15]\n [35]]\n\nDimensões - X: (6, 3), Y: (6, 1)\n```\n:::\n:::\n\n\n### Resolvendo o Sistema Normal\n\nCalculamos os coeficientes usando a mesma fórmula: \n\n$$\\boldsymbol{\\hat{\\beta}} = (X^T X)^{-1} X^T Y$$\n\n::: {#f693a197 .cell execution_count=7}\n``` {.python .cell-code}\n# Calculando X transposta vezes X\nXTX = X.T @ X  # Usando o operador @ para multiplicação matricial\nprint(\"X^T X:\")\nprint(XTX)\n\n# Calculando X transposta vezes Y\nXTY = X.T @ Y\nprint(\"\\nX^T Y:\")\nprint(XTY)\n\n# Calculando os coeficientes: B = (X^T X)^(-1) (X^T Y)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\nB = XTX_inv @ XTY\n\nprint(\"\\n🎯 Coeficientes estimados:\")\nprint(f\"β₀ (intercepto) = {B[0, 0]:.4f}\")\nprint(f\"β₁ (coeficiente linear) = {B[1, 0]:.4f}\")\nprint(f\"β₂ (coeficiente quadrático) = {B[2, 0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX^T X:\n[[  6  15  55]\n [ 15  55 225]\n [ 55 225 979]]\n\nX^T Y:\n[[  75]\n [ 281]\n [1229]]\n\n🎯 Coeficientes estimados:\nβ₀ (intercepto) = 5.7500\nβ₁ (coeficiente linear) = -4.5679\nβ₂ (coeficiente quadrático) = 1.9821\n```\n:::\n:::\n\n\n**📚 Interpretação**:\n\n- $\\beta_0$: valor de y quando x = 0\n- $\\beta_1$: relacionado à taxa de variação linear\n- $\\beta_2$: relacionado à curvatura da parábola\n\n::: {.callout-note title=\"Observação importante\"}\n\nPara um modelo polinomial de segundo grau:\n\n- Se $\\beta_2 > 0$: parábola com concavidade para cima\n- Se $\\beta_2 < 0$: parábola com concavidade para baixo\n\n:::\n\n### Avaliando a Qualidade do Ajuste\n\nVamos calcular o coeficiente de determinação $R^2$:\n\n::: {#48713328 .cell execution_count=8}\n``` {.python .cell-code}\n# Valores ajustados (preditos)\nY_ajustado = X @ B\n\n# Resíduos: diferença entre valores observados e ajustados\nresiduos = Y - Y_ajustado\n\n# Soma dos Quadrados dos Resíduos\nSQres = (residuos.T @ residuos)[0, 0]\n\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\ndesvios_media = Y - Y_medio\nSQtot = (desvios_media.T @ desvios_media)[0, 0]\n\n# Coeficiente de Determinação R²\nR2 = 1 - (SQres / SQtot)\n\nprint(\"📊 Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos Resíduos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de Determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos Resíduos (SQres): 59.2643\nSoma dos Quadrados Total (SQtot): 705.5000\nCoeficiente de Determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%\n```\n:::\n:::\n\n\n## 📊 Visualizando o Resultado Final\n\nVamos plotar os dados originais junto com a curva ajustada:\n\n::: {#c09c13da .cell execution_count=9}\n``` {.python .cell-code}\n# Criando pontos para desenhar a curva suave\nx_curva = np.linspace(min(x) - 0.5, max(x) + 0.5, 100)\ny_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2\n\n# Valores ajustados nos pontos originais\ny_ajustados = B[0, 0] + B[1, 0] * np.array(x) + B[2, 0] * np.array(x)**2\n\nprint(\"Valores y ajustados:\")\nprint(y_ajustados)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores y ajustados:\n[ 5.75        3.16428571  4.54285714  9.88571429 19.19285714 32.46428571]\n```\n:::\n:::\n\n\n::: {#cb4a86f2 .cell execution_count=10}\n``` {.python .cell-code}\n# Criando o gráfico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nsns.scatterplot(data = df, x = 'x', y = 'y', \n                color='blue', marker='o', s=120, alpha=0.8,\n                label=f'Dados observados (n={n})', zorder=3)\n\n# Valores ajustados\nplt.scatter(x, y_ajustados, color='red', marker='x', s=100, \n           label='Valores ajustados', zorder=3)\n\n# Curva ajustada\nplt.plot(x_curva, y_curva, color='red', linewidth=2.5, \n         label=fr'Curva ajustada: $\\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')\n\n# Linhas dos resíduos\nfor i in range(len(x)):\n    plt.plot([x[i], x[i]], [y[i], y_ajustados[i]], 'gray', \n             linestyle='--', alpha=0.6, label='Resíduos' if i == 0 else '')\n\n# Configurações do gráfico\nplt.title(f'Regressão Polinomial (2º grau) - MMQ\\nR² = {R2:.4f}', \n          fontsize=15, fontweight='bold')\nplt.xlabel('Variável X', fontsize=12)\nplt.ylabel('Variável Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-11-output-1.png){width=759 height=569}\n:::\n:::\n\n\n## 🎯 Resumo dos Resultados\n\n::: {#56568930 .cell execution_count=11}\n``` {.python .cell-code}\nprint(\"=\"*60)\nprint(\"         RESUMO DA REGRESSÃO POLINOMIAL\")\nprint(\"=\"*60)\nprint(f\"Equação ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}x²\")\nprint(f\"Coeficiente de determinação (R²): {R2:.4f}\")\nprint(f\"Porcentagem da variação explicada: {R2*100:.2f}%\")\nprint(\"=\"*60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\n         RESUMO DA REGRESSÃO POLINOMIAL\n============================================================\nEquação ajustada: y = 5.7500 -4.5679x + 1.9821x²\nCoeficiente de determinação (R²): 0.9160\nPorcentagem da variação explicada: 91.60%\n============================================================\n```\n:::\n:::\n\n\n## 🔍 Comparação: Linear vs Polinomial\n\nVamos comparar o ajuste linear e polinomial para os mesmos dados:\n\n::: {#b078d632 .cell execution_count=12}\n``` {.python .cell-code}\n# Ajuste LINEAR para comparação\nX_linear = np.column_stack((f0, f1))  # Apenas f0 e f1\nB_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)\n\n# R² do modelo linear\nY_ajustado_linear = X_linear @ B_linear\nresiduos_linear = Y - Y_ajustado_linear\nSQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]\nR2_linear = 1 - (SQres_linear / SQtot)\n\nprint(\"📊 Comparação dos Modelos:\")\nprint(\"-\" * 40)\nprint(f\"Modelo Linear:     R² = {R2_linear:.4f}\")\nprint(f\"Modelo Polinomial: R² = {R2:.4f}\")\nprint(f\"Melhoria no R²:    {R2 - R2_linear:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 Comparação dos Modelos:\n----------------------------------------\nModelo Linear:     R² = 0.7081\nModelo Polinomial: R² = 0.9160\nMelhoria no R²:    0.2079\n```\n:::\n:::\n\n\n::: {#82eff60c .cell execution_count=13}\n``` {.python .cell-code}\ny_linear = B_linear[0, 0] + B_linear[1, 0] * np.array(x)\n\n# Gráfico comparativo\nplt.figure(figsize=(8, 4))\n\n# Subplot 1: Modelo Linear\nplt.subplot(1, 2, 1)\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\nplt.plot(x, y_linear, color='red', linewidth=2, label='Ajuste linear')\nplt.title(f'Modelo Linear\\nR² = {R2_linear:.4f}', fontweight='bold')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Subplot 2: Modelo Polinomial\nplt.subplot(1, 2, 2)\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\nplt.plot(x_curva, y_curva, color='red', linewidth=2, label='Ajuste polinomial')\nplt.title(f'Modelo Polinomial\\nR² = {R2:.4f}', fontweight='bold')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-14-output-1.png){width=758 height=375}\n:::\n:::\n\n\n## 🚀 Exercício Prático\n\n**Agora é sua vez!** Teste o código com novos dados:\n\n::: {#61885884 .cell execution_count=14}\n``` {.python .cell-code}\n# Experimente com estes dados (padrão quadrático diferente):\ndf_novo = pd.DataFrame({\n  'x_novo': [1, 2, 3, 4, 5, 6, 7],\n  'y_novo': [30, 12, 18, 9, 7, 8, 6]\n})\n\nprint(df_novo)\n\n# Questões para investigar:\n# 1. Qual é o R² do modelo polinomial para estes dados?\n# 2. O coeficiente β₂ é positivo ou negativo? O que isso significa?\n# 3. Compare com o modelo linear - qual é a diferença no R²?\n\n# Implemente todo o processo do MMQ polinomial com os novos dados\n# Dica: você pode copiar e adaptar o código acima!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x_novo  y_novo\n0       1      30\n1       2      12\n2       3      18\n3       4       9\n4       5       7\n5       6       8\n6       7       6\n```\n:::\n:::\n\n\n## 💡 Conceitos Importantes Revisados\n\n1. **Regressão Polinomial**: Extensão da regressão linear para relações curvas\n2. **Matriz de Design**: Agora com três colunas: $[1, x, x^2]$\n3. **Interpretação dos Coeficientes**: Cada coeficiente tem significado específico\n4. **Comparação de Modelos**: Uso do $R^2$ para avaliar qual modelo é melhor\n\n## 🔗 Próximos Passos\n\n- Experimente com polinômios de grau maior ($x^3$, $x^4$, etc.)\n- Investigue o conceito de **overfitting** com graus muito altos\n- Compare com outras técnicas de ajuste de curvas\n\n",
    "supporting": [
      "mmq_regressao_polinomial_files"
    ],
    "filters": [],
    "includes": {}
  }
}