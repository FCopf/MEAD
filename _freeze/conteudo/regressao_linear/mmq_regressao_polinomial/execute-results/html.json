{
  "hash": "50a37e7bed860f1a86db05480033bc66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"MÃ©todo dos MÃ­nimos Quadrados na RegressÃ£o Polinomial\"\nsubtitle: \"ImplementaÃ§Ã£o em Python usando Ãlgebra Matricial\"\ndescription: \"Tutorial prÃ¡tico para implementar o mÃ©todo dos mÃ­nimos quadrados em Python para modelos polinomiais, aplicando os conceitos de Ã¡lgebra linear e estatÃ­stica bÃ¡sica.\"\nCategories: [\n          \"RegressÃ£o polinomial\",\n          \"MÃ©todo dos MÃ­nimos Quadrados\",\n          \"Ãlgebra Matricial\",\n          \"Python\"\n        ]\n\nimage: \"images/mmq_regressao_polinomial.png\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n---\n\n\n## ğŸ“š IntroduÃ§Ã£o\n\n::: {.callout-tip title=\"Objetivos\"}\n\nNeste tutorial, vamos implementar o **MÃ©todo dos MÃ­nimos Quadrados (MMQ)** em Python para ajustar um modelo de **regressÃ£o polinomial** de segundo grau.\n\n**Objetivo**: Encontrar os coeficientes $\\beta_0$, $\\beta_1$ e $\\beta_2$ da equaÃ§Ã£o $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ que melhor se ajustam aos nossos dados.\n\n:::\n\n## ğŸ› ï¸ Importando as Bibliotecas\n\nVamos comeÃ§ar importando as bibliotecas necessÃ¡rias:\n\n::: {#af30b33d .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd           # Para manipulaÃ§Ã£o de dados\nimport matplotlib.pyplot as plt  # Para criar grÃ¡ficos\nimport seaborn as sns        # Para grÃ¡ficos mais bonitos\nimport numpy as np           # Para operaÃ§Ãµes matemÃ¡ticas e matriciais\n```\n:::\n\n\n**ğŸ’¡ Dica**: Estas sÃ£o as mesmas bibliotecas do tutorial anterior!\n\n## ğŸ“Š Definindo os Dados\n\nVamos trabalhar com dados que apresentam uma relaÃ§Ã£o quadrÃ¡tica. Ao invÃ©s de digitarmos os dados diretamente como listas, iremos ler os dados a partir de um arquivo. O arquivo estÃ¡ disponÃ­vel no link [regressao_polinomial_exemplo](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv).\n\n::: {#e220f0c0 .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/regressao_polinomial_exemplo.csv')\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x   y\n0  0   5\n1  1   2\n2  2  10\n3  3   8\n4  4  15\n5  5  35\n```\n:::\n:::\n\n\n## ğŸ“ˆ Visualizando os Dados\n\nAntes de ajustar o modelo, vamos visualizar nossos dados:\n\n::: {#8dcec6a2 .cell execution_count=3}\n``` {.python .cell-code}\n# Criando the grÃ¡fico de dispersÃ£o\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\n\n# Configurando o grÃ¡fico\nplt.title('GrÃ¡fico de DispersÃ£o dos Dados', fontsize=14, fontweight='bold')\nplt.xlabel('VariÃ¡vel X', fontsize=12)\nplt.ylabel('VariÃ¡vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-4-output-1.png){width=662 height=529}\n:::\n:::\n\n\n**ğŸ“ ObservaÃ§Ã£o 1**: Um modelo polinomial de segundo grau pode oferecer um ajuste melhor a estes dados do que a regressÃ£o linear simples. Nosso objetivo serÃ¡ explorar esse modelo e, ao final, comparÃ¡-lo com o modelo linear.\n\n**ğŸ“ ObservaÃ§Ã£o 2**: Como importamos os dados diretamente de um arquivo `.csv` para o objeto `df`, utilizamos a funÃ§Ã£o `scatterplot` da biblioteca [Seaborn](https://seaborn.pydata.org/) para plotar o grÃ¡fico de dispersÃ£o entre as variÃ¡veis $y$ e $x$.\n\n## ğŸ§® Implementando o MMQ Polinomial - Passo a Passo\n\n### Criando os Vetores Base\n\nPara o modelo polinomial $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$, precisamos dos vetores:\n\n$$\\vec{f}_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_1 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad \\text{,} \\quad \\vec{f}_2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#2f221b2d .cell execution_count=4}\n``` {.python .cell-code}\n# Passando as variÃ¡veis do data frame para listas\nx = df['x'].to_list()\ny = df['y'].to_list()\n```\n:::\n\n\n::: {#e522655b .cell execution_count=5}\n``` {.python .cell-code}\n# NÃºmero de observaÃ§Ãµes\nn = len(x)\n\n# Vetor f0: vetor de 1's (para o intercepto Î²â‚€)\nf0 = [1] * n\n\n# Vetor f1: valores de x (para o coeficiente linear Î²â‚)\nf1 = x.copy()\n\n# Vetor f2: valores de xÂ² (para o coeficiente quadrÃ¡tico Î²â‚‚)\nf2 = np.array(x)**2  # Eleva cada elemento de x ao quadrado\n\nprint(\"Vetor f0 (intercepto):\", f0)\nprint(\"Vetor f1 (termo linear):\", f1)\nprint(\"Vetor f2 (termo quadrÃ¡tico):\", f2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVetor f0 (intercepto): [1, 1, 1, 1, 1, 1]\nVetor f1 (termo linear): [0, 1, 2, 3, 4, 5]\nVetor f2 (termo quadrÃ¡tico): [ 0  1  4  9 16 25]\n```\n:::\n:::\n\n\n### Construindo as Matrizes X e Y\n\nAgora vamos montar as matrizes do sistema polinomial:\n\n$$X = \\begin{bmatrix} \\vec{f}_0 & \\vec{f}_1 & \\vec{f}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\quad \\text{e} \\quad Y = \\begin{bmatrix} \\vec{y} \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n\n::: {#3b1ee968 .cell execution_count=6}\n``` {.python .cell-code}\n# Matriz X: combinando f0, f1 e f2 em colunas\nX = np.column_stack((f0, f1, f2))\n\n# Matriz Y: transformando y em matriz com n linhas e 1 coluna\nY = np.array(y).reshape(n, 1)\n\nprint(\"Matriz X:\")\nprint(X)\nprint(\"\\nMatriz Y:\")\nprint(Y)\nprint(f\"\\nDimensÃµes - X: {X.shape}, Y: {Y.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatriz X:\n[[ 1  0  0]\n [ 1  1  1]\n [ 1  2  4]\n [ 1  3  9]\n [ 1  4 16]\n [ 1  5 25]]\n\nMatriz Y:\n[[ 5]\n [ 2]\n [10]\n [ 8]\n [15]\n [35]]\n\nDimensÃµes - X: (6, 3), Y: (6, 1)\n```\n:::\n:::\n\n\n### Resolvendo o Sistema Normal\n\nCalculamos os coeficientes usando a mesma fÃ³rmula: \n\n$$\\boldsymbol{\\hat{\\beta}} = (X^T X)^{-1} X^T Y$$\n\n::: {#f693a197 .cell execution_count=7}\n``` {.python .cell-code}\n# Calculando X transposta vezes X\nXTX = X.T @ X  # Usando o operador @ para multiplicaÃ§Ã£o matricial\nprint(\"X^T X:\")\nprint(XTX)\n\n# Calculando X transposta vezes Y\nXTY = X.T @ Y\nprint(\"\\nX^T Y:\")\nprint(XTY)\n\n# Calculando os coeficientes: B = (X^T X)^(-1) (X^T Y)\nXTX_inv = np.linalg.inv(XTX)  # Inversa de X^T X\nB = XTX_inv @ XTY\n\nprint(\"\\nğŸ¯ Coeficientes estimados:\")\nprint(f\"Î²â‚€ (intercepto) = {B[0, 0]:.4f}\")\nprint(f\"Î²â‚ (coeficiente linear) = {B[1, 0]:.4f}\")\nprint(f\"Î²â‚‚ (coeficiente quadrÃ¡tico) = {B[2, 0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX^T X:\n[[  6  15  55]\n [ 15  55 225]\n [ 55 225 979]]\n\nX^T Y:\n[[  75]\n [ 281]\n [1229]]\n\nğŸ¯ Coeficientes estimados:\nÎ²â‚€ (intercepto) = 5.7500\nÎ²â‚ (coeficiente linear) = -4.5679\nÎ²â‚‚ (coeficiente quadrÃ¡tico) = 1.9821\n```\n:::\n:::\n\n\n**ğŸ“š InterpretaÃ§Ã£o**:\n\n- $\\beta_0$: valor de y quando x = 0\n- $\\beta_1$: relacionado Ã  taxa de variaÃ§Ã£o linear\n- $\\beta_2$: relacionado Ã  curvatura da parÃ¡bola\n\n::: {.callout-note title=\"ObservaÃ§Ã£o importante\"}\n\nPara um modelo polinomial de segundo grau:\n\n- Se $\\beta_2 > 0$: parÃ¡bola com concavidade para cima\n- Se $\\beta_2 < 0$: parÃ¡bola com concavidade para baixo\n\n:::\n\n### Avaliando a Qualidade do Ajuste\n\nVamos calcular o coeficiente de determinaÃ§Ã£o $R^2$:\n\n::: {#48713328 .cell execution_count=8}\n``` {.python .cell-code}\n# Valores ajustados (preditos)\nY_ajustado = X @ B\n\n# ResÃ­duos: diferenÃ§a entre valores observados e ajustados\nresiduos = Y - Y_ajustado\n\n# Soma dos Quadrados dos ResÃ­duos\nSQres = (residuos.T @ residuos)[0, 0]\n\n# Soma dos Quadrados Total\nY_medio = np.mean(Y)\ndesvios_media = Y - Y_medio\nSQtot = (desvios_media.T @ desvios_media)[0, 0]\n\n# Coeficiente de DeterminaÃ§Ã£o RÂ²\nR2 = 1 - (SQres / SQtot)\n\nprint(\"ğŸ“Š Medidas de Qualidade do Ajuste:\")\nprint(f\"Soma dos Quadrados dos ResÃ­duos (SQres): {SQres:.4f}\")\nprint(f\"Soma dos Quadrados Total (SQtot): {SQtot:.4f}\")\nprint(f\"Coeficiente de DeterminaÃ§Ã£o (RÂ²): {R2:.4f}\")\nprint(f\"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š Medidas de Qualidade do Ajuste:\nSoma dos Quadrados dos ResÃ­duos (SQres): 59.2643\nSoma dos Quadrados Total (SQtot): 705.5000\nCoeficiente de DeterminaÃ§Ã£o (RÂ²): 0.9160\nPorcentagem da variaÃ§Ã£o explicada: 91.60%\n```\n:::\n:::\n\n\n## ğŸ“Š Visualizando o Resultado Final\n\nVamos plotar os dados originais junto com a curva ajustada:\n\n::: {#c09c13da .cell execution_count=9}\n``` {.python .cell-code}\n# Criando pontos para desenhar a curva suave\nx_curva = np.linspace(min(x) - 0.5, max(x) + 0.5, 100)\ny_curva = B[0, 0] + B[1, 0] * x_curva + B[2, 0] * x_curva**2\n\n# Valores ajustados nos pontos originais\ny_ajustados = B[0, 0] + B[1, 0] * np.array(x) + B[2, 0] * np.array(x)**2\n\nprint(\"Valores y ajustados:\")\nprint(y_ajustados)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores y ajustados:\n[ 5.75        3.16428571  4.54285714  9.88571429 19.19285714 32.46428571]\n```\n:::\n:::\n\n\n::: {#cb4a86f2 .cell execution_count=10}\n``` {.python .cell-code}\n# Criando o grÃ¡fico final\nplt.figure(figsize=(8, 6))\n\n# Pontos observados\nsns.scatterplot(data = df, x = 'x', y = 'y', \n                color='blue', marker='o', s=120, alpha=0.8,\n                label=f'Dados observados (n={n})', zorder=3)\n\n# Valores ajustados\nplt.scatter(x, y_ajustados, color='red', marker='x', s=100, \n           label='Valores ajustados', zorder=3)\n\n# Curva ajustada\nplt.plot(x_curva, y_curva, color='red', linewidth=2.5, \n         label=fr'Curva ajustada: $\\hat{{y}} = {B[0,0]:.3f} {B[1,0]:.3f}x + {B[2,0]:.3f}x^2$')\n\n# Linhas dos resÃ­duos\nfor i in range(len(x)):\n    plt.plot([x[i], x[i]], [y[i], y_ajustados[i]], 'gray', \n             linestyle='--', alpha=0.6, label='ResÃ­duos' if i == 0 else '')\n\n# ConfiguraÃ§Ãµes do grÃ¡fico\nplt.title(f'RegressÃ£o Polinomial (2Âº grau) - MMQ\\nRÂ² = {R2:.4f}', \n          fontsize=15, fontweight='bold')\nplt.xlabel('VariÃ¡vel X', fontsize=12)\nplt.ylabel('VariÃ¡vel Y', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-11-output-1.png){width=759 height=569}\n:::\n:::\n\n\n## ğŸ¯ Resumo dos Resultados\n\n::: {#56568930 .cell execution_count=11}\n``` {.python .cell-code}\nprint(\"=\"*60)\nprint(\"         RESUMO DA REGRESSÃƒO POLINOMIAL\")\nprint(\"=\"*60)\nprint(f\"EquaÃ§Ã£o ajustada: y = {B[0,0]:.4f} {B[1,0]:.4f}x + {B[2,0]:.4f}xÂ²\")\nprint(f\"Coeficiente de determinaÃ§Ã£o (RÂ²): {R2:.4f}\")\nprint(f\"Porcentagem da variaÃ§Ã£o explicada: {R2*100:.2f}%\")\nprint(\"=\"*60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\n         RESUMO DA REGRESSÃƒO POLINOMIAL\n============================================================\nEquaÃ§Ã£o ajustada: y = 5.7500 -4.5679x + 1.9821xÂ²\nCoeficiente de determinaÃ§Ã£o (RÂ²): 0.9160\nPorcentagem da variaÃ§Ã£o explicada: 91.60%\n============================================================\n```\n:::\n:::\n\n\n## ğŸ” ComparaÃ§Ã£o: Linear vs Polinomial\n\nVamos comparar o ajuste linear e polinomial para os mesmos dados:\n\n::: {#b078d632 .cell execution_count=12}\n``` {.python .cell-code}\n# Ajuste LINEAR para comparaÃ§Ã£o\nX_linear = np.column_stack((f0, f1))  # Apenas f0 e f1\nB_linear = np.linalg.inv(X_linear.T @ X_linear) @ (X_linear.T @ Y)\n\n# RÂ² do modelo linear\nY_ajustado_linear = X_linear @ B_linear\nresiduos_linear = Y - Y_ajustado_linear\nSQres_linear = (residuos_linear.T @ residuos_linear)[0, 0]\nR2_linear = 1 - (SQres_linear / SQtot)\n\nprint(\"ğŸ“Š ComparaÃ§Ã£o dos Modelos:\")\nprint(\"-\" * 40)\nprint(f\"Modelo Linear:     RÂ² = {R2_linear:.4f}\")\nprint(f\"Modelo Polinomial: RÂ² = {R2:.4f}\")\nprint(f\"Melhoria no RÂ²:    {R2 - R2_linear:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š ComparaÃ§Ã£o dos Modelos:\n----------------------------------------\nModelo Linear:     RÂ² = 0.7081\nModelo Polinomial: RÂ² = 0.9160\nMelhoria no RÂ²:    0.2079\n```\n:::\n:::\n\n\n::: {#82eff60c .cell execution_count=13}\n``` {.python .cell-code}\ny_linear = B_linear[0, 0] + B_linear[1, 0] * np.array(x)\n\n# GrÃ¡fico comparativo\nplt.figure(figsize=(8, 4))\n\n# Subplot 1: Modelo Linear\nplt.subplot(1, 2, 1)\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\nplt.plot(x, y_linear, color='red', linewidth=2, label='Ajuste linear')\nplt.title(f'Modelo Linear\\nRÂ² = {R2_linear:.4f}', fontweight='bold')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Subplot 2: Modelo Polinomial\nplt.subplot(1, 2, 2)\nsns.scatterplot(data = df, x = 'x', y = 'y', color='blue', marker='o', s=100, alpha=0.7, label='Dados observados')\nplt.plot(x_curva, y_curva, color='red', linewidth=2, label='Ajuste polinomial')\nplt.title(f'Modelo Polinomial\\nRÂ² = {R2:.4f}', fontweight='bold')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mmq_regressao_polinomial_files/figure-html/cell-14-output-1.png){width=758 height=375}\n:::\n:::\n\n\n## ğŸš€ ExercÃ­cio PrÃ¡tico\n\n**Agora Ã© sua vez!** Teste o cÃ³digo com novos dados:\n\n::: {#61885884 .cell execution_count=14}\n``` {.python .cell-code}\n# Experimente com estes dados (padrÃ£o quadrÃ¡tico diferente):\ndf_novo = pd.DataFrame({\n  'x_novo': [1, 2, 3, 4, 5, 6, 7],\n  'y_novo': [30, 12, 18, 9, 7, 8, 6]\n})\n\nprint(df_novo)\n\n# QuestÃµes para investigar:\n# 1. Qual Ã© o RÂ² do modelo polinomial para estes dados?\n# 2. O coeficiente Î²â‚‚ Ã© positivo ou negativo? O que isso significa?\n# 3. Compare com o modelo linear - qual Ã© a diferenÃ§a no RÂ²?\n\n# Implemente todo o processo do MMQ polinomial com os novos dados\n# Dica: vocÃª pode copiar e adaptar o cÃ³digo acima!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x_novo  y_novo\n0       1      30\n1       2      12\n2       3      18\n3       4       9\n4       5       7\n5       6       8\n6       7       6\n```\n:::\n:::\n\n\n## ğŸ’¡ Conceitos Importantes Revisados\n\n1. **RegressÃ£o Polinomial**: ExtensÃ£o da regressÃ£o linear para relaÃ§Ãµes curvas\n2. **Matriz de Design**: Agora com trÃªs colunas: $[1, x, x^2]$\n3. **InterpretaÃ§Ã£o dos Coeficientes**: Cada coeficiente tem significado especÃ­fico\n4. **ComparaÃ§Ã£o de Modelos**: Uso do $R^2$ para avaliar qual modelo Ã© melhor\n\n## ğŸ”— PrÃ³ximos Passos\n\n- Experimente com polinÃ´mios de grau maior ($x^3$, $x^4$, etc.)\n- Investigue o conceito de **overfitting** com graus muito altos\n- Compare com outras tÃ©cnicas de ajuste de curvas\n\n",
    "supporting": [
      "mmq_regressao_polinomial_files"
    ],
    "filters": [],
    "includes": {}
  }
}