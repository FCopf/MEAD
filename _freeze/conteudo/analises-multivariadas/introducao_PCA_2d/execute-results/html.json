{
  "hash": "bc05504c1d8a8f999b12c8188bffecde",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introdução à Análise de Componentes Principais (PCA)\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n  eval: true\nformat: \n  html:\n    css: styles.css\n---\n\n::: {#ddf13066 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\n# Importação de bibliotecas essenciais\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom scipy.stats import pearsonr\nfrom numpy.linalg import norm\nfrom math import acos, degrees\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n```\n:::\n\n\nA Análise de Componentes Principais (PCA) é uma técnica multivariada utilizada para organizar e representar objetos (como locais de amostragem, estações, indivíduos, etc.) em um espaço de dimensões reduzidas. Seu objetivo é simplificar conjuntos de dados complexos, com muitas variáveis correlacionadas, transformando-os em um número menor de variáveis não correlacionadas, chamadas **componentes principais**. Essas componentes são combinações lineares das variáveis originais, ortogonais entre si e ordenadas pela quantidade de variância que explicam. A primeira componente captura a maior parte da variação, enquanto as seguintes representam a variância remanescente, em ordem decrescente. Assim, cada nova componente contribui com frações progressivamente menores da variabilidade total, até que as últimas retêm apenas pequenas parcelas da informação, muitas vezes associadas a ruído ou a variações de baixa relevância, que podem ser descartadas sem comprometer a identificação dos principais padrões de ordenação nos dados.\n\nAo identificar as direções de máxima variabilidade, a PCA projeta os dados nesses novos eixos, permitindo eliminar redundâncias, reduzir ruídos e destacar padrões relevantes. Isso é especialmente útil quando os objetos em estudo são descritos por um grande número de variáveis que podem ser correlacionadas entre si. Dessa forma, em vez de analisar separadamente gráficos de dispersão para todos os pares de variáveis, a PCA organiza os objetos em um espaço multidimensional e os projeta em gráficos bidimensionais ou tridimensionais, cujos eixos concentram grande parte da variabilidade total e facilitam a interpretação das relações e agrupamentos presentes nos dados.\n\n::: {.callout-tip title=\"Código em Python\"}\n\nAcompanha este tutorial o [Código em Python sobre Introdução à PCA](pca_python.py){target=\"_blank\" title=\"Código em Python\"}\n\n:::\n\n**Propriedades Fundamentais da PCA:**\n\n1. Os eixos principais são **ortogonais** entre si, representando direções linearmente independentes.\n2. Os autovalores ($\\lambda$), que representam a variância ao longo de cada eixo, são sempre positivos ou nulos.\n3. A técnica permite resumir, em poucas dimensões, a maior parte da variabilidade de uma matriz de dados com muitos descritores, além de medir a quantidade de variância explicada por esses eixos.\n\nA PCA preserva a **distância euclidiana** entre os objetos, o que significa que a posição relativa entre eles não muda após a rotação dos eixos.\n\n## A Matemática da PCA\n\nA PCA é definida como a análise de autovalores e autovetores de uma matriz de dispersão (covariância ou correlação).\n\n### A Matriz de Dados\n\nConsidere um conjunto de dados organizado em uma matriz $\\mathbf{Y}$, na qual as linhas representam $n$ objetos (observações) e as colunas representam $p$ descritores (variáveis). Para aplicar a PCA, o primeiro passo é centralizar os dados, subtraindo de cada valor a média de sua respectiva coluna. O resultado é a matriz $\\mathbf{Y_c}$.\n\nPara ilustrar, vamos usar um exemplo didático com apenas duas variáveis, o que facilita compreender como a PCA transforma os dados em um novo espaço multidimensional. Nesse caso, cada aluno é descrito por duas informações: o tempo de estudo dedicado a uma determinada disciplina e a frequência em sala de aula, ambos expressos em horas.\n\nA matriz de dados brutos $\\mathbf{Y}$ e a matriz centralizada $\\mathbf{Y}$ teriam o seguinte formato:\n\n- **Matriz $\\mathbf{Y}$ ($n \\times p$):**\n\n::: {#e62dc600 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nY = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Notas.csv')\nn = Y.shape[0]\np = Y.shape[1]\n```\n:::\n\n\n::: {.table-narrow}\n\n::: {#tbl-alunos2d .cell tbl-cap='Nome, frequência em sala de aula e tempo dedicado ao estudo.' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\ncolunas = ['Nome', 'Frequencia', 'Estudo_horas']\nY_lista = Y[colunas].values.tolist()\n\nMarkdown(tabulate(\n  Y_lista, \n  headers=colunas\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=68}\nNome        Frequencia    Estudo_horas\n--------  ------------  --------------\nAna                 36               9\nHelena              38              10\nDiana               34               5\nCarlos              36              11\nEduardo             40              14\nFernanda            40              15\nGustavo             32               7\nBruno               33               6\nIgor                31               5\nJuliana             35               6\n:::\n:::\n\n\n:::\n\nOnde $n = 10$ é o número de alunos e $p = 4$ o número de descritores.\n\n- **Matriz Yc (Dados Centralizados):**\n  \n  Cada valor $y_{ij}$ é transformado em $y_{ij} - \\bar{y}_j$, onde $\\bar{y}_j$ é a média da coluna $j$.\n  \n  $$Y_c = Y - \\bar{y}$$\n\n::: {#d817841f .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nvariaveis = [\"Frequencia\", \"Estudo_horas\"]\ny_bar = Y[variaveis].mean()\nFreq_avg = float(y_bar['Frequencia'])\nE_avg = float(y_bar['Estudo_horas'])\n```\n:::\n\n\nNeste exemplo, a  Frequência média em sala de aula é 35.5, enquanto o tempo de estudo médio por estudante é 8.8. Desta modo a matriz centralizada é expressa por:\n\n::: {.table-narrow}\n\n::: {#tbl-alunos_centralizado2d .cell tbl-cap='Matriz de daods centralizados.' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nY_c = pd.DataFrame(Y['Nome'])\nY_c[variaveis] = Y[colunas[1:]] - y_bar\nY_c_lista = Y_c.values.tolist()\n\nMarkdown(tabulate(\n  Y_c_lista, \n  headers=colunas\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=70}\nNome        Frequencia    Estudo_horas\n--------  ------------  --------------\nAna                0.5             0.2\nHelena             2.5             1.2\nDiana             -1.5            -3.8\nCarlos             0.5             2.2\nEduardo            4.5             5.2\nFernanda           4.5             6.2\nGustavo           -3.5            -1.8\nBruno             -2.5            -2.8\nIgor              -4.5            -3.8\nJuliana           -0.5            -2.8\n:::\n:::\n\n\n:::\n\n### Matriz de Dispersão (Covariância)\n\nA PCA opera sobre uma matriz de dispersão $\\mathbf{S}$, que pode ser uma matriz de covariância ou correlação. A matriz de covariância $\\mathbf{S}$ é calculada a partir dos dados centralizados $\\mathbf{Y_c}$:\n\n$$\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Y_c'}\\mathbf{Y_c}$$\n\nOnde $\\mathbf{Y_c'}$ é a transposta de $\\mathbf{Y_c}$.\n\nA matriz de covariância dos dados deste exemplo é dada por:\n\n::: {#4e9f7fc8 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nS = Y_c[variaveis].cov(ddof=0)\nS\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Frequencia</th>\n      <th>Estudo_horas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Frequencia</th>\n      <td>8.85</td>\n      <td>9.30</td>\n    </tr>\n    <tr>\n      <th>Estudo_horas</th>\n      <td>9.30</td>\n      <td>11.96</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNesta matriz, as diagonais representam as variâncias da frequência ($s_{freq} = 8.85$) e tempo de estudo ($s_{estudo} = 11.96$), enquando a covariância entre as variáveis é $9.3$\n\n### Autovalores e Autovetores da matris de covariância\n\nOs eixos principais da matriz de dispersão $\\mathbf{S}$ são encontrados resolvendo a seguinte equação para os autovalores ($\\lambda$) e autovetores ($u$):\n\n$$(S - \\lambda_k I)u_k = 0$$\n\nOnde:\n\n- $\\lambda_k$ é o $k$-ésimo **autovalor**. Ele representa a quantidade de variância dos dados ao longo do $k$-ésimo eixo principal.\n- $u_k$ é o $k$-ésimo **autovetor** associado a $\\lambda_k$. Ele define a direção do $k$-ésimo eixo principal.\n- $I$ é uma matriz identidade.\n\nOs autovalores são calculados a partir da equação característica:\n\n$$|S - \\lambda_k I| = 0$$\n\nE neste exemplo os autovetores são:\n\n::: {#d8f02e78 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nYcv = Y_c[variaveis]\npca_2d = PCA()\npca_2d.fit(Ycv)\nautovalores = pca_2d.explained_variance_\nLambda_list = autovalores.tolist()\nLambda_formatada = [float(np.round(num,2)) for num in Lambda_list]\nR2 = Lambda_list / np.sum(Lambda_list)\nR2_formatada = [float(np.round(num,4)) for num in R2]\nR2_perc = [float(np.round(num*100,2)) for num in R2]\n```\n:::\n\n\n$\\Lambda = [22.04, 1.08]$\n\nA soma de todos os autovalores ($\\lambda_k$) é igual à variância total dos dados, que é a soma dos elementos da diagonal da matriz $\\mathbf{S}$ (traço da matriz). A proporção da variância total explicada por um conjunto de $m$ componentes principais é dada por:\n\n$$R^2 = \\frac{\\sum_{k=1}^{m}\\lambda_k}{\\sum_{k=1}^{p}\\lambda_k} = [0.9531, 0.0469]$$\n\nNo exemplo, o PC1 explica a maior parte da variância ($\\lambda_1 = 95.31$%) e o PC2 explica o restante ($\\lambda_2 = 4.69$%), somando 100%.\n\nOs **autovetores** são então calculados e normalizados para terem comprimento unitário ($u'u = 1$). Eles formam as colunas da matriz $\\mathbf{U}$. \n\n::: {.table-narrow}\n\n::: {#e466a876 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nU = pca_2d.components_\npd.DataFrame(U, columns=['u1', 'u2'])\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>u1</th>\n      <th>u2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.646175</td>\n      <td>0.763189</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763189</td>\n      <td>-0.646175</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\nOs elementos dos autovetores são também chamados de **loadings** (pesos) e indicam como cada variável original contribui para a formação de cada componente principal.\n\n### Componentes Principais (Scores)\n\nOs componentes principais, também conhecidos como *scores*, são as coordenadas dos objetos no novo sistema de eixos. Eles são calculados projetando os dados centralizados nos autovetores. A posição de um objeto $i$ no primeiro eixo principal é dada por:\n\n$$f_{i1} = (y_{i1}-\\bar{y}_1)u_{11} + ... + (y_{ip}-\\bar{y}_p)u_{p1}$$\n\nMatricialmente, a matriz **F** dos componentes principais para todos os objetos é calculada como:\n\n$$F = Y_c U$$\n\n::: {#4e03c36a .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nF_2d_lista = pca_2d.transform(Ycv)\nF_2d = pd.DataFrame(Y['Nome'])\nF_2d[['PCA1', 'PCA2']] = pd.DataFrame(F_2d_lista)\n```\n:::\n\n\n::: {.table-narrow}\n\n::: {#tbl-pca_2d_loadings .cell tbl-cap='Matriz **F** contendo os scores dos componentes principais 1 e 2 para cada objeto da matriz original.' execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nMarkdown(tabulate(\n  F_2d.values.tolist(), \n  headers=['PCA1', 'PCA2']\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=75}\n               PCA1        PCA2\n--------  ---------  ----------\nAna        0.475725   0.252359\nHelena     2.53127    1.13256\nDiana     -3.86938    1.31068\nCarlos     2.0021    -1.03999\nEduardo    6.87637    0.0742388\nFernanda   7.63956   -0.571936\nGustavo   -3.63535   -1.50805\nBruno     -3.75237   -0.0986816\nIgor      -5.80791   -0.978884\nJuliana   -2.46002    1.4277\n:::\n:::\n\n\n:::\n\nAs colunas da matriz **F** contêm os *scores* dos objetos (nas linhas) para cada componente principal e podem ser usadas para criar gráficos de ordenação, como os biplots.\n\n\n## Interpretando os Resultados da PCA: Biplots\n\nUm **biplot** é um gráfico que exibe simultaneamente os objetos (scores, da matriz $\\mathbf{F}$) e os descritores (loadings, da matriz $\\mathbf{U}$). Existem duas formas principais de escalonamento (scaling) para biplots, cada uma com uma interpretação diferente.\n\n### Scaling 1 (Biplot de Distância)\n\n- **Objetivo:** Preservar a distância Euclidiana entre os objetos.\n- **Matrizes:** Usa os scores da matriz $\\mathbf{F}$ e os autovetores da matriz $\\mathbf{U}$ (comprimento 1).\n- **Interpretação:**\n  - A distância entre os pontos dos objetos no gráfico aproxima a sua distância Euclidiana no espaço multidimensional.\n  - A projeção ortogonal de um objeto sobre um eixo de descritor aproxima o valor daquele objeto para aquele descritor.\n  - Os ângulos entre os vetores dos descritores **não** presenvam os ângulos entre os vetores originais.\n\n::: {#cell-fig-pca2d-biplot-scaling1 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# Criar o biplot\nplt.figure(figsize=(8, 6))\nsns.set_style(\"whitegrid\")\n\n# Plotar os objetos (alunos) usando o dataframe F_2d\nscatter = sns.scatterplot(data=F_2d, x='PCA1', y='PCA2', \n                         s=100, alpha=0.8, color='steelblue')\n\n# Adicionar labels para os alunos\nfor i, row in F_2d.iterrows():\n    plt.annotate(row['Nome'], (row['PCA1'], row['PCA2']), \n                xytext=(5, 5), textcoords='offset points', \n                fontsize=9, alpha=0.8, fontweight='medium')\n\n# Plotar os vetores das variáveis (loadings)\nscale_factor = 2.5  # Fator de escala para melhor visualização\n\nfor i, var in enumerate(variaveis):\n    plt.arrow(0, 0, U[0, i] * scale_factor, U[1, i] * scale_factor, \n              head_width=0.15, head_length=0.15, fc='red', ec='red', \n              linewidth=2.5, alpha=0.9)\n    plt.text(U[0, i] * scale_factor * 1.15, U[1, i] * scale_factor * 1.15, \n             var, color='red', fontsize=13, fontweight='bold',\n             ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", \n                                               facecolor='white', \n                                               alpha=0.8))\n\n# Configurações do gráfico\nplt.axhline(y=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)\nplt.axvline(x=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)\n\nplt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variância explicada)',\n          fontsize=12, fontweight='bold')\nplt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variância explicada)',\n          fontsize=12, fontweight='bold')\nplt.title('Biplot - Análise de Componentes Principais\\nDistâncias entre alunos e correlações entre variáveis',\n         fontsize=14, fontweight='bold', pad=20)\n\n# Adicionar grid\nplt.grid(True, alpha=0.3, linestyle='--')\n\n# Ajustar limites para melhor visualização\nplt.xlim(-10, 10)\nplt.ylim(-3, 3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Gráfico de um biplot de distância (Scaling 1). A distância entre os objetos presenva suas distâncias eucildianas originais.](introducao_PCA_2d_files/figure-html/fig-pca2d-biplot-scaling1-output-1.png){#fig-pca2d-biplot-scaling1 width=759 height=565}\n:::\n:::\n\n\n### Scaling 2 (Biplot de Correlação)\n\n- **Objetivo:** Representar as correlações (ou covariâncias) entre os descritores.\n- **Matrizes:** Usa os autovetores escalados pela raiz quadrada dos autovalores ($U_{sc2} = U\\Lambda^{1/2}$) para os descritores e scores reescalados ($G = F\\Lambda^{-1/2}$) para os objetos.\n- **Interpretação:**\n  - Os ângulos entre os vetores dos descritores refletem suas correlações: vetores próximos indicam alta correlação positiva; vetores em direções opostas indicam correlação negativa; vetores em 90° indicam ausência de correlação.\n  - O comprimento do vetor de um descritor é uma aproximação de seu desvio padrão.\n  - A distância entre os objetos no gráfico aproxima sua **distância de Mahalanobis**, não a distância Euclidiana.\n\n::: {#cell-fig-pca2d-biplot-scaling2 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# Calcular os componentes para Scaling 2\nautovalores = pca_2d.explained_variance_\nlambda_sqrt = np.sqrt(autovalores)  # Λ^(1/2)\nlambda_inv_sqrt = 1 / lambda_sqrt   # Λ^(-1/2)\n\n# Reescalar os loadings: U_sc2 = U * Λ^(1/2)\nU_sc2 = U * lambda_sqrt.reshape(-1, 1)\n\n# Reescalar os scores: G = F * Λ^(-1/2)\nG_sc2 = F_2d_lista * lambda_inv_sqrt\n\n# Criar dataframe com scores reescalados\nG_2d = Y[['Nome']].copy()\nG_2d[['PC1_sc2', 'PC2_sc2']] = pd.DataFrame(G_sc2)\n\n# Criar o biplot de correlação\nplt.figure(figsize=(8, 6))\nsns.set_style(\"whitegrid\")\n\n# Plotar os objetos (alunos) reescalados\nscatter = sns.scatterplot(data=G_2d, x='PC1_sc2', y='PC2_sc2', \n                         s=100, alpha=0.8, color='steelblue')\n\n# Adicionar labels para os alunos\nfor i, row in G_2d.iterrows():\n    plt.annotate(row['Nome'], (row['PC1_sc2'], row['PC2_sc2']), \n                xytext=(5, 5), textcoords='offset points', \n                fontsize=9, alpha=0.8, fontweight='medium')\n\n# Plotar os vetores das variáveis (loadings reescalados)\nscale_factor = .5  # Fator de escala para melhor visualização\n\nfor i, var in enumerate(variaveis):\n    plt.arrow(0, 0, U_sc2[0, i] * scale_factor, U_sc2[1, i] * scale_factor, \n              head_width=0.15, head_length=0.15, fc='red', ec='red', \n              linewidth=2.5, alpha=0.9)\n    plt.text(U_sc2[0, i] * scale_factor * 1.15, U_sc2[1, i] * scale_factor * 1.15, \n             var, color='red', fontsize=13, fontweight='bold',\n             ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", \n                                               facecolor='white', \n                                               alpha=0.8))\n\n# Configurações do gráfico\nplt.axhline(y=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)\nplt.axvline(x=0, color='gray', linestyle='-', alpha=0.4, linewidth=1)\n\nplt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variância explicada)',\n          fontsize=12, fontweight='bold')\nplt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variância explicada)',\n          fontsize=12, fontweight='bold')\nplt.title('Biplot de Correlação (Scaling 2) - Análise de Componentes Principais\\nÂngulos entre vetores representam correlações entre variáveis',\n         fontsize=14, fontweight='bold', pad=20)\n\n# Adicionar grid\nplt.grid(True, alpha=0.3, linestyle='--')\n\n# Ajustar limites para melhor visualização\nplt.xlim(-2, 3)\nplt.ylim(-2, 2)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Gráfico de um biplot de correlação (Scaling 2). Os ângulos entre os vetores dos descritores (setas vermelhas) indicam a correlação entre as variáveis originais.](introducao_PCA_2d_files/figure-html/fig-pca2d-biplot-scaling2-output-1.png){#fig-pca2d-biplot-scaling2 width=781 height=565}\n:::\n:::\n\n\n### Conclusão\n\n::: {#7c3dbf3f .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\ndef angle_between_vectors(v1, v2):\n    \"\"\"Calcula o ângulo entre dois vetores em graus\"\"\"\n    cos_angle = np.dot(v1, v2) / (norm(v1) * norm(v2))\n    return degrees(acos(np.clip(cos_angle, -1.0, 1.0)))\n\ncorr_matrix = Y[variaveis].corr()\ncorr_coef = f\"{float(corr_matrix.iloc[0, 1]):.2f}\"\n\nangle_vars = angle_between_vectors(U_sc2[:, 0], U_sc2[:, 1])\nangle = f\"{angle_vars:.2f}\"\n```\n:::\n\n\nA primeira componente principal (PC1) explicou 95.31% da variância total, enquanto a segunda (PC2) respondeu por apenas 4.69%. Observamos que as variáveis originais, *Frequência* e *Estudo_horas*, apresentam uma forte correlação positiva (coeficiente de correlação = 0\\.90), o que é visualmente confirmado no biplot de correlação (Scaling 2): seus vetores apontam praticamente na mesma direção, formando um ângulo de 25\\.32°. Isso indica que estudantes que dedicam mais tempo de estudo tendem também a ter maior frequência em sala de aula.\n\nNo biplot de distância (Scaling 1), nota-se a proximidade entre os alunos com melhor desempenho, como Fernanda e Eduardo, que obtiveram os maiores valores em ambas as variáveis. Assim, a ordenação dos alunos a partir das variáveis originais pode ser bem representada por um único componente, o PC1. Nesse exemplo, o PC1 reflete a associação conjunta entre *Frequência* e *Estudo_horas*, podendo ser interpretado como um indicador de *nível de engajamento* do estudante na disciplina.\n\n## A Escolha da Matriz de Dispersão na PCA: Covariância vs. Correlação\n\nA Análise de Componentes Principais (PCA) opera sobre uma matriz de dispersão, que pode ser uma **matriz de covariância** ($\\mathbf{S}$) ou uma **matriz de correlação** ($\\mathbf{R}$). A escolha entre essas duas matrizes depende diretamente da natureza das variáveis originais.\n\n1. **PCA baseada na Matriz de Covariância ($\\mathbf{S}$):** A análise é realizada diretamente sobre os dados centralizados ($Y_c$), em que cada valor teve a média de sua coluna subtraída. Essa abordagem é adequada quando as variáveis são **dimensionalmente homogêneas**, ou seja, possuem as mesmas unidades físicas e ordens de magnitude e variabilidade semelhantes. Nesse caso, as diferenças de variância entre as variáveis são consideradas informações relevantes, e a PCA buscará os eixos que maximizam a covariância total.\n\n2. **PCA baseada na Matriz de Correlação ($\\mathbf{R}$):** Quando as variáveis não são dimensionalmente homogêneas — por exemplo, ao analisar dados que combinam temperatura (°C), peso (kg) e concentração (mg/L) — ou quando suas variâncias são muito discrepantes, a padronização dos dados se torna essencial. A padronização consiste em centralizar os dados (subtrair a média) e, adicionalmente, dividir cada valor pelo desvio padrão da respectiva variável. Esse processo transforma os dados para que tenham média 0 e desvio padrão 1. **Uma propriedade deste processo é que a matriz de covariância das variáveis padronizadas é idêntica à matriz de correlação das variáveis originais.** Portanto, realizar a PCA sobre dados padronizados equivale a realizá-la sobre a matriz de correlação ($\\mathbf{R}$). Essa abordagem garante que todas as variáveis contribuam igualmente para a análise, evitando que descritores com maior variância dominem a definição dos componentes principais.\n\n### Exemplo: Frequência versus Tempo de Estudo\n\nNo exemplo dos alunos, com as variáveis `Frequencia` e `Estudo_horas`, a PCA foi conduzida sobre a matriz de covariância. Isso foi possível porque as variâncias das duas variáveis são relativamente parecidas, como mostrado na @tbl-alunos2d. Neste cenário, a padronização não foi estritamente necessária, pois nenhuma das variáveis tinha variância suficientemente grande para ofuscar a outra, permitindo que a análise capturasse a estrutura de covariância e refletisse a forte associação positiva entre as duas variáveis.\n\nEm casos mais gerais, especialmente quando se mede uma ampla gama de descritores com unidades e escalas distintas, **a padronização é fortemente recomendada**, pois, se a PCA for aplicada a uma matriz de covariância de variáveis com variâncias muito díspares, os primeiros componentes principais seriam predominantemente determinados pelas variáveis de maior variância, e a estrutura de correlação entre as demais variáveis não seria descrita adequadamente.\n\nA decisão de usar a matriz $\\mathbf{S}$ ou $\\mathbf{R}$ se resume a determinar se a **magnitude da variância das variáveis originais deve influenciar a análise ou se todas as variáveis devem ter o mesmo peso**. Se for importante manter as diferenças de magnitude, utiliza-se a matriz de covariância. Caso contrário, para garantir que a PCA revele a estrutura de correlação subjacente entre os descritores, a padronização dos dados e, consequentemente, o uso da matriz de correlação é o caminho mais seguro.\n\n",
    "supporting": [
      "introducao_PCA_2d_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}