{
  "hash": "81f843eb7eee93a7009267ee100981d8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Análise de Componentes Principais (PCA)\"\nsubtitle: \"Tutorial Prático com Python\"\nexecute:\n  echo: false\n  warning: false\n  include: true\n  message: false\n  eval: true\nformat: pdf\n---\n\n\n\n## Introdução à Análise de Componentes Principais (PCA)\n\nA Análise de Componentes Principais (PCA) é uma técnica multivariada utilizada para organizar e representar objetos (como locais de amostragem, estações, indivíduos, etc.) em um espaço de dimensões reduzidas. Seu objetivo é simplificar conjuntos de dados complexos, com muitas variáveis correlacionadas, transformando-os em um número menor de variáveis não correlacionadas, chamadas **componentes principais**. Essas componentes são combinações lineares das variáveis originais, ortogonais entre si e ordenadas pela quantidade de variância que explicam. A primeira componente captura a maior parte da variação, enquanto as seguintes representam a variância remanescente, em ordem decrescente. Assim, cada nova componente contribui com frações progressivamente menores da variabilidade total, até que as últimas retêm apenas pequenas parcelas da informação, muitas vezes associadas a ruído ou a variações de baixa relevância, que podem ser descartadas sem comprometer a identificação dos principais padrões de ordenação nos dados.\n\nAo identificar as direções de máxima variabilidade, a PCA projeta os dados nesses novos eixos, permitindo eliminar redundâncias, reduzir ruídos e destacar padrões relevantes. Isso é especialmente útil quando os objetos em estudo são descritos por um grande número de variáveis que podem ser correlacionadas entre si. Dessa forma, em vez de analisar separadamente gráficos de dispersão para todos os pares de variáveis, a PCA organiza os objetos em um espaço multidimensional e os projeta em gráficos bidimensionais ou tridimensionais, cujos eixos concentram grande parte da variabilidade total e facilitam a interpretação das relações e agrupamentos presentes nos dados.\n\n**Propriedades Fundamentais da PCA:**\n\n1. Os eixos principais são **ortogonais** entre si, representando direções linearmente independentes.\n2. Os autovalores ($\\lambda$), que representam a variância ao longo de cada eixo, são sempre positivos ou nulos.\n3. A técnica permite resumir, em poucas dimensões, a maior parte da variabilidade de uma matriz de dados com muitos descritores, além de medir a quantidade de variância explicada por esses eixos.\n\nA PCA preserva a **distância euclidiana** entre os objetos, o que significa que a posição relativa entre eles não muda após a rotação dos eixos.\n\n## A Matemática da PCA\n\nA PCA é definida como a análise de autovalores e autovetores de uma matriz de dispersão (covariância ou correlação).\n\n### A Matriz de Dados\n\nConsidere um conjunto de dados organizado em uma matriz $\\mathbf{Y}$, na qual as linhas representam $n$ objetos (observações) e as colunas representam $p$ descritores (variáveis). Para aplicar a PCA, o primeiro passo é centralizar os dados, subtraindo de cada valor a média de sua respectiva coluna. O resultado é a matriz $\\mathbf{Y_c}$.\n\nPara ilustrar, vamos usar um exemplo didático com apenas duas variáveis, o que facilita compreender como a PCA transforma os dados em um novo espaço multidimensional. Nesse caso, cada aluno é descrito por duas informações: o tempo de estudo dedicado a uma determinada disciplina e a frequência em sala de aula, ambos expressos em horas.\n\nA matriz de dados brutos $\\mathbf{Y}$ e a matriz centralizada $\\mathbf{Y}$ teriam o seguinte formato:\n\n- **Matriz $\\mathbf{Y}$ ($n \\times p$):**\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n| Nome    |   Frequencia |   Estudo_horas |   Nota_final |\n|:--------|-------------:|---------------:|-------------:|\n| Ana     |           36 |              9 |          7   |\n| Helena  |           38 |             10 |          5.5 |\n| Diana   |           34 |              5 |          3   |\n| Carlos  |           36 |             11 |          6.2 |\n| Eduardo |           40 |             14 |          9.6 |\n```\n:::\n:::\n\n\n  | Nome   | Frequência   | Estudo     |\n  |--------|--------------|------------|\n  | Ana | 36  | 9 |\n  | Helena | 38  | 10 |\n  | Diana | 34  | 5 |\n  | ...    | ...          | ...        |\n  | Juliana | 35  | 9 |\n\n  \n  Onde $n$ é o número de alunos e $p = 2$.\n\n- **Matriz Yc (Dados Centralizados):**\n  \n  Cada valor $y_{ij}$ é transformado em $y_{ij} - \\bar{y}_j$, onde $\\bar{y}_j$ é a média da coluna $j$.\n  \n  $$Y_c = Y - \\bar{y}$$\n\n### Matriz de Dispersão (Covariância)\n\nA PCA opera sobre uma matriz de dispersão **S**, que pode ser uma matriz de covariância ou correlação. A matriz de covariância **S** é calculada a partir dos dados centralizados **Yc**:\n\n$$S = \\frac{1}{n-1} Y_c'Y_c$$\n\nOnde $Y_c'$ é a transposta de $Y_c$.\n\n### Autovalores (Eigenvalues) e Autovetores (Eigenvectors)\n\nOs eixos principais da matriz de dispersão **S** são encontrados resolvendo a seguinte equação para os autovalores ($\\lambda$) e autovetores ($u$):\n\n$$(S - \\lambda_k I)u_k = 0$$\n\nOnde:\n\n- $\\lambda_k$ é o $k$-ésimo **autovalor**. Ele representa a quantidade de variância dos dados ao longo do $k$-ésimo eixo principal.\n- $u_k$ é o $k$-ésimo **autovetor** associado a $\\lambda_k$. Ele define a direção do $k$-ésimo eixo principal.\n- $I$ é uma matriz identidade.\n\nOs autovalores são calculados a partir da equação característica:\n\n$$|S - \\lambda_k I| = 0$$\n\nA soma de todos os autovalores ($\\lambda_k$) é igual à variância total dos dados, que é a soma dos elementos da diagonal da matriz **S** (traço da matriz). A proporção da variância total explicada por um conjunto de $m$ componentes principais é dada por:\n\n$$R^2 = \\frac{\\sum_{k=1}^{m}\\lambda_k}{\\sum_{k=1}^{p}\\lambda_k}$$\n\nNo exemplo bidimensional, o PC1 explica a maior parte da variância e o PC2 explica o restante, somando 100%. Para o exemplo tridimensional (incluindo `Nota_final`), o PC1 explica 85.5% da variância, enquanto o PC1 e o PC2 juntos explicam 97.4%.\n\nOs **autovetores** são então calculados e normalizados para terem comprimento unitário ($u'u = 1$). Eles formam as colunas da matriz **U**. Os elementos dos autovetores são também chamados de **loadings** (pesos) e indicam como cada variável original contribui para a formação de cada componente principal.\n\n### Componentes Principais (Scores)\n\nOs componentes principais, também conhecidos como *scores*, são as coordenadas dos objetos no novo sistema de eixos. Eles são calculados projetando os dados centralizados nos autovetores. A posição de um objeto $i$ no primeiro eixo principal é dada por:\n\n$$f_{i1} = (y_{i1}-\\bar{y}_1)u_{11} + ... + (y_{ip}-\\bar{y}_p)u_{p1}$$\n\nMatricialmente, a matriz **F** dos componentes principais para todos os objetos é calculada como:\n\n$$F = Y_c U$$\n\nAs colunas da matriz **F** contêm os *scores* dos objetos para cada componente principal e podem ser usadas para criar gráficos de ordenação, como os biplots.\n\n## Padronização: Análise sobre a Matriz de Correlação\n\nQuando as variáveis não estão nas mesmas unidades ou possuem ordens de magnitude muito diferentes, elas devem ser padronizadas antes da PCA. A padronização (subtrair a média e dividir pelo desvio padrão) transforma a matriz de covariância **S** em uma matriz de correlação **R**.\n\nA PCA realizada sobre a matriz **R** garante que todas as variáveis contribuam igualmente para a análise, independentemente de suas variâncias originais. No exemplo tridimensional do tutorial, os dados são padronizados porque `Estudo_horas`, `Frequencia` e `Nota_final` possuem escalas diferentes. **Importante:** a matriz de covariância das variáveis padronizadas é igual à matriz de correlação das variáveis originais.\n\n## Interpretando os Resultados da PCA: Biplots\n\nUm **biplot** é um gráfico que exibe simultaneamente os objetos (scores, da matriz **F** ou **G**) e os descritores (loadings, da matriz **U** ou **Usc2**). Existem duas formas principais de escalonamento (scaling) para biplots, cada uma com uma interpretação diferente.\n\n### Scaling 1 (Biplot de Distância)\n\n- **Objetivo:** Preservar a distância Euclidiana entre os objetos.\n- **Matrizes:** Usa os scores da matriz **F** e os autovetores da matriz **U** (comprimento 1).\n- **Interpretação:**\n  - A distância entre os pontos dos objetos no gráfico aproxima a sua distância Euclidiana no espaço multidimensional.\n  - A projeção ortogonal de um objeto sobre um eixo de descritor aproxima o valor daquele objeto para aquele descritor.\n  - Os ângulos entre os vetores dos descritores **não** são significativos (são ortogonais no espaço completo).\n\n![Gráfico de um biplot de distância (Scaling 1)](https://i.imgur.com/kYmB0t5.png)\n\n*Figura 1: Representação de um biplot de distância (scaling 1). A projeção dos objetos nos eixos dos descritores é a chave para a interpretação. Baseado na Figura 9.3a.*\n\n### Scaling 2 (Biplot de Correlação)\n\n- **Objetivo:** Representar as correlações (ou covariâncias) entre os descritores.\n- **Matrizes:** Usa os autovetores escalados pela raiz quadrada dos autovalores ($U_{sc2} = U\\Lambda^{1/2}$) para os descritores e scores reescalados ($G = F\\Lambda^{-1/2}$) para os objetos.\n- **Interpretação:**\n  - Os ângulos entre os vetores dos descritores refletem suas correlações: vetores próximos indicam alta correlação positiva; vetores em direções opostas indicam correlação negativa; vetores em 90° indicam ausência de correlação.\n  - O comprimento do vetor de um descritor é uma aproximação de seu desvio padrão.\n  - A distância entre os objetos no gráfico aproxima sua **distância de Mahalanobis**, não a distância Euclidiana.\n  \nNo exemplo tridimensional do tutorial, o biplot gerado é um biplot de correlação (scaling 2), onde `Estudo_horas`, `Frequencia` e `Nota_final` formam ângulos pequenos entre si, indicando que estão positivamente correlacionadas.\n\n![Gráfico de um biplot de correlação (Scaling 2)](https://i.imgur.com/x4hCq9o.png)\n\n*Figura 2: Representação de um biplot de correlação (scaling 2). Os ângulos entre os vetores dos descritores (setas vermelhas) indicam a correlação entre as variáveis originais.*\n\n## Escolhendo o Número de Componentes Significativos\n\nDecidir quantos componentes principais reter é um passo crucial. Algumas abordagens incluem:\n\n- **Scree Plot:** Um gráfico de barras dos autovalores (ou da variância explicada) em ordem decrescente. Procura-se um \"cotovelo\" (um ponto onde a queda da variância explicada se torna menos acentuada) para determinar o número de componentes a reter. O tutorial utiliza este gráfico para mostrar a importância relativa de cada componente na análise tridimensional.\n\n- **Critério de Kaiser-Guttman:** Para PCA em uma matriz de correlação, retêm-se apenas os componentes com autovalores ($\\lambda$) maiores que 1. A lógica é que um autovalor de 1 representa a variância de uma única variável padronizada.\n\n- **Modelo \"Broken Stick\":** Compara-se a proporção de variância de cada componente com os valores esperados de um modelo nulo onde a variância total é dividida aleatoriamente entre os componentes. Componentes que explicam mais variância do que o esperado pelo modelo são considerados significativos.\n\n",
    "supporting": [
      "tutorial_PCA_NL_files"
    ],
    "filters": []
  }
}