{
  "hash": "449f158803c5953e06ed70166f6b59fc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"An√°lise de Componentes Principais (PCA)\"\nsubtitle: \"Tutorial Pr√°tico com Python\"\nexecute:\n  echo: true\n  warning: false\n  include: true\n  message: false\n  eval: false\nformat: pdf\n---\n\n\n# Introdu√ß√£o √† An√°lise de Componentes Principais\n\nA **An√°lise de Componentes Principais (PCA)** √© uma t√©cnica alg√©brica, usada para reduzir a dimensionalidade de conjuntos de dados complexos. Ela simplifica dados com muitas vari√°veis correlacionadas, transformando-as em um n√∫mero menor de vari√°veis *n√£o correlacionadas*, chamadas **componentes principais**. Essas componentes s√£o *combina√ß√µes lineares* das vari√°veis originais, **ortogonais entre si** e ordenadas pela quantidade de vari√¢ncia que explicam, sendo que a primeira componente captura a maior parte da varia√ß√£o dos dados.\n\nAo identificar as dire√ß√µes onde a variabilidade dos dados √© m√°xima, a PCA projeta os dados nessas novas dire√ß√µes, o que permite eliminar redund√¢ncias, reduzir ru√≠dos e descobrir padr√µes principais. Isso facilita a visualiza√ß√£o de dados multivariados mesmo em espa√ßos bidimensionais ou tridimensionais, al√©m de acelerar algoritmos de machine learning ao diminuir o n√∫mero de atributos necess√°rios para descrever padr√µes no conjunto de dados.\n\n# Prepara√ß√£o do Ambiente\n\nVamos come√ßar importando as bibliotecas necess√°rias para nossa an√°lise:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Importa√ß√£o de bibliotecas essenciais\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom scipy.stats import pearsonr\n\n# Configura√ß√µes visuais para gr√°ficos mais bonitos\nplt.style.use('default')\nsns.set_palette(\"husl\")\n```\n:::\n\n\n# PCA com Dados Bidimensionais\n\nVamos trabalhar com um dataset de notas de alunos que relaciona tempo de estudo, frequ√™ncia nas aulas e nota final:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Carregando o dataset\nnotas = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Notas.csv')\nnotas\n```\n:::\n\n\nPara entender melhor o PCA, vamos come√ßar com apenas duas vari√°veis: **tempo de estudo** e **frequ√™ncia nas aulas**.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Selecionando apenas duas vari√°veis para an√°lise inicial\nnotas_simpl = notas[['Estudo_horas', 'Frequencia']]\n\nnotas_simpl\n```\n:::\n\n\n## Visualizando os Dados Originais\n\nVamos visualizar como os alunos se distribuem no espa√ßo bidimensional original:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=notas_simpl, x='Estudo_horas', y='Frequencia', \n                alpha=0.7, s=100, color='steelblue')\n\n# Adicionando r√≥tulos dos alunos para identifica√ß√£o\nfor i, nome in enumerate(notas['Nome']):\n    plt.text(x=notas_simpl['Estudo_horas'][i] + 0.1,\n             y=notas_simpl['Frequencia'][i] + 0.1,\n             s=nome, fontsize=9, alpha=0.7)\n\nplt.title('Rela√ß√£o entre Tempo de Estudo e Frequ√™ncia nas Aulas', \n          fontsize=14, pad=20)\nplt.xlabel('Tempo de Estudo (horas por semana)', fontsize=12)\nplt.ylabel('Frequ√™ncia nas Aulas (%)', fontsize=12)\nplt.ylim(30.5, 40.5)\nplt.xlim(4, 15.5)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üí° Observe:** Os pontos mostram uma tend√™ncia positiva - alunos que estudam mais tendem a ter maior frequ√™ncia nas aulas. Isso indica **correla√ß√£o** entre as vari√°veis!\n\n## Aplicando PCA aos Dados Bidimensionais\n\nAgora vamos aplicar o PCA para encontrar as dire√ß√µes de maior variabilidade:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Criando e ajustando o modelo PCA\npca_2d = PCA()\npca_2d.fit(notas_simpl)\n\n# Transformando os dados para o novo espa√ßo de componentes principais\nF_2d = pd.DataFrame(pca_2d.transform(notas_simpl), columns=['PCA1', 'PCA2'])\n\n# Adicionando os componentes principais ao dataframe original\nnotas[['PCA1', 'PCA2']] = F_2d\n\nF_2d\n```\n:::\n\n\n### Vari√¢ncia Explicada\n\nUma m√©trica fundamental da PCA √© o quanto da vari√¢ncia total cada componente explica:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Analisando a vari√¢ncia explicada por cada componente\nvariancia_explicada = pca_2d.explained_variance_ratio_\n\nprint(\"üìä AN√ÅLISE DA VARI√ÇNCIA:\")\nprint(\"=\" * 50)\nprint(f\"PC1 (1¬™ Componente): {variancia_explicada[0]*100:.2f}% da vari√¢ncia\")\nprint(f\"PC2 (2¬™ Componente): {variancia_explicada[1]*100:.2f}% da vari√¢ncia\")\nprint(f\"\\n‚úÖ Vari√¢ncia Total Explicada: {sum(variancia_explicada)*100:.2f}%\")\nprint(\"=\" * 50)\n\n# Nota: em 2D, sempre teremos 100% da vari√¢ncia explicada\nprint(\"\\nüí° Importante: Com 2 vari√°veis originais, 2 componentes principais\")\nprint(\"   sempre explicar√£o 100% da vari√¢ncia. O poder do PCA aparece\")\nprint(\"   quando reduzimos dimens√µes (ex: de 10 vari√°veis para 2 componentes).\")\n```\n:::\n\n\n## Visualizando os Componentes Principais\n\n### Componentes no Espa√ßo Original\n\nVamos visualizar as dire√ß√µes dos componentes principais sobrepostas aos dados originais:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 7))\n\n# Plotando os pontos originais\nsns.scatterplot(data=notas, x='Estudo_horas', y='Frequencia', \n                alpha=0.7, s=100, color='steelblue')\n\n# Adicionando r√≥tulos dos alunos\nfor i, nome in enumerate(notas['Nome']):\n    plt.text(x=notas['Estudo_horas'][i] + 0.1,\n             y=notas['Frequencia'][i] + 0.1,\n             s=nome, fontsize=9, alpha=0.7)\n\n# Calculando e marcando o centro dos dados\ncentro = notas_simpl.mean().to_numpy()\nplt.scatter(centro[0], centro[1], color='black', s=200, marker='x', \n            linewidth=3, label='Centro dos dados', zorder=5)\n\n# Obtendo os autovetores (dire√ß√µes dos componentes principais)\nU = pca_2d.components_\nescala = 10  # Fator de escala para visualiza√ß√£o\n\n# Calculando os √¢ngulos de rota√ß√£o para os r√≥tulos\nrotacao = np.degrees(np.arctan2(U[:, 1], U[:, 0]))\n\n# --- Desenhando o Componente Principal 1 (PC1) ---\n# Seta indicando a dire√ß√£o\nplt.arrow(centro[0], centro[1], \n          U[0, 0] * escala * 0.6, U[0, 1] * escala * 0.6,\n          head_width=0.3, head_length=0.2, fc='red', ec='red', \n          linewidth=2, alpha=0.8, zorder=4)\n\n# Linha pontilhada estendida\nx1, y1 = centro[0] - U[0, 0] * escala, centro[1] - U[0, 1] * escala\nx2, y2 = centro[0] + U[0, 0] * escala, centro[1] + U[0, 1] * escala\nplt.plot([x1, x2], [y1, y2], color='red', linestyle='--', linewidth=1.5, alpha=0.6)\n\n# R√≥tulo do PC1\nplt.text(x=centro[0] + U[0, 0] * escala * 0.3, \n         y=centro[1] + U[0, 1] * escala * 0.3 + 0.5,\n         s=\"Componente Principal 1\", \n         rotation=rotacao[0], fontsize=11, \n         color='red', fontweight='bold')\n\n# --- Desenhando o Componente Principal 2 (PC2) ---\n# Seta indicando a dire√ß√£o\nplt.arrow(centro[0], centro[1], \n          U[1, 0] * escala * 0.6, U[1, 1] * escala * 0.6,\n          head_width=0.3, head_length=0.2, fc='red', ec='red', \n          linewidth=2, alpha=0.8, zorder=4)\n\n# Linha pontilhada estendida\nx1, y1 = centro[0] - U[1, 0] * escala, centro[1] - U[1, 1] * escala\nx2, y2 = centro[0] + U[1, 0] * escala, centro[1] + U[1, 1] * escala\nplt.plot([x1, x2], [y1, y2], color='red', linestyle='--', linewidth=1.5, alpha=0.6)\n\n# R√≥tulo do PC2\nplt.text(x=centro[0] + U[1, 0] * escala * 0.5, \n         y=centro[1] + U[1, 1] * escala * 0.15,\n         s=\"Componente Principal 2\", \n         rotation=rotacao[1]+180, fontsize=11, \n         color='red', fontweight='bold')\n\nplt.title('Componentes Principais no Espa√ßo Original', fontsize=14, pad=20)\nplt.xlabel('Tempo de Estudo (horas)', fontsize=12)\nplt.ylabel('Frequ√™ncia (%)', fontsize=12)\nplt.ylim(29.5, 41.5)\nplt.xlim(3, 17)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üí° Conceitos Importantes:**\n\n- **Componente Principal 1 (PC1)**: Dire√ß√£o de maior variabilidade dos dados  \n- **Componente Principal 2 (PC2)**: Perpendicular ao PC1, captura a variabilidade restante  \n- Os componentes s√£o **ortogonais** (formam √¢ngulo de 90¬∞)  \n- Juntos formam um novo sistema de coordenadas centrado nos dados  \n\n### Proje√ß√µes dos Pontos no PC1\n\nVamos visualizar como cada ponto √© projetado no primeiro componente principal:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Fun√ß√£o auxiliar para projetar pontos\ndef projetar_ponto(p, v, centro):\n    \"\"\"Projeta o ponto p sobre o vetor v a partir do centro\"\"\"\n    p_rel = p - centro\n    escalar = np.dot(p_rel, v) / np.dot(v, v)\n    return centro + escalar * v\n\nplt.figure(figsize=(7, 7))\n\n# Plotando os pontos originais\nsns.scatterplot(data=notas, x='Estudo_horas', y='Frequencia', \n                alpha=0.7, s=100, color='steelblue')\n\n# Centro e componentes principais\ncentro = notas_simpl.mean().to_numpy()\nU = pca_2d.components_\nescala = 10\n\n# Desenhando PC1\nplt.arrow(centro[0], centro[1], \n          U[0, 0] * escala * 0.6, U[0, 1] * escala * 0.6,\n          head_width=0.3, head_length=0.2, fc='red', ec='red', \n          linewidth=2, alpha=0.8)\n\n# Linha estendida do PC1\nx1, y1 = centro[0] - U[0, 0] * escala, centro[1] - U[0, 1] * escala\nx2, y2 = centro[0] + U[0, 0] * escala, centro[1] + U[0, 1] * escala\nplt.plot([x1, x2], [y1, y2], color='red', linestyle='-', linewidth=2, alpha=0.6,\n         label='PC1 (dire√ß√£o de maior vari√¢ncia)')\n\n# Desenhando as proje√ß√µes de cada ponto no PC1\nfor i in range(len(notas_simpl)):\n    ponto = notas_simpl.iloc[i].to_numpy()\n    ponto_projetado = projetar_ponto(ponto, U[0, :], centro)\n    \n    # Linha de proje√ß√£o\n    plt.plot([ponto[0], ponto_projetado[0]], \n             [ponto[1], ponto_projetado[1]], \n             color='gray', linestyle=':', linewidth=1, alpha=0.5)\n    \n    # Ponto projetado\n    plt.scatter(ponto_projetado[0], ponto_projetado[1], \n               color='red', s=30, alpha=0.6, zorder=3)\n\n# Adicionando r√≥tulos dos alunos para identifica√ß√£o\nfor i, nome in enumerate(notas['Nome']):\n    plt.text(x=notas_simpl['Estudo_horas'][i] + 0.1,\n             y=notas_simpl['Frequencia'][i] + 0.1,\n             s=nome, fontsize=9, alpha=0.7)\n\nplt.title('Proje√ß√£o dos Dados no Primeiro Componente Principal', fontsize=14, pad=20)\nplt.xlabel('Tempo de Estudo (horas)', fontsize=12)\nplt.ylabel('Frequ√™ncia (%)', fontsize=12)\nplt.ylim(29.5, 41.5)\nplt.xlim(3, 17)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üí° O que estamos vendo:**  \n- As linhas pontilhadas mostram como cada ponto √© **projetado** no PC1  \n- Os pontos vermelhos s√£o as proje√ß√µes no PC1  \n- A varia√ß√£o ao longo do PC1 captura a maior parte da variabilidade total  \n- As proje√ß√µes minimizam a dist√¢ncia perpendicular ao PC1  \n\n## Dados Transformados no Espa√ßo PCA\n\nAgora vamos ver como ficam os dados ap√≥s a transforma√ß√£o PCA:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 7))\n\n# Plotando no espa√ßo transformado\nsns.scatterplot(data=notas, x='PCA1', y='PCA2', alpha=0.7, s=100, color='steelblue')\n\n# Linhas de refer√™ncia nos eixos (origem)\nplt.axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\nplt.axvline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n\n# Adicionando nomes dos alunos\nfor i, nome in enumerate(notas['Nome']):\n    plt.text(x=F_2d['PCA1'][i] + 0.1,\n             y=F_2d['PCA2'][i] + 0.1,\n             s=nome, fontsize=9, alpha=0.7)\n\n# PC1 ‚Üí eixo horizontal\nplt.arrow(0, 0, escala, 0,\n          head_width=0.2, head_length=0.3,\n          fc='red', ec='red', linewidth=2, alpha=0.7)\n\n# PC2 ‚Üí eixo vertical\nplt.arrow(0, 0, 0, escala,\n          head_width=0.2, head_length=0.3,\n          fc='red', ec='red', linewidth=2, alpha=0.7)\n\n# R√≥tulos das setas\nplt.text(escala + 0.2, 0.1, \"PC1\", fontsize=12, color='red', fontweight='bold')\nplt.text(0.1, escala + 0.2, \"PC2\", fontsize=12, color='red', fontweight='bold')\n\n\nplt.title('Dados Transformados: Espa√ßo dos Componentes Principais', fontsize=14, pad=20)\nplt.xlabel(f'PC1 ({variancia_explicada[0]*100:.1f}% da vari√¢ncia)', fontsize=12)\nplt.ylabel(f'PC2 ({variancia_explicada[1]*100:.1f}% da vari√¢ncia)', fontsize=12)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Compara√ß√£o: Espa√ßo Original vs Espa√ßo PCA\n\nVamos comparar lado a lado as duas representa√ß√µes:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 1, figsize=(6, 12))\n\n# Gr√°fico 1: Dados Originais\nax1 = axes[0]\nsns.scatterplot(data=notas, x='Estudo_horas', y='Frequencia', \n                alpha=0.7, s=100, ax=ax1, color='steelblue')\nfor i, nome in enumerate(notas['Nome']):\n    ax1.text(x=notas['Estudo_horas'][i] + 0.1,\n             y=notas['Frequencia'][i] + 0.1,\n             s=nome, fontsize=9, alpha=0.7)\nax1.set_title('Espa√ßo Original', fontsize=14, fontweight='bold')\nax1.set_xlabel('Tempo de Estudo (horas)', fontsize=12)\nax1.set_ylabel('Frequ√™ncia (%)', fontsize=12)\nax1.set_ylim(30.5, 40.5)\nax1.set_xlim(4, 15.5)\nax1.grid(True, alpha=0.3)\n\n# Gr√°fico 2: Espa√ßo PCA\nax2 = axes[1]\nsns.scatterplot(data=notas, x='PCA1', y='PCA2', alpha=0.7, s=100, ax=ax2, color='steelblue')\nax2.axhline(0, color='red', linestyle='--', alpha=0.5)\nax2.axvline(0, color='red', linestyle='--', alpha=0.5)\n\nfor i, nome in enumerate(notas['Nome']):\n    ax2.text(x=F_2d['PCA1'][i] + 0.1, \n             y=F_2d['PCA2'][i] + 0.1, \n             s=nome, fontsize=9, alpha=0.7)\n\n# Adicionando setas indicativas dos novos eixos\nescala = 3\nax2.arrow(0, 0, escala, 0,\n          head_width=0.2, head_length=0.3,\n          fc='red', ec='red', linewidth=2, alpha=0.7)\nax2.arrow(0, 0, 0, escala,\n          head_width=0.2, head_length=0.3,\n          fc='red', ec='red', linewidth=2, alpha=0.7)\n\nax2.text(escala + 0.2, 0.1, \"PC1\", fontsize=12, color='red', fontweight='bold')\nax2.text(0.1, escala + 0.2, \"PC2\", fontsize=12, color='red', fontweight='bold')\n\nax2.set_title('Espa√ßo dos Componentes Principais', fontsize=14, fontweight='bold')\nax2.set_xlabel(f'PC1 ({variancia_explicada[0]*100:.1f}% da vari√¢ncia)', fontsize=12)\nax2.set_ylabel(f'PC2 ({variancia_explicada[1]*100:.1f}% da vari√¢ncia)', fontsize=12)\nax2.set_ylim(-6, 8)\nax2.set_xlim(-6, 8)\nax2.grid(True, alpha=0.3)\n\nplt.suptitle('Compara√ß√£o: Transforma√ß√£o PCA', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üéØ Principais Diferen√ßas:**  \n- **Espa√ßo Original**: Eixos representam vari√°veis f√≠sicas (horas, percentual)  \n- **Espa√ßo PCA**: Eixos representam combina√ß√µes lineares que maximizam vari√¢ncia  \n- **Centraliza√ß√£o**: No espa√ßo PCA, os dados est√£o centrados na origem  \n- **Ortogonalidade**: PC1 e PC2 s√£o perpendiculares e n√£o-correlacionados  \n\n# PCA com Tr√™s Vari√°veis\n\n## Incluindo a Nota Final na An√°lise\n\nAgora vamos expandir nossa an√°lise incluindo a vari√°vel **Nota Final**, tornando o problema tridimensional:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Recarregando os dados para uma an√°lise limpa\nnotas = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/Notas.csv')\n\n# Selecionando tr√™s vari√°veis para an√°lise\nvars_pca = ['Estudo_horas', 'Frequencia', 'Nota_final']\nX = notas[vars_pca]\n```\n:::\n\n\n## Padronizando os Dados\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Aplicando padroniza√ß√£o (m√©dia=0, desvio padr√£o=1)\nscaler = StandardScaler()\nX_padronizado = scaler.fit_transform(X)\nX_padronizado_df = pd.DataFrame(X_padronizado, columns=vars_pca)\n\nprint(\"‚úÖ DADOS AP√ìS PADRONIZA√á√ÉO:\")\nprint(\"=\" * 50)\nprint(\"\\nM√©dias (devem ser ~0):\")\nprint(X_padronizado_df.mean())\nprint(\"\\nDesvios Padr√£o (devem ser 1):\")\nprint(X_padronizado_df.std(ddof=0))\n```\n:::\n\n\n### Efeito da Padroniza√ß√£o nas Covari√¢ncias\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nprint(\"üîç COMPARA√á√ÉO: Covari√¢ncia antes e depois da padroniza√ß√£o\")\nprint(\"=\" * 60)\nprint(\"\\nüìä Matriz de Covari√¢ncia ORIGINAL:\")\nprint(X.cov())\nprint(\"\\nüìä Matriz de Covari√¢ncia PADRONIZADA:\")\nprint(X_padronizado_df.cov())\nprint(\"\\nüí° Observe: As correla√ß√µes s√£o alteradas ap√≥s a padroniza√ß√£o!\")\n```\n:::\n\n\n### Efeito da Padroniza√ß√£o nas Correla√ß√µes\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nprint(\"üîç COMPARA√á√ÉO: Correla√ß√µes antes e depois da padroniza√ß√£o\")\nprint(\"=\" * 60)\nprint(\"\\nüìä Matriz de Correla√ß√£o ORIGINAL:\")\nprint(X.corr())\nprint(\"\\nüìä Matriz de Correla√ß√£o PADRONIZADA:\")\nprint(X_padronizado_df.corr())\nprint(\"\\nüí° Observe: As correla√ß√µes s√£o preservadas ap√≥s a padroniza√ß√£o!\")\nprint(\"\\nüí° Observe: A matriz de covari√¢ncia das vari√°veis padronizadas √© igual a matriz de correla√ß√£o das varivaveis originais!\")\n```\n:::\n\n\n## Aplicando PCA aos Dados Tridimensionais\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Aplicando PCA aos dados padronizados\npca_3d = PCA()\nF_3d = pca_3d.fit_transform(X_padronizado)\nF_3d_df = pd.DataFrame(F_3d, columns=[f'PCA{i+1}' for i in range(F_3d.shape[1])])\n\n# Adicionando componentes ao dataframe original\nnotas[['PCA1', 'PCA2', 'PCA3']] = F_3d_df\n\nF_3d_df\n```\n:::\n\n\n### Vari√¢ncia Explicada\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Analisando a vari√¢ncia explicada\nvar_explicada = pca_3d.explained_variance_\nvar_explicada_ratio = pca_3d.explained_variance_ratio_\nvar_acumulada = np.cumsum(var_explicada_ratio)\n\nprint(\"üìä AN√ÅLISE DETALHADA DA VARI√ÇNCIA\")\nprint(\"=\" * 60)\nprint(\"\\nüìà Vari√¢ncia Explicada por Componente:\")\nfor i in range(len(var_explicada_ratio)):\n    print(f\"  PC{i+1}: {var_explicada_ratio[i]*100:6.2f}% \"\n          f\"(acumulada: {var_acumulada[i]*100:6.2f}%)\")\n\nprint(\"\\nüéØ Interpreta√ß√£o:\")\nif var_acumulada[1] > 0.8:\n    print(f\"  ‚úÖ Apenas 2 componentes explicam {var_acumulada[1]*100:.1f}% da vari√¢ncia!\")\n    print(\"     Podemos reduzir de 3 para 2 dimens√µes com pouca perda de informa√ß√£o.\")\nelse:\n    print(f\"  ‚ö†Ô∏è 2 componentes explicam {var_acumulada[1]*100:.1f}% da vari√¢ncia.\")\n    print(\"     Pode ser necess√°rio manter todas as componentes.\")\n```\n:::\n\n\n## Scree Plot: Visualizando a Import√¢ncia dos Componentes\n\nO Scree Plot nos ajuda a decidir quantos componentes principais manter:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Gr√°fico de barras - Vari√¢ncia individual\nplt.figure(figsize=(7, 5))\nplt.bar(range(1, len(var_explicada_ratio) + 1), \n        var_explicada_ratio * 100, \n        alpha=0.7, color='steelblue')\nplt.xticks(range(1, len(var_explicada_ratio) + 1))\nplt.xlabel('Componente Principal', fontsize=12)\nplt.ylabel('Vari√¢ncia Explicada (%)', fontsize=12)\nplt.title('Scree Plot - Vari√¢ncia Individual', fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Adicionar valores no topo das barras\nfor i, v in enumerate(var_explicada_ratio):\n    plt.text(i + 1, v * 100 + 1, f'{v*100:.1f}%', \n             ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Biplot: Visualizando Scores e Loadings\n\nO **biplot** permite visualizar ao mesmo tempo as observa√ß√µes (pontos, ou scores) e os vetores de contribui√ß√£o das vari√°veis originais (loadings), facilitando a interpreta√ß√£o dos resultados da PCA.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 7))\n\n# Plotando os pontos das observa√ß√µes no plano das componentes principais\nsns.scatterplot(data=notas, x='PCA1', y='PCA2', alpha=0.7, s=100)\n\n# Linhas de refer√™ncia nas origens dos eixos\nplt.axhline(0, color='gray', linestyle='--', alpha=0.3)\nplt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n\n# Adicionando nomes dos alunos\nfor i, nome in enumerate(notas['Nome']):\n    plt.text(x=F_3d_df['PCA1'][i] + 0.05, \n             y=F_3d_df['PCA2'][i] + 0.05, \n             s=nome, fontsize=9)\n\n# Calculando os vetores de loading das vari√°veis originais\nloadings = pca_3d.components_.T * np.sqrt(pca_3d.explained_variance_)\nescala_loading = 1.5\n\nfor i, var in enumerate(vars_pca):\n    plt.arrow(0, 0, \n              loadings[i, 0] * escala_loading, \n              loadings[i, 1] * escala_loading,\n              color='red', alpha=0.7, head_width=0.03, \n              head_length=0.08, linewidth=2)\n    plt.text(loadings[i, 0] * escala_loading * 1.25, \n             loadings[i, 1] * escala_loading * 1.25,\n             var, color='red', fontsize=11,\n             ha='center')\n\nplt.title('Biplot: Scores e Loadings do PCA', fontsize=14)\nplt.xlabel(f'PC1 ({var_explicada_ratio[0]*100:.1f}% da vari√¢ncia)', fontsize=12)\nplt.ylabel(f'PC2 ({var_explicada_ratio[1]*100:.1f}% da vari√¢ncia)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n:::\n\n\n**üí° Interpreta√ß√£o:**  \n- Pontos representam as observa√ß√µes projetadas.  \n- Vetores vermelhos mostram como cada vari√°vel contribui para cada componente.  \n- Vetores pr√≥ximos indicam correla√ß√£o entre vari√°veis, vetores perpendiculares indicam independ√™ncia.  \n\n\n# An√°lise de Correla√ß√µes e Covari√¢ncias\n\nVisualizar correla√ß√µes √© fundamental para compreender como o PCA reorganiza os dados.\n\n## Fun√ß√µes Did√°ticas Auxiliares\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndef scatter_com_lowess(x, y, color=None, label=None, frac=0.6, **kws):\n    \"\"\"Gr√°fico de dispers√£o com curva LOWESS suavizada\"\"\"\n    ax = plt.gca()\n    ax.scatter(x, y, alpha=0.7, color=color, s=50)\n    smoothed = lowess(y, x, frac=frac)\n    ax.plot(smoothed[:, 0], smoothed[:, 1], \n            color=\"red\", linewidth=2, alpha=0.8)\n\ndef mostrar_correlacao(x, y, **kws):\n    \"\"\"Calcula e exibe o coeficiente de correla√ß√£o de Pearson\"\"\"\n    r, _ = pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(f\"r = {r:.2f}\", \n                xy=(.5, .5), xycoords=ax.transAxes,\n                ha='center', va='center', \n                fontsize=14, color='red', fontweight='bold',\n                bbox=dict(boxstyle=\"round,pad=0.3\", \n                         facecolor=\"yellow\", alpha=0.3))\n```\n:::\n\n\n## Visualizando a Matriz de Correla√ß√£o\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nprint(\"\\nMatriz de Correla√ß√£o:\")\nprint(notas[vars_pca].corr())\n\nplt.figure(figsize=(12, 10))\ng = sns.PairGrid(notas[vars_pca], diag_sharey=False)\ng.map_lower(scatter_com_lowess, frac=0.7)    # Inferior: gr√°fico + tend√™ncia\ng.map_upper(mostrar_correlacao)              # Superior: Pearson r\ng.map_diag(sns.histplot, kde=True, alpha=0.6, color='steelblue')\nplt.suptitle('Matriz de Correla√ß√µes e Distribui√ß√µes', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üí° An√°lise:**  \n- Rela√ß√µes entre vari√°veis, tend√™ncias, e for√ßa da associa√ß√£o visualizadas.  \n- PCA √© mais eficiente quando h√° correla√ß√µes entre vari√°veis.  \n\n\n# Exemplos Simulados: Estrutura de Correla√ß√£o\n\nSimular dados com diferentes estruturas de correla√ß√£o revela como PCA se comporta em cada caso.\n\n## Simula√ß√£o: Vari√°veis Correlacionadas\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nrng = np.random.default_rng(42)\nmedia = np.array([0, 0, 0, 0])\ncov_correlacionada = np.array([\n    [1.0, 0.9, 0.1, 0.1],\n    [0.9, 1.0, 0.1, 0.1],\n    [0.1, 0.1, 1.0, 0.8],\n    [0.1, 0.1, 0.8, 1.0]\n])\nn_amostras = 200\ndados_correlacionados = rng.multivariate_normal(media, cov_correlacionada, size=n_amostras)\nX_corr = pd.DataFrame(dados_correlacionados, columns=[f'X{i+1}' for i in range(4)])\n\nplt.figure(figsize=(7, 7))\ng = sns.PairGrid(X_corr, diag_sharey=False)\ng.map_lower(scatter_com_lowess, frac=0.7)\ng.map_upper(mostrar_correlacao)\ng.map_diag(sns.histplot, kde=True, alpha=0.6, color='steelblue')\nplt.suptitle('Matriz de Correla√ß√µes e Distribui√ß√µes', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Simula√ß√£o: Vari√°veis Independentes\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ncov_independente = np.eye(4)\ndados_independentes = rng.multivariate_normal(media, cov_independente, size=n_amostras)\nX_indep = pd.DataFrame(dados_independentes, columns=[f'X{i+1}' for i in range(4)])\n\nplt.figure(figsize=(7, 7))\ng = sns.PairGrid(X_indep, diag_sharey=False)\ng.map_lower(scatter_com_lowess, frac=0.7)\ng.map_upper(mostrar_correlacao)\ng.map_diag(sns.histplot, kde=True, alpha=0.6, color='steelblue')\nplt.suptitle('Matriz de Correla√ß√µes e Distribui√ß√µes', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Comparando PCAs nas Situa√ß√µes Simuladas\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 2, figsize=(7, 7))\n\nfor idx, (X_sim, titulo) in enumerate([(X_corr, 'Vari√°veis Correlacionadas'), \n                                       (X_indep, 'Vari√°veis Independentes')]):\n    X_sim_scaled = StandardScaler().fit_transform(X_sim)\n    pca_sim = PCA()\n    F_sim = pca_sim.fit_transform(X_sim_scaled)\n    F_sim_df = pd.DataFrame(F_sim, columns=[f'PC{i+1}' for i in range(4)])\n    var_exp_sim = pca_sim.explained_variance_ratio_\n\n    ax = axes[idx, 0]\n    ax.bar(range(1, 5), var_exp_sim * 100, alpha=0.7, color='steelblue')\n    ax.set_xlabel('Componente Principal')\n    ax.set_ylabel('Vari√¢ncia Explicada (%)')\n    ax.set_title(f'{titulo}\\nScree Plot')\n    ax.set_xticks(range(1, 5))\n    ax.grid(True, alpha=0.3)\n    for i, v in enumerate(var_exp_sim):\n        ax.text(i + 1, v * 100 + 1, f'{v*100:.1f}%', ha='center', fontsize=9)\n\n    ax = axes[idx, 1]\n    ax.scatter(F_sim_df['PC1'], F_sim_df['PC2'], alpha=0.5, s=30)\n    ax.axhline(0, color='gray', linestyle='--', alpha=0.3)\n    ax.axvline(0, color='gray', linestyle='--', alpha=0.3)\n    ax.set_xlabel(f'PC1 ({var_exp_sim[0]*100:.1f}%)')\n    ax.set_ylabel(f'PC2 ({var_exp_sim[1]*100:.1f}%)')\n    ax.set_title(f'{titulo}\\nPC1 vs PC2')\n    ax.grid(True, alpha=0.3)\n\n    loadings_sim = pca_sim.components_.T * np.sqrt(pca_sim.explained_variance_)\n    for i, var in enumerate(X_sim.columns):\n        ax.arrow(0, 0, loadings_sim[i, 0]*2, loadings_sim[i, 1]*2,\n                color='red', alpha=0.7, head_width=0.1, linewidth=1.5)\n        ax.text(loadings_sim[i, 0]*2.2, loadings_sim[i, 1]*2.2,\n               var, color='red', fontsize=10, fontweight='bold')\n\nplt.suptitle('Compara√ß√£o: Efeito da Estrutura de Correla√ß√£o no PCA', \n             fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**üí° Interpreta√ß√£o dos Resultados:**  \n- Na presen√ßa de correla√ß√µes, poucas componentes explicam a maior parte da vari√¢ncia.  \n- Com vari√°veis independentes, a vari√¢ncia √© distribu√≠da mais uniformemente.  \n\n",
    "supporting": [
      "pca_python_REVISADO_files"
    ],
    "filters": []
  }
}