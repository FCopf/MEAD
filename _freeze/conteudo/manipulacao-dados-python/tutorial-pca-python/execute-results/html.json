{
  "hash": "b901b5dc62130460a23c5c3cdcd5259c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tutorial Completo: Análise de Componentes Principais (PCA) com Python\"\nauthor: \"Tutorial para Ciências de Dados\"\ndate: today\nformat: \n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\n  pdf:\n    toc: true\njupyter: python3\n---\n\n\n# Introdução à Análise de Componentes Principais (PCA)\n\nA **Análise de Componentes Principais (PCA)** é uma técnica fundamental de redução de dimensionalidade e análise exploratória de dados. Seu objetivo principal é transformar um conjunto de variáveis possivelmente correlacionadas em um conjunto menor de variáveis não correlacionadas, chamadas de **componentes principais**.\n\nA PCA funciona encontrando as direções (componentes) que explicam a maior variabilidade nos dados. O primeiro componente principal captura a maior variância, o segundo captura a segunda maior variância (ortogonal ao primeiro), e assim por diante.\n\n## Conceitos Fundamentais\n\n- **Autovalores (eigenvalues):** Indicam a quantidade de variância explicada por cada componente principal\n- **Autovetores (eigenvectors):** Definem a direção de cada componente principal\n- **Variância explicada:** Proporção da variância total dos dados capturada por cada componente\n\nVamos explorar esses conceitos através de exemplos práticos!\n\n::: {#5f8e4d9b .cell execution_count=1}\n``` {.python .cell-code}\n# Importação das bibliotecas necessárias\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurações para os gráficos\nplt.style.use('default')\nsns.set_palette(\"husl\")\n```\n:::\n\n\n# Exemplo 1: Intuição Básica da PCA com Duas Variáveis\n\nVamos começar com um exemplo simples usando duas variáveis altamente correlacionadas. Isso nos permitirá visualizar claramente como a PCA funciona.\n\n::: {#3989603e .cell execution_count=2}\n``` {.python .cell-code}\n# Configuração da semente para reprodutibilidade\nnp.random.seed(42)\n\n# Gerando dados correlacionados\nn_samples = 200\nx1 = np.random.normal(0, 1, n_samples)\n# x2 é altamente correlacionado com x1, mas com algum ruído\nx2 = 0.8 * x1 + np.random.normal(0, 0.3, n_samples)\n\n# Criando DataFrame\ndata_2d = pd.DataFrame({\n    'X1': x1,\n    'X2': x2\n})\n\nprint(\"Correlação entre X1 e X2:\", np.corrcoef(x1, x2)[0,1].round(3))\nprint(\"Estatísticas descritivas:\")\nprint(data_2d.describe().round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelação entre X1 e X2: 0.934\nEstatísticas descritivas:\n            X1       X2\ncount  200.000  200.000\nmean    -0.041   -0.007\nstd      0.931    0.827\nmin     -2.620   -2.390\n25%     -0.705   -0.538\n50%     -0.004    0.058\n75%      0.501    0.495\nmax      2.720    2.215\n```\n:::\n:::\n\n\n::: {#e4a0529e .cell execution_count=3}\n``` {.python .cell-code}\n# Visualizando os dados originais\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Gráfico de dispersão\naxes[0].scatter(data_2d['X1'], data_2d['X2'], alpha=0.6)\naxes[0].set_xlabel('X1')\naxes[0].set_ylabel('X2')\naxes[0].set_title('Dados Originais (X1 vs X2)')\naxes[0].grid(True, alpha=0.3)\n\n# Matriz de correlação\nsns.heatmap(data_2d.corr(), annot=True, cmap='coolwarm', center=0, ax=axes[1])\naxes[1].set_title('Matriz de Correlação')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-4-output-1.png){width=1169 height=489}\n:::\n:::\n\n\nAgora vamos aplicar a PCA e examinar os componentes principais:\n\n::: {#2bd90829 .cell execution_count=4}\n``` {.python .cell-code}\n# Aplicando PCA\npca_2d = PCA()\ndata_2d_scaled = StandardScaler().fit_transform(data_2d)\npca_components = pca_2d.fit_transform(data_2d_scaled)\n\n# Resultados da PCA\nprint(\"=== RESULTADOS DA PCA ===\")\nprint(f\"Variância explicada por componente: {pca_2d.explained_variance_ratio_.round(3)}\")\nprint(f\"Variância explicada acumulada: {pca_2d.explained_variance_ratio_.cumsum().round(3)}\")\nprint(f\"Autovalores: {pca_2d.explained_variance_.round(3)}\")\n\nprint(\"\\nAutovetores (Componentes Principais):\")\ncomponents_df = pd.DataFrame(\n    pca_2d.components_.T, \n    columns=[f'PC{i+1}' for i in range(len(pca_2d.components_))],\n    index=['X1', 'X2']\n)\nprint(components_df.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== RESULTADOS DA PCA ===\nVariância explicada por componente: [0.967 0.033]\nVariância explicada acumulada: [0.967 1.   ]\nAutovalores: [1.944 0.066]\n\nAutovetores (Componentes Principais):\n      PC1    PC2\nX1  0.707 -0.707\nX2  0.707  0.707\n```\n:::\n:::\n\n\n::: {#4b4e675f .cell execution_count=5}\n``` {.python .cell-code}\n# Visualizando a transformação PCA\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Dados transformados\naxes[0].scatter(pca_components[:, 0], pca_components[:, 1], alpha=0.6)\naxes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} da variância)')\naxes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} da variância)')\naxes[0].set_title('Dados Transformados (Espaço PCA)')\naxes[0].grid(True, alpha=0.3)\n\n# Comparação das variâncias\ncomponents = ['PC1', 'PC2']\nvariances = pca_2d.explained_variance_ratio_\naxes[1].bar(components, variances, alpha=0.7)\naxes[1].set_ylabel('Proporção da Variância Explicada')\naxes[1].set_title('Variância Explicada por Componente')\naxes[1].set_ylim(0, 1)\n\n# Adicionando valores no gráfico de barras\nfor i, v in enumerate(variances):\n    axes[1].text(i, v + 0.02, f'{v:.1%}', ha='center')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-6-output-1.png){width=1190 height=489}\n:::\n:::\n\n\n**Interpretação:** O primeiro componente principal (PC1) captura a maior parte da variância (~85%), indicando que existe uma forte correlação linear entre X1 e X2. O segundo componente (PC2) captura a variância restante.\n\n# Exemplo 2: PCA com Quatro Variáveis Correlacionadas\n\nAgora vamos simular um cenário mais complexo com quatro variáveis, onde X1 se correlaciona fortemente com X3, e X2 se correlaciona fortemente com X4.\n\n::: {#fb0c2446 .cell execution_count=6}\n``` {.python .cell-code}\n# Simulando dados com padrão específico de correlação\nnp.random.seed(123)\nn_samples = 300\n\n# Variáveis base\nz1 = np.random.normal(0, 1, n_samples)\nz2 = np.random.normal(0, 1, n_samples)\n\n# X1 e X3 correlacionados através de z1\nx1 = z1 + np.random.normal(0, 0.2, n_samples)\nx3 = z1 + np.random.normal(0, 0.2, n_samples)\n\n# X2 e X4 correlacionados através de z2\nx2 = z2 + np.random.normal(0, 0.2, n_samples)\nx4 = z2 + np.random.normal(0, 0.2, n_samples)\n\n# Criando DataFrame\ndata_4d_corr = pd.DataFrame({\n    'X1': x1,\n    'X2': x2,\n    'X3': x3,\n    'X4': x4\n})\n\nprint(\"Matriz de Correlação:\")\nprint(data_4d_corr.corr().round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatriz de Correlação:\n       X1     X2     X3     X4\nX1  1.000 -0.081  0.963 -0.112\nX2 -0.081  1.000 -0.094  0.966\nX3  0.963 -0.094  1.000 -0.119\nX4 -0.112  0.966 -0.119  1.000\n```\n:::\n:::\n\n\n::: {#150706ec .cell execution_count=7}\n``` {.python .cell-code}\n# Gráfico de dispersão par a par (pairplot)\nplt.figure(figsize=(10, 8))\npairplot = sns.pairplot(data_4d_corr, diag_kind='hist', plot_kws={'alpha': 0.6})\npairplot.fig.suptitle('Gráfico de Dispersão Par a Par - Dados Correlacionados', y=1.02)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 1000x800 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-8-output-2.png){width=986 height=1023}\n:::\n:::\n\n\n::: {#09161e3a .cell execution_count=8}\n``` {.python .cell-code}\n# Aplicando PCA nos dados correlacionados\nscaler_4d = StandardScaler()\ndata_4d_corr_scaled = scaler_4d.fit_transform(data_4d_corr)\n\npca_4d_corr = PCA()\npca_4d_corr_transformed = pca_4d_corr.fit_transform(data_4d_corr_scaled)\n\nprint(\"=== RESULTADOS PCA - DADOS CORRELACIONADOS ===\")\nprint(f\"Variância explicada por componente: {pca_4d_corr.explained_variance_ratio_.round(3)}\")\nprint(f\"Variância explicada acumulada: {pca_4d_corr.explained_variance_ratio_.cumsum().round(3)}\")\n\nprint(\"\\nAutovetores (Componentes Principais):\")\ncomponents_4d_df = pd.DataFrame(\n    pca_4d_corr.components_.T, \n    columns=[f'PC{i+1}' for i in range(len(pca_4d_corr.components_))],\n    index=['X1', 'X2', 'X3', 'X4']\n)\nprint(components_4d_df.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== RESULTADOS PCA - DADOS CORRELACIONADOS ===\nVariância explicada por componente: [0.542 0.44  0.01  0.008]\nVariância explicada acumulada: [0.542 0.982 0.992 1.   ]\n\nAutovetores (Componentes Principais):\n      PC1    PC2    PC3    PC4\nX1 -0.496  0.505 -0.622 -0.336\nX2  0.495  0.506 -0.324  0.627\nX3 -0.500  0.499  0.632  0.319\nX4  0.508  0.490  0.331 -0.626\n```\n:::\n:::\n\n\n::: {#b1e187ae .cell execution_count=9}\n``` {.python .cell-code}\n# Visualizações dos resultados\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Scree plot\ncomponents_names = [f'PC{i+1}' for i in range(len(pca_4d_corr.explained_variance_ratio_))]\naxes[0,0].plot(components_names, pca_4d_corr.explained_variance_ratio_, 'bo-', linewidth=2)\naxes[0,0].set_ylabel('Proporção da Variância Explicada')\naxes[0,0].set_title('Scree Plot')\naxes[0,0].grid(True, alpha=0.3)\n\n# 2. Variância explicada acumulada\naxes[0,1].plot(components_names, pca_4d_corr.explained_variance_ratio_.cumsum(), 'ro-', linewidth=2)\naxes[0,1].axhline(y=0.8, color='k', linestyle='--', alpha=0.5)\naxes[0,1].set_ylabel('Variância Explicada Acumulada')\naxes[0,1].set_title('Variância Explicada Acumulada')\naxes[0,1].grid(True, alpha=0.3)\n\n# 3. PC1 vs PC2\nscatter = axes[1,0].scatter(pca_4d_corr_transformed[:, 0], pca_4d_corr_transformed[:, 1], alpha=0.6)\naxes[1,0].set_xlabel(f'PC1 ({pca_4d_corr.explained_variance_ratio_[0]:.1%})')\naxes[1,0].set_ylabel(f'PC2 ({pca_4d_corr.explained_variance_ratio_[1]:.1%})')\naxes[1,0].set_title('Primeiros Dois Componentes Principais')\naxes[1,0].grid(True, alpha=0.3)\n\n# 4. Heatmap dos componentes\nsns.heatmap(components_4d_df.T, annot=True, cmap='RdBu_r', center=0, ax=axes[1,1])\naxes[1,1].set_title('Composição dos Componentes Principais')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-10-output-1.png){width=1390 height=989}\n:::\n:::\n\n\n**Interpretação:** Como esperado, os dois primeiros componentes capturam a maior parte da variância (~80%). PC1 combina principalmente X1 e X3, enquanto PC2 combina X2 e X4, refletindo a estrutura de correlação dos dados.\n\n# Exemplo 3: PCA com Variáveis Não Correlacionadas\n\nAgora vamos ver o que acontece quando aplicamos PCA em dados onde as variáveis são independentes entre si.\n\n::: {#53d666b7 .cell execution_count=10}\n``` {.python .cell-code}\n# Simulando variáveis independentes\nnp.random.seed(456)\nn_samples = 300\n\n# Quatro variáveis independentes\nx1_indep = np.random.normal(0, 1, n_samples)\nx2_indep = np.random.normal(0, 2, n_samples)  # Variância diferente\nx3_indep = np.random.normal(0, 0.5, n_samples)  # Variância menor\nx4_indep = np.random.normal(0, 1.5, n_samples)\n\ndata_4d_indep = pd.DataFrame({\n    'X1': x1_indep,\n    'X2': x2_indep,\n    'X3': x3_indep,\n    'X4': x4_indep\n})\n\nprint(\"Matriz de Correlação (dados independentes):\")\nprint(data_4d_indep.corr().round(3))\nprint(\"\\nVariâncias originais:\")\nprint(data_4d_indep.var().round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatriz de Correlação (dados independentes):\n       X1     X2     X3     X4\nX1  1.000 -0.034 -0.039  0.041\nX2 -0.034  1.000 -0.015  0.130\nX3 -0.039 -0.015  1.000 -0.086\nX4  0.041  0.130 -0.086  1.000\n\nVariâncias originais:\nX1    1.023\nX2    4.174\nX3    0.227\nX4    1.989\ndtype: float64\n```\n:::\n:::\n\n\n::: {#dda9e350 .cell execution_count=11}\n``` {.python .cell-code}\n# Pairplot dos dados independentes\nplt.figure(figsize=(10, 8))\npairplot_indep = sns.pairplot(data_4d_indep, diag_kind='hist', plot_kws={'alpha': 0.6})\npairplot_indep.fig.suptitle('Gráfico de Dispersão Par a Par - Dados Independentes', y=1.02)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 1000x800 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-12-output-2.png){width=986 height=1023}\n:::\n:::\n\n\n::: {#4c256e9e .cell execution_count=12}\n``` {.python .cell-code}\n# Aplicando PCA nos dados independentes\ndata_4d_indep_scaled = StandardScaler().fit_transform(data_4d_indep)\npca_4d_indep = PCA()\npca_4d_indep_transformed = pca_4d_indep.fit_transform(data_4d_indep_scaled)\n\nprint(\"=== RESULTADOS PCA - DADOS INDEPENDENTES ===\")\nprint(f\"Variância explicada por componente: {pca_4d_indep.explained_variance_ratio_.round(3)}\")\nprint(f\"Variância explicada acumulada: {pca_4d_indep.explained_variance_ratio_.cumsum().round(3)}\")\n\nprint(\"\\nAutovetores (Componentes Principais):\")\ncomponents_4d_indep_df = pd.DataFrame(\n    pca_4d_indep.components_.T, \n    columns=[f'PC{i+1}' for i in range(len(pca_4d_indep.components_))],\n    index=['X1', 'X2', 'X3', 'X4']\n)\nprint(components_4d_indep_df.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== RESULTADOS PCA - DADOS INDEPENDENTES ===\nVariância explicada por componente: [0.292 0.261 0.237 0.21 ]\nVariância explicada acumulada: [0.292 0.553 0.79  1.   ]\n\nAutovetores (Componentes Principais):\n      PC1    PC2    PC3    PC4\nX1  0.162  0.742  0.600 -0.252\nX2  0.546 -0.522  0.244 -0.608\nX3 -0.442 -0.419  0.748  0.263\nX4  0.693 -0.029  0.145  0.706\n```\n:::\n:::\n\n\n::: {#e3587ef9 .cell execution_count=13}\n``` {.python .cell-code}\n# Comparando resultados: correlacionados vs independentes\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Comparação da variância explicada\ncomponents_names = ['PC1', 'PC2', 'PC3', 'PC4']\n\naxes[0,0].bar([x - 0.2 for x in range(4)], pca_4d_corr.explained_variance_ratio_, \n              width=0.4, label='Correlacionados', alpha=0.7)\naxes[0,0].bar([x + 0.2 for x in range(4)], pca_4d_indep.explained_variance_ratio_, \n              width=0.4, label='Independentes', alpha=0.7)\naxes[0,0].set_xticks(range(4))\naxes[0,0].set_xticklabels(components_names)\naxes[0,0].set_ylabel('Proporção da Variância Explicada')\naxes[0,0].set_title('Comparação: Variância Explicada')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# Scree plot para dados independentes\naxes[0,1].plot(components_names, pca_4d_indep.explained_variance_ratio_, 'go-', linewidth=2)\naxes[0,1].set_ylabel('Proporção da Variância Explicada')\naxes[0,1].set_title('Scree Plot - Dados Independentes')\naxes[0,1].grid(True, alpha=0.3)\n\n# PC1 vs PC2 para dados independentes\naxes[1,0].scatter(pca_4d_indep_transformed[:, 0], pca_4d_indep_transformed[:, 1], alpha=0.6, color='green')\naxes[1,0].set_xlabel(f'PC1 ({pca_4d_indep.explained_variance_ratio_[0]:.1%})')\naxes[1,0].set_ylabel(f'PC2 ({pca_4d_indep.explained_variance_ratio_[1]:.1%})')\naxes[1,0].set_title('PC1 vs PC2 - Dados Independentes')\naxes[1,0].grid(True, alpha=0.3)\n\n# Heatmap dos componentes independentes\nsns.heatmap(components_4d_indep_df.T, annot=True, cmap='RdBu_r', center=0, ax=axes[1,1])\naxes[1,1].set_title('Componentes - Dados Independentes')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-14-output-1.png){width=1390 height=989}\n:::\n:::\n\n\n**Interpretação:** Com variáveis independentes, a PCA não consegue reduzir significativamente a dimensionalidade. Cada componente explica uma proporção similar da variância (~25%), indicando que não há redundância nos dados.\n\n# Exemplo 4: PCA com Dataset Real - Iris\n\nFinalmente, vamos aplicar a PCA no famoso dataset Iris para demonstrar seu uso em dados reais.\n\n::: {#f725c2e9 .cell execution_count=14}\n``` {.python .cell-code}\n# Carregando o dataset Iris\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target\niris_df['species_name'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\nprint(\"Dataset Iris - Primeiras 5 linhas:\")\nprint(iris_df.head())\nprint(f\"\\nShape do dataset: {iris_df.shape}\")\nprint(f\"\\nNomes das características: {list(iris.feature_names)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset Iris - Primeiras 5 linhas:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n   species species_name  \n0        0       setosa  \n1        0       setosa  \n2        0       setosa  \n3        0       setosa  \n4        0       setosa  \n\nShape do dataset: (150, 6)\n\nNomes das características: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n```\n:::\n:::\n\n\n::: {#244eb332 .cell execution_count=15}\n``` {.python .cell-code}\n# Análise exploratória do dataset Iris\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Estatísticas descritivas por espécie\nprint(\"Estatísticas descritivas por espécie:\")\nprint(iris_df.groupby('species_name')[iris.feature_names].mean().round(2))\n\n# Matriz de correlação\nsns.heatmap(iris_df[iris.feature_names].corr(), annot=True, cmap='coolwarm', center=0, ax=axes[0,0])\naxes[0,0].set_title('Matriz de Correlação - Iris')\n\n# Distribuições das características\niris_df[iris.feature_names].hist(bins=20, ax=axes[0,1], alpha=0.7)\naxes[0,1].set_title('Distribuições das Características')\n\n# Boxplot por espécie\niris_melted = iris_df.melt(id_vars=['species_name'], value_vars=iris.feature_names)\nsns.boxplot(data=iris_melted, x='variable', y='value', hue='species_name', ax=axes[1,0])\naxes[1,0].set_title('Boxplot por Espécie')\naxes[1,0].tick_params(axis='x', rotation=45)\n\n# Pairplot de duas características importantes\nsns.scatterplot(data=iris_df, x='sepal length (cm)', y='petal length (cm)', \n                hue='species_name', ax=axes[1,1])\naxes[1,1].set_title('Sepal Length vs Petal Length')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstatísticas descritivas por espécie:\n              sepal length (cm)  sepal width (cm)  petal length (cm)  \\\nspecies_name                                                           \nsetosa                     5.01              3.43               1.46   \nversicolor                 5.94              2.77               4.26   \nvirginica                  6.59              2.97               5.55   \n\n              petal width (cm)  \nspecies_name                    \nsetosa                    0.25  \nversicolor                1.33  \nvirginica                 2.03  \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-16-output-2.png){width=1388 height=988}\n:::\n:::\n\n\n::: {#eac39d7c .cell execution_count=16}\n``` {.python .cell-code}\n# Aplicando PCA no dataset Iris\n# Separando as características (features) das classes\nX_iris = iris_df[iris.feature_names]\ny_iris = iris_df['species_name']\n\n# Padronizando os dados\nscaler_iris = StandardScaler()\nX_iris_scaled = scaler_iris.fit_transform(X_iris)\n\n# Aplicando PCA\npca_iris = PCA()\nX_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n\nprint(\"=== RESULTADOS PCA - DATASET IRIS ===\")\nprint(f\"Variância explicada por componente: {pca_iris.explained_variance_ratio_.round(3)}\")\nprint(f\"Variância explicada acumulada: {pca_iris.explained_variance_ratio_.cumsum().round(3)}\")\n\n# Quantos componentes para explicar 95% da variância?\ncumsum_var = pca_iris.explained_variance_ratio_.cumsum()\nn_components_95 = np.argmax(cumsum_var >= 0.95) + 1\nprint(f\"\\nComponentes necessários para 95% da variância: {n_components_95}\")\n\nprint(\"\\nAutovetores (Componentes Principais):\")\ncomponents_iris_df = pd.DataFrame(\n    pca_iris.components_.T, \n    columns=[f'PC{i+1}' for i in range(len(pca_iris.components_))],\n    index=iris.feature_names\n)\nprint(components_iris_df.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== RESULTADOS PCA - DATASET IRIS ===\nVariância explicada por componente: [0.73  0.229 0.037 0.005]\nVariância explicada acumulada: [0.73  0.958 0.995 1.   ]\n\nComponentes necessários para 95% da variância: 2\n\nAutovetores (Componentes Principais):\n                     PC1    PC2    PC3    PC4\nsepal length (cm)  0.521  0.377  0.720 -0.261\nsepal width (cm)  -0.269  0.923 -0.244  0.124\npetal length (cm)  0.580  0.024 -0.142  0.801\npetal width (cm)   0.565  0.067 -0.634 -0.524\n```\n:::\n:::\n\n\n::: {#030a6636 .cell execution_count=17}\n``` {.python .cell-code}\n# Visualizações do PCA no Iris\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Scree Plot\ncomponents_names = [f'PC{i+1}' for i in range(len(pca_iris.explained_variance_ratio_))]\naxes[0,0].plot(components_names, pca_iris.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\naxes[0,0].set_ylabel('Proporção da Variância Explicada')\naxes[0,0].set_title('Scree Plot - Iris Dataset')\naxes[0,0].grid(True, alpha=0.3)\n\n# 2. Variância explicada acumulada\naxes[0,1].plot(components_names, pca_iris.explained_variance_ratio_.cumsum(), 'ro-', linewidth=2, markersize=8)\naxes[0,1].axhline(y=0.95, color='k', linestyle='--', alpha=0.7, label='95%')\naxes[0,1].set_ylabel('Variância Explicada Acumulada')\naxes[0,1].set_title('Variância Explicada Acumulada')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# 3. Contribuições das variáveis originais\nsns.heatmap(components_iris_df[['PC1', 'PC2']].T, annot=True, cmap='RdBu_r', center=0, ax=axes[0,2])\naxes[0,2].set_title('Contribuições para PC1 e PC2')\n\n# 4. PC1 vs PC2 colorido por espécie\ncolors = ['red', 'green', 'blue']\nspecies_names = ['setosa', 'versicolor', 'virginica']\nfor i, (species, color) in enumerate(zip(species_names, colors)):\n    mask = y_iris == species\n    axes[1,0].scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1], \n                     c=color, label=species, alpha=0.7, s=50)\n\naxes[1,0].set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%})')\naxes[1,0].set_ylabel(f'PC2 ({pca_iris.explained_variance_ratio_[1]:.1%})')\naxes[1,0].set_title('PCA - PC1 vs PC2 por Espécie')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# 5. PC1 vs PC3\nfor i, (species, color) in enumerate(zip(species_names, colors)):\n    mask = y_iris == species\n    axes[1,1].scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 2], \n                     c=color, label=species, alpha=0.7, s=50)\n\naxes[1,1].set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%})')\naxes[1,1].set_ylabel(f'PC3 ({pca_iris.explained_variance_ratio_[2]:.1%})')\naxes[1,1].set_title('PCA - PC1 vs PC3 por Espécie')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\n# 6. Biplot (PC1 vs PC2 com vetores das variáveis originais)\n# Plotando os pontos\nfor i, (species, color) in enumerate(zip(species_names, colors)):\n    mask = y_iris == species\n    axes[1,2].scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1], \n                     c=color, label=species, alpha=0.6, s=30)\n\n# Adicionando vetores das variáveis\nfeature_vectors = pca_iris.components_[:2].T * np.sqrt(pca_iris.explained_variance_[:2])\nfor i, (feature, vector) in enumerate(zip(iris.feature_names, feature_vectors)):\n    axes[1,2].arrow(0, 0, vector[0]*3, vector[1]*3, head_width=0.1, \n                   head_length=0.1, fc='black', ec='black', alpha=0.8)\n    axes[1,2].text(vector[0]*3.2, vector[1]*3.2, feature.replace(' (cm)', ''), \n                  fontsize=9, ha='center', va='center')\n\naxes[1,2].set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%})')\naxes[1,2].set_ylabel(f'PC2 ({pca_iris.explained_variance_ratio_[1]:.1%})')\naxes[1,2].set_title('Biplot - PC1 vs PC2')\naxes[1,2].legend()\naxes[1,2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](tutorial-pca-python_files/figure-html/cell-18-output-1.png){width=1783 height=1189}\n:::\n:::\n\n\n::: {#35efd453 .cell execution_count=18}\n``` {.python .cell-code}\n# Análise detalhada dos componentes principais no Iris\nprint(\"=== INTERPRETAÇÃO DOS COMPONENTES PRINCIPAIS - IRIS ===\")\nprint(\"\\nPC1 - Primeiro Componente Principal:\")\npc1_loadings = components_iris_df['PC1'].abs().sort_values(ascending=False)\nprint(\"Características mais importantes (em módulo):\")\nfor feature, loading in pc1_loadings.items():\n    print(f\"  {feature}: {components_iris_df.loc[feature, 'PC1']:.3f}\")\n\nprint(f\"\\nPC1 explica {pca_iris.explained_variance_ratio_[0]:.1%} da variância total\")\nprint(\"Interpretação: PC1 parece capturar o 'tamanho geral' da flor\")\n\nprint(\"\\nPC2 - Segundo Componente Principal:\")\npc2_loadings = components_iris_df['PC2'].abs().sort_values(ascending=False)\nprint(\"Características mais importantes (em módulo):\")\nfor feature, loading in pc2_loadings.items():\n    print(f\"  {feature}: {components_iris_df.loc[feature, 'PC2']:.3f}\")\n\nprint(f\"\\nPC2 explica {pca_iris.explained_variance_ratio_[1]:.1%} da variância total\")\nprint(\"Interpretação: PC2 contrasta sepal width com as outras características\")\n\n# Comparação antes e depois da PCA\nprint(f\"\\nDimensionalidade:\")\nprint(f\"Original: {X_iris.shape[1]} dimensões\")\nprint(f\"Com 95% da variância: {n_components_95} dimensões\")\nprint(f\"Redução: {(1 - n_components_95/X_iris.shape[1]):.1%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== INTERPRETAÇÃO DOS COMPONENTES PRINCIPAIS - IRIS ===\n\nPC1 - Primeiro Componente Principal:\nCaracterísticas mais importantes (em módulo):\n  petal length (cm): 0.580\n  petal width (cm): 0.565\n  sepal length (cm): 0.521\n  sepal width (cm): -0.269\n\nPC1 explica 73.0% da variância total\nInterpretação: PC1 parece capturar o 'tamanho geral' da flor\n\nPC2 - Segundo Componente Principal:\nCaracterísticas mais importantes (em módulo):\n  sepal width (cm): 0.923\n  sepal length (cm): 0.377\n  petal width (cm): 0.067\n  petal length (cm): 0.024\n\nPC2 explica 22.9% da variância total\nInterpretação: PC2 contrasta sepal width com as outras características\n\nDimensionalidade:\nOriginal: 4 dimensões\nCom 95% da variância: 2 dimensões\nRedução: 50.0%\n```\n:::\n:::\n\n\n**Interpretação Final:** No dataset Iris, a PCA nos permite reduzir de 4 para 2 dimensões mantendo ~95% da variância original. O primeiro componente captura principalmente o tamanho geral das flores, enquanto o segundo foca na largura das sépalas. A separação das espécies é bem preservada no espaço de menor dimensionalidade.\n\n# Resumo e Considerações Finais\n\n## Principais Insights dos Exemplos\n\n1. **Exemplo 1 (2D correlacionado):** A PCA é muito eficaz quando há correlação forte entre variáveis, concentrando a informação no primeiro componente.\n\n2. **Exemplo 2 (4D com padrão):** Quando existem grupos de variáveis correlacionadas, a PCA identifica esses padrões e os separa em componentes distintos.\n\n3. **Exemplo 3 (4D independente):** Com variáveis independentes, a PCA oferece pouco benefício para redução de dimensionalidade.\n\n4. **Exemplo 4 (Iris real):** Em dados reais, a PCA frequentemente permite reduções significativas mantendo a maior parte da informação.\n\n## Quando Usar PCA\n\n✅ **Recomendado quando:**\n- Você tem muitas variáveis correlacionadas\n- Deseja reduzir dimensionalidade para visualização\n- Precisa remover multicolinearidade\n- Quer acelerar algoritmos de machine learning\n- Busca identificar padrões latentes nos dados\n\n❌ **Cuidado quando:**\n- As variáveis são independentes\n- A interpretabilidade é crítica\n- Você tem poucas variáveis\n- Os dados contêm muitos outliers\n\n## Passos Práticos para Aplicar PCA\n\n1. **Padronize os dados** (especialmente importante se as variáveis têm escalas diferentes)\n2. **Calcule a matriz de correlação** para entender as relações entre variáveis\n3. **Aplique a PCA** e examine a variância explicada\n4. **Escolha o número de componentes** baseado na variância explicada desejada (ex: 80-95%)\n5. **Interprete os componentes** analisando os autovetores\n6. **Valide os resultados** verificando se fazem sentido no contexto do problema\n\n## Limitações da PCA\n\n- **Linearidade:** PCA assume relações lineares entre variáveis\n- **Interpretabilidade:** Componentes principais são combinações das variáveis originais\n- **Outliers:** Podem ter impacto significativo nos resultados\n- **Padronização:** Necessária quando variáveis têm escalas diferentes\n\nA PCA é uma ferramenta poderosa e versátil para análise de dados, especialmente útil em datasets de alta dimensionalidade. Dominar sua aplicação e interpretação é fundamental para qualquer cientista de dados!\n\n",
    "supporting": [
      "tutorial-pca-python_files"
    ],
    "filters": [],
    "includes": {}
  }
}