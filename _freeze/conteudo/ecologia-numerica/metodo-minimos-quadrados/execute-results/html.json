{
  "hash": "d767e9939b939dc5513d517f1e89a821",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Método dos Mínimos Quadrados na Regressão Linear Simples\"\nsubtitle: \"Representação Vetorial e Matricial\"\ninstitute: \"9833 - BASES DA MATEMÁTICA E ESTATÍSTICA PARA CIÊNCIAS DO MAR\"\nauthor: \t\"Fabio Cop Ferreira e William Remo Pedroso Conti\"\nimage: \"images/metodo-minimos-quadrados.png\"\ndescription: Método dos mínimos quadrados na Regressão linear simples por meio da representação vetorial e matricial.\nformat:\n  revealjs: \n    slide-number: true\n    chalkboard: \n      buttons: false\n    preview-links: auto\n    logo: images/metodo-minimos-quadrados.png \n    footer: |\n      BICT Mar - Unifesp · <a href=\"/ecologia-numerica.html\" target=\"_blank\">Ecologia Numérica</a>\n    theme: [default, custom.scss]\ntitle-slide-attributes:\n   data-background: \"#d6e0e4\" #\"linear-gradient(135deg, #d0dee4, #b8c0ce, #adc8f1)\"\n   # data-background-image: images/watershed-graph.png\n   # data-background-size: cover\n   # background-repeat: no-repeat\n   # data-background-opacity: \"0.45\"\n    \n---\n\n\n## Conteúdo da Aula\n\n::: {.content-box}\n1. Introdução à Regressão Linear Simples\n2. Definição dos Resíduos\n3. Método dos Mínimos Quadrados\n4. Representação Vetorial dos Resíduos\n5. Geometria da Solução de Mínimos Quadrados\n6. Solução Matricial do Método dos Mínimos Quadrados\n7. Valores preditos\n8. Soma dos quadrados dos resíduos\n9. Soma dos quadrados dos totais\n10. Coeficiente de determinação\n:::\n\n## Introdução à Regressão Linear Simples\n\nA regressão linear simples é um método para modelar a relação entre uma variável dependente $y$ e uma variável independente $x$. A equação da reta ajustada é dada por:\n\n$$ \\hat{y} = \\beta_0 + \\beta_1 x $$\n\n::: {style=\"font-size: .6em\"}\n\n:::: columns\n::: {.column width=\"30%\"}\n\n| Observação | $x_i$ | $y_i$ |\n|------------|---------|---------|\n| $1$      | $x_1$ | $y_1$ |\n| $2$      | $x_2$ | $y_2$ |\n| $3$      | $x_3$ | $y_3$ |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n| $n$      | $x_n$ | $y_n$ |\n\n\n:::\n\n::: {.column width=\"70%\"}\n\n![](images/diagrama_regressao.png){style=\"background-color:#ffffff; width:600px\"}\n\n:::\n::::\n\n:::\n\n## Definição dos Resíduos\n\nNa figura abaixo, os resíduos $e_i$ representam as diferenças entre os valores observados $y_i$ e os valores ajustados $\\hat{y}_i$ pela reta de regressão:\n\n$$ e_i = y_i - (\\beta_0 + \\beta_1 x_i) $$\n\nPortando na regressão linear, assume-se que o valor observado em $y_i$ é dado por:\n\n$$ y_i = \\beta_0 + \\beta_1 x_i + e_i$$\n\n::: {.callout-tip}\n\n\n\n:::: columns\n\n::: {.column width=\"50%\"}\n\nAcesse o link [Regresão linear Geogebra](https://www.geogebra.org/calculator/d8a9x9j3){target=\"_blank\"}\n\n::: {style=\"font-size: .6em\"}\n\n<div style=\"text-align: center;\">\n\n| Observação | $x_i$ | $y_i$ |\n|------------|---------|---------|\n| $1$      | $x_1$ | $y_1$ |\n| $2$      | $x_2$ | $y_2$ |\n| $3$      | $x_3$ | $y_3$ |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n| $n$      | $x_n$ | $y_n$ |\n\n</div>\n\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n<div style=\"text-align: center;\">\n\n![](images/diagrama_regressao.png){style=\"background-color:#ffffff; width:70%\"}\n\n</div>\n:::\n::::\n\n\n\n:::\n\n## Método dos Mínimos Quadrados\n\nO Método dos Mínimos Quadrados busca minimizar a soma dos quadrados dos resíduos:\n\n$$ SQ_{res} = \\sum_{i=1}^{n} e_i^2 = e_1^2 + e_2^2 + \\cdots + e_n^2 $$\n\nQue pode ser representada como:\n\n$$\n\\begin{cases}\ne_1 = y_1 - (\\beta_0 + \\beta_1 x_1) \\\\\ne_2 = y_2 - (\\beta_0 + \\beta_1 x_2) \\\\\n\\vdots \\\\\ne_n = y_n - (\\beta_0 + \\beta_1 x_n) \\\\\n\\end{cases}\n$$\n\n## Representação Vetorial dos Resíduos\n\nPodemos portanto representar os resíduos como vetor em que o vetor $\\vec{e}$ é igual ao vetor $y$ menos uma **combinação linear** dos vetores $\\vec{f}_0$ e $\\vec{f}_0$ com constantes $\\beta_0$ e $\\beta_1$.\n\n</br>\n\n$$ \\vec{e} = \\vec{y} - (\\beta_0 \\vec{f}_0 + \\beta_1 \\vec{f}_1) $$\n\n</br>\n\n::: {style=\"font-size: .7em\"}\n\n$$\n\\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right]\n= \n\\left[ \\begin{array}{c}\ny_1 - (\\beta_0 + \\beta_1 x_1) \\\\\ny_2 - (\\beta_0 + \\beta_1 x_2) \\\\\n\\vdots \\\\\ny_n - (\\beta_0 + \\beta_1 x_n) \\\\\n\\end{array} \\right] \n=\n\\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array} \\right]\n-\n\\left(\n\\beta_0\n\\left[ \\begin{array}{c}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{array} \\right]\n+\n\\beta_1\n\\left[ \\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{array} \\right]\n\\right)\n$$\n\n\nOnde:\n\n$$\\vec{e} =\n\\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right];\n\\vec{y} =\n\\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\n\\vec{f}_0 =\n\\left[ \\begin{array}{c}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{array} \\right];\n\\vec{f}_1 =\n\\left[ \\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{array} \\right]$$\n\n:::\n\n## Geometria da Solução de Mínimos Quadrados\n\nA Soma dos quadrados dos resíduos ($SQ_{res}$) pode ser obtida pela **norma ao quadrado** do vetor $\\vec{e}$:\n\n$$SQ_{res} = \\Vert\\vec{e}\\Vert^{2}=\\vec{e}\\cdot\\vec{e}=e_{1}^{2}+e_{2}^{2}+\\cdots+e_{n}^{2}$$\n\n</br>\n\n::: {.callout-tip title='Representação da Solução do MMQ no GeoGebra'}\n\nO Método dos Mínimos Quadrados determina $\\beta_0$ e $\\beta_1$ de modo a *minimizar o comprimento* (a norma) do vetor $\\vec{e}$ que pode ser obtida impondo que o vetor $\\vec{e}$ seja ortogonal aos vetores $\\vec{f_0}$ e $\\vec{f_1}$, ou seja:\n\n$$ \\vec{f_0} \\cdot \\vec{e} = 0 $$\n$$ \\vec{f_1} \\cdot \\vec{e} = 0 $$\n\n[Link para solução vetorial do MMQ](https://www.geogebra.org/calculator/eyk8ammw){target=\"_blank\"}\n\n:::\n\n## Geometria da Solução de Mínimos Quadrados\n\n$$\n\\left\\{\\begin{array} {c}\n\\vec{f_0} \\cdot \\vec{e} = 0 \\Leftrightarrow \\vec{f_0}\\cdot(\\vec{y}-\\beta_0\\vec{f_0}-\\beta_1\\vec{f_1})=0\\\\\n\\vec{f_1} \\cdot \\vec{e} = 0 \\Leftrightarrow \\vec{f_1}\\cdot(\\vec{y}-\\beta_0\\vec{f_0}-\\beta_1\\vec{f_1})=0\n\\end{array} \\right.\n$$\nque é equivalente a:\n$$\n\\left\\{\\begin{array} {c}\n\\beta_0\\vec{f_0}\\cdot\\vec{f_0}+\\beta_1\\vec{f_0}\\cdot\\vec{f_1}=\\vec{f_0}\\cdot\\vec{y}\\\\\n\\beta_0\\vec{f_1}\\cdot\\vec{f_0}+\\beta_1\\vec{f_1}\\cdot\\vec{f_1}=\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right.\n,\n$$\nque ainda pode ser escrito na forma matricial:\n$$\n\\left[ \\begin{array}{cc}\n\\vec{f_0}\\cdot\\vec{f_0} & \\vec{f_0}\\cdot\\vec{f_1}\\\\\n\\vec{f_1}\\cdot\\vec{f_0} & \\vec{f_1}\\cdot\\vec{f_1}\n\\end{array} \\right]\n\\left[ \\begin{array}{c}\n\\beta_0\\\\\n\\beta_1\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\n\\vec{f_0}\\cdot\\vec{y}\\\\\n\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right]\n$$\n\n## Solução Matricial do Método dos Mínimos Quadrados\n\nA combinação linear:\n\n$$\n\\left[ \\begin{array}{cc}\n\\vec{f_0}\\cdot\\vec{f_0} & \\vec{f_0}\\cdot\\vec{f_1}\\\\\n\\vec{f_1}\\cdot\\vec{f_0} & \\vec{f_1}\\cdot\\vec{f_1}\n\\end{array} \\right]\n\\left[ \\begin{array}{c}\n\\beta_0\\\\\n\\beta_1\n\\end{array} \\right]\n=\n\\left[ \\begin{array}{c}\n\\vec{f_0}\\cdot\\vec{y}\\\\\n\\vec{f_1}\\cdot\\vec{y}\n\\end{array} \\right]\n$$\n\npor ser expressa pelas matrizes:\n\n$$X = \\left[ \\begin{array}{ccc}\n\\vec{f_0} & \\vdots & \\vec{f_1}\n\\end{array} \\right] = \n\\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right]; \nY = \\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\nB = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n$$\n\nE finalmente:\n\n$$B = (X^{T} X)^{-1}(X^{T}Y)$$\n\n## Calculando os valores preditos ($\\hat{y}$)\n\nDefinimos $\\mathbf{F}$ como a matriz coluna que contém os valores **preditos** de $y$ (denominados $\\hat{y}$), isto é, aquela que contém os pontos em $y$ que se sobrepõem à reta da regressão linear. Podemos obter $\\mathbf{F}$ por meio da operação matricial abaixo:\n\n<hr>\n$$\\mathbf{F} = \\mathbf{X}\\mathbf{B}$$\n<hr>\n\n$$\\mathbf{F} = \\left[ \\begin{array}{c}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots & \\vdots \\\\\n\\hat{y}_n \\\\\n\\end{array} \\right]; \\mathbf{X} = \\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right]; \\mathbf{B} = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n$$\n\n## Vetor de resíduos ($e$)\n\nFinalmente, o vetor de resíduos é obtido por:\n\n<hr>\n$$e = \\mathbf{Y} - \\mathbf{F}$$\n<hr>\n\nAgora temos todos os componentes da regressão linear estabelecida inicialmente:\n\n$$ \\hat{y_i} = \\beta_0 + \\beta_1 x_i $$\n\ne \n\n$$ y_i = \\hat{y_i} + e_i $$\n\n## Soma dos quadrados dos resíduos ($SQ_{res}$)\n\nA Soma dos quadrados dos resíduos foi definida pela expressão abaixo:\n\n$$SQ_{res} = \\Vert\\vec{e}\\Vert^{2}=\\vec{e}\\cdot\\vec{e}=e_{1}^{2}+e_{2}^{2}+\\cdots+e_{n}^{2}$$\n\nConsiderando $\\vec{e}$ como a *matriz coluna* $\\mathbf{e}$:\n\n$$\\mathbf{e} = \\left[ \\begin{array}{c}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \\\\\n\\end{array} \\right]\n$$\n\nPodemos fazer:\n\n<hr>\n\n$$SQ_{res} = \\mathbf{e}^\\top \\mathbf{e}$$\n\n<hr>\n\n## Soma dos quadrados totais ($SQ_{tot}$)\n\n$SQ_{tot}$ pode ser definido como:\n\n<hr>\n\n$$SQ_{tot} = \\sum_{i}^{n}{(y_i - \\overline{y})^{2}} = (y_1 - \\overline{y})^{2} + (y_2 - \\overline{y})^{2} + \\cdots + (y_n - \\overline{y})^{2}$$\n\nem que $\\overline{y}$ é a **média aritmética** de $y$\n\n<hr>\n\n::: {.columns}\n::: {.column width=\"50%\"}\nPodemos definir a matrix coluna $\\mathbf{D}$\n\n$$\\mathbf{D} = \\left[ \\begin{array}{c}\n(y_1 - \\overline{y})^{2} \\\\\n(y_2 - \\overline{y})^{2} \\\\\n\\vdots\\\\\n(y_n - \\overline{y})^{2} \\\\\n\\end{array} \\right]\n$$\n\n:::\n\n::: {.column width=\"50%\"}\nE obter $SQ_{tot}$ por:\n\n$$SQ_{tot} = \\mathbf{D}^\\top \\mathbf{D}$$\n\n:::\n:::\n\n## Coeficiente de determinação ($R^2$)\n\nA qualidade do ajuste pode ser determinada pelo **coeficiente de determinação** ($R^2$), um índice que varia entre 0 e 1.\n\n<hr>\n$$R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}$$\n<fr>\n\n## Coeficiente de determinação ($R^2$)\n\n\n\n::: {#49450a4d .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](metodo-minimos-quadrados_files/figure-revealjs/cell-3-output-1.png){width=1430 height=609}\n:::\n:::\n\n\n## Método dos mínimos quadrados: Resumo dos passos {.smaller}\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.callout-tip title=\"Resolução do MMQ\"}\n\n1. Definição das matrizes do sistema\n\n$$X = \\left[ \\begin{array}{cc}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n \\\\\n\\end{array} \\right]; \nY = \\left[ \\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\\\\n\\end{array} \\right];\nB = \\left[ \\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\end{array} \\right]\n$$\n\n2. Cálculo dos coeficientes\n\n$$B = (X^{T} X)^{-1}(X^{T}Y)$$\n\n3. Valores preditos\n\n$$\\mathbf{F} = \\mathbf{X}\\mathbf{B}$$\n\n\n4. Matriz coluna de Resíduos\n\n$$\\mathbf{e} = \\mathbf{Y} - \\mathbf{F}$$\n\n:::\n\n:::\n::: {.column width=\"50%\"}\n\n::: {.callout-tip title=\"Qualidade do ajuste\"}\n\n5. Soma dos quadrados dos resíduos\n\n$$SQ_{res} = \\mathbf{e}^\\top \\mathbf{e}$$\n\n\n6. Soma dos quadrados totais\n\n$$SQ_{tot} = \\mathbf{D}^\\top \\mathbf{D}$$\n\n\n7. Coeficiente de determinação\n\n$$R^2 = 1 - \\frac{SQ_{res}}{SQ_{tot}}$$\n\n:::\n\n:::\n:::\n\n",
    "supporting": [
      "metodo-minimos-quadrados_files"
    ],
    "filters": [],
    "includes": {}
  }
}