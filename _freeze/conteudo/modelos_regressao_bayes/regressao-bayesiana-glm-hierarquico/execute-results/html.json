{
  "hash": "5c2c31a7a09a7ff0a1f763bbd9880091",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Explorando Modelos de Regressão Bayesiana\"\nsubtitle: \"Dos modelos lineares a respostas generalizadas e estruturas hierárquicas.\"\nauthor: \"5290 - PROBABILIDADE E ESTATÍSTICA\"\nformat:\n  revealjs: \n    slide-number: true\n    chalkboard: \n      buttons: false\n    preview-links: auto\n    logo: images/logo_white.png \n    footer: |\n      BICT Mar - Unifesp · <a href=\"https://fcopf.github.io/uc-probabilidade-estatistica/\" target=\"_blank\">5290 - PROBABILIDADE E ESTATÍSTICA</a>\n    theme: [default, custom.scss]\ntitle-slide-attributes:\n   data-background: \"#d6e0e4\" #\"linear-gradient(135deg, #d0dee4, #b8c0ce, #adc8f1)\"\n   # data-background-image: images/watershed-graph.png\n   # data-background-size: cover\n   # background-repeat: no-repeat\n   # data-background-opacity: \"0.45\"\n\nexecute:\n    eval: false\n---\n\n\n## O que aprendemos até aqui: prioris e posterioris\n\n<iframe src=\"https://fcopf-binomial-bayesiana.share.connect.posit.cloud/\" style=\"width: 100%; height: 80%; zoom: 0.6;\" frameborder=\"0\"></iframe>\n\n---\n\n## O que aprendemos até aqui: distribuição Normal de Probabilidade\n\n$$\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2} \\left(\\frac{y - \\mu}{\\sigma} \\right)^2} \\longrightarrow \\quad y \\sim \\mathcal{N}(\\mu, \\sigma)\n$$\n\n![](images/normal-distributions.png){width=100% fig-align=\"center\"}\n\n\n---\n\n## O que aprendemos até aqui: o modelo de Regressão linear\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n\n---\n\n**Variável aleatória resposta**\n\n---\n\n$$\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma)\n$$\n\n---\n\n**Prioris**\n\n---\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n$$\n\n$$\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n$$\n\n$$\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n$$\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](images/regressao_linear_retas.png)\n\n:::\n\n:::\n\n---\n\n## O que aprendemos até aqui: Programação Probabilística\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n\n---\n\n**PyMC**\n\n---\n\n```{.python code-line-numbers=\"|1|2-5|7-10|12-13\"}\nwith pm.Model() as modelo:\n    # Definição das prioris\n    Intercept = pm.Normal(\"Intercept\", mu=60, sigma=5)\n    calcado = pm.Normal(\"calcado\", mu=2.8, sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n    \n    # Definição do modelo\n    mu = beta_0 + beta_1 * X\n    altura = pm.Normal(\"altura\", mu=mu, sigma=sigma, \n                        observed=Y)\n    \n    # Amostra a distribuição posterior\n    resultados = pm.sample()\n```\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n---\n\n**Bambi**\n\n---\n\n```{.python code-line-numbers=\"|1-6|8-10|12-13\"}\n# Definição das prioris\ncustom_priors = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu=60, sigma=5),\n    \"calcado\": bmb.Prior(\"Normal\", mu=2.8, sigma=0.1),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=10)\n}\n\n# Definição do modelo\nmodelo = bmb.Model(\"altura ~ calcado\", df, \n                    priors=custom_priors)\n\n# Amostra a distribuição posterior\nresultados = modelo.fit()\n```\n\n:::\n\n:::\n\n---\n\n## O que aprendemos até aqui: ajuste da posteriori\n\n::: columns\n\n::: {.column width=\"50%\"}\n\n![](images/trace-plots.png)\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](images/regressao_linear_retas.png)\n\n:::\n\n:::\n\n---\n\n## Daqui para frente: uma variedade de modelos e estruturas\n\n1.  Extenção da Regressão Linear para:\n    -   Múltiplos preditores.\n    -   Diferentes tipos de variáveis resposta (GLMs).\n    -   Dados com estrutura de agrupamento (Modelos Hierárquicos).\n\n---\n\n## O Modelo Linear Bayesiano Simples\n\n-   Modelamos uma variável resposta contínua ($y$) como uma distribuição Normal.\n-   A *média* da Normal depende linearmente do preditor ($x$).\n-   Todos os parâmetros ($\\beta_0, \\beta_1, \\sigma$) são tratados como variáveis aleatórias e recebem distribuições *a priori* (Prioris).\n\n---\n\n## Verossimilhança e Prioris (RL Simples)\n\n**Verossimilhança** (Como os dados são gerados dado os parâmetros):\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\text{onde } \\mu = \\beta_0 + \\beta_1 x\n$$\n\n**Prioris** (Nossas crenças sobre os parâmetros antes de ver os dados):\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\quad \\text{(Intercepto)}\n$$\n\n$$\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1}) \\quad \\text{(Inclinação)}\n$$\n\n$$\n\\sigma \\sim \\text{HalfCauchy}(S) \\quad \\text{(Desvio Padrão, deve ser positivo)}\n$$\n\n\n---\n\n## Ajustando RL Simples com Bambi\n\n::: columns\n\n::: {.column width=\"55%\"}\n\nVamos carregar um conjunto de dados de exemplo e ajustar o modelo. Usaremos os dados `penguins` que vêm com o Bambi/Arviz.\n\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n\n:::\n:::\n\n## Interpretando o Posterior (RL Simples)\n\nA tabela mostra o resumo das distribuições posteriores para cada parâmetro (`β₀`, `β₁`, `σ`).\n\n- **mean, sd**: Média e desvio padrão da distribuição posterior.\n- **hdi_3%, hdi_97%**: Intervalo Crível de Maior Densidade (HDI) de 94%. Representa a região onde os parâmetros estão com 94% de probabilidade.\n- **mcse_mean, mcse_sd**: Erro padrão da média e do desvio padrão Monte Carlo (relacionado à qualidade da amostragem).\n- **ess_bulk, ess_tail**: Número efetivo de amostras (*Effective Sample Size*). Valores maiores indicam melhor amostragem.\n- **r_hat**: Estatística de Raftery-Lewis. Valores próximos de 1 indicam boa convergência das cadeias MCMC. Deve ser próximo de 1 (≤ 1.05).\n\n**Exemplo (saída anterior):**\n\n`bill_length_mm`: O intervalo HDI para `β₁` não inclui 0, sugerindo um efeito positivo da medida do bico na massa corporal, com 94% de credibilidade.\n\n---\n\n## Regressão com Múltiplos Preditores\n\n### Extendendo o Modelo Linear\n\nPodemos incluir mais variáveis explicativas (`x₂`, `x₃`, …).\n\nA estrutura da **verossimilhança** continua a mesma (Normal), mas a média linear agora inclui mais termos.\n\nCada novo preditor adiciona um novo coeficiente `β` ao modelo, que também precisa de uma *prior*.\n\n---\n\n### Verossimilhança e Prioris (Múltiplos Preditores)\n\n**Verossimilhança:**\n\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n$$\n\n**Prioris:**\n\n$$\n\\beta_j \\sim \\mathcal{N}(\\mu_j, \\sigma_j), \\quad \\text{para } j = 0, 1, \\dots, p \\\\\n\\sigma \\sim \\text{HalfCauchy}(S)\n$$\n\n\n> **Nota:** O `bambi` atribui *prioris* padrão para todos os coeficientes de forma similar.\n\n\n\n\n\n$$y \\sim \\mathcal{N}(\\mu, \\sigma), \\quad \\mu = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p$$\n\n## Ajustando com Múltiplos Preditores em Bambi\n\n::: columns\n\n::: {.column width=\"55%\"}\n\nAdicionamos flipper_length_mm como preditor.\n\n\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n\n:::\n:::\n\nInterpretando (Múltiplos Preditores)\nCada coeficiente \nβ\nj\nβ \nj\n​\n \n agora representa o efeito médio de \nx\nj\nx \nj\n​\n \n na resposta \ny\ny\n, mantendo todos os outros preditores no modelo constantes. Este é o efeito parcial ou efeito controlado.\n\nExemplo: O coeficiente para bill_length_mm mostra o efeito de aumentar o comprimento do bico em 1mm na massa corporal, para pinguins com o mesmo comprimento de nadadeira.\n\nModelando Diferentes Tipos de Dados (GLMs)\nPor Que Mudar a Distribuição?\nNem todas as respostas são contínuas e aproximadamente Normais.\n\nContagens (não negativas, inteiras): número de acidentes.\n\nBinário (0/1): sucesso/falha, sim/não.\n\nProporções: taxa de germinação.\n\nTempos/Durações (positivos contínuos, assimétricos): tempo de vida útil.\n\nModelos Gaussianos podem ser inadequados (podem prever valores negativos, violar suposições, etc.).\n\nModelos Lineares Generalizados (GLMs) permitem modelar respostas com outras distribuições.\n\nA Estrutura dos GLMs\nMantêm o preditor linear: \nη\n=\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\nη=β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n \n.\n\nA resposta \ny\ny\n segue uma distribuição de uma família exponencial (Normal, Poisson, Binomial, Gamma, etc.).\n\nA média (\nμ\nμ\n) da distribuição da resposta é relacionada ao preditor linear \nη\nη\n através de uma função de ligação (link function) \ng\ng\n: \ng\n(\nμ\n)\n=\nη\ng(μ)=η\n.\n\nA escolha da família de distribuição e da função de ligação depende do tipo de dados resposta.\n\nRegressão de Poisson (Para Contagens)\nContexto: Variáveis resposta que são contagens não negativas (0, 1, 2, ...).\n\nIdeia: A resposta \ny\ny\n segue uma distribuição de Poisson. A média (\nλ\nλ\n) da distribuição de Poisson está relacionada ao preditor linear pela função de ligação logarítmica: \nlog\n⁡\n(\nλ\n)\n=\nη\nlog(λ)=η\n.\n\nλ\nλ\n deve ser positivo, e o log garante que \nη\nη\n (que pode ser qualquer valor real) mapeie para um valor positivo.\n\nVerossimilhança e Prioris (Poisson)\nVerossimilhança:\n\ny\n∼\nPoisson\n(\nλ\n)\nonde \nlog\n⁡\n(\nλ\n)\n=\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\ny∼Poisson(λ)\nonde log(λ)=β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n \nOu equivalentemente:\n\nλ\n=\nexp\n⁡\n(\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\n)\nλ=exp(β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n )\nPrioris:\n\nβ\nj\n∼\nN\n(\nμ\nj\n,\nσ\nj\n)\npara \nj\n=\n0\n,\n1\n,\n…\n,\np\nβ \nj\n​\n ∼N(μ \nj\n​\n ,σ \nj\n​\n )para j=0,1,…,p\n(Não há parâmetro de dispersão separado como \nσ\nσ\n na Poisson básica; a média é igual à variância.)\n\nAjustando Poisson com Bambi\n::: columns\n\n::: {.column width=\"55%\"}\n\nVamos gerar alguns dados sintéticos para ilustrar a Regressão Poisson.\n\n# Gerar dados sintéticos\nnp.random.seed(42)\nn_obs = 100\nx_poisson = np.random.rand(n_obs) * 5\n# Modelo verdadeiro: log(lambda) = 1 + 0.5 * x\nlambda_poisson = np.exp(1 + 0.5 * x_poisson)\ny_poisson = np.random.poisson(lambda_poisson)\ndata_poisson = pd.DataFrame({'y': y_poisson, 'x': x_poisson})\n\n# Especificar o modelo Poisson\n# Usamos family='poisson'\nmodel_poisson = bm.Model('y ~ x', data=data_poisson, family='poisson')\n\n# Ajustar o modelo\nidata_poisson = model_poisson.fit(\n    draws=1000,\n    tune=1000,\n    target_accept=0.95,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_poisson, hdi_prob=0.94)\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\nimport numpy as np\n\n# Gerar dados sintéticos\nnp.random.seed(42)\nn_obs = 100\nx_poisson = np.random.rand(n_obs) * 5\nlambda_poisson = np.exp(1 + 0.5 * x_poisson)\ny_poisson = np.random.poisson(lambda_poisson)\ndata_poisson = pd.DataFrame({'y': y_poisson, 'x': x_poisson})\n\n\n# Especificar o modelo Poisson\nmodel_poisson = bm.Model('y ~ x', data=data_poisson, family='poisson')\n\n# Ajustar o modelo\nidata_poisson = model_poisson.fit(\n    draws=500, # Fewer draws for speed\n    tune=500,\n    target_accept=0.9,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_poisson, hdi_prob=0.94)\n```\n\n{python}\n:::\n:::\n\nInterpretando (Poisson)\nOs coeficientes (\nβ\nβ\ns) estão na escala da função de ligação (escala logarítmica).\n\nUm aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n está associado a um aumento de \nβ\nj\nβ \nj\n​\n \n na escala logarítmica da média.\n\nNa escala original (\nλ\nλ\n), o efeito é multiplicativo: um aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n multiplica a média esperada por \nexp\n⁡\n(\nβ\nj\n)\nexp(β \nj\n​\n )\n. \nexp\n⁡\n(\nβ\nj\n)\nexp(β \nj\n​\n )\n é a Razão de Médias Esperadas.\n\nRegressão Logística / Binomial (Para Binário/Proporção)\nContexto: Variável resposta é binária (0 ou 1) ou o número de \"sucessos\" em um número fixo de tentativas (dados de proporção).\n\nIdeia: A resposta \ny\ny\n segue uma distribuição Bernoulli (para binário) ou Binomial (para proporção). A probabilidade de sucesso (\np\np\n) está relacionada ao preditor linear pela função de ligação logit: \nlogit\n(\np\n)\n=\nlog\n⁡\n(\np\n1\n−\np\n)\n=\nη\nlogit(p)=log( \n1−p\np\n​\n )=η\n.\n\nA função logit mapeia probabilidades (0 a 1) para a linha real (\n±\n∞\n±∞\n).\n\nVerossimilhança e Prioris (Binomial)\nVerossimilhança:\n\ny\n∼\nBernoulli\n(\np\n)\n(\nou Binomial\n(\nN\n,\np\n)\n)\nonde logit\n(\np\n)\n=\nlog\n⁡\n(\np\n1\n−\np\n)\n=\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\ny∼Bernoulli(p)(ou Binomial(N,p))\nonde logit(p)=log( \n1−p\np\n​\n )=β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n \nOu equivalentemente:\n\np\n=\nexp\n⁡\n(\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\n)\n1\n+\nexp\n⁡\n(\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\n)\n(Fun\nc\n¸\na\n˜\no Sigm\no\nˊ\nide/Log\nı\nˊ\nstica Inversa)\np= \n1+exp(β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n )\nexp(β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n )\n​\n (Fun \nc\n¸\n​\n  \na\n˜\n o Sigm \no\nˊ\n ide/Log \nı\nˊ\n stica Inversa)\nPrioris:\n\nβ\nj\n∼\nN\n(\nμ\nj\n,\nσ\nj\n)\npara \nj\n=\n0\n,\n1\n,\n…\n,\np\nβ \nj\n​\n ∼N(μ \nj\n​\n ,σ \nj\n​\n )para j=0,1,…,p\nAjustando Binomial com Bambi\n::: columns\n\n::: {.column width=\"55%\"}\n\nVamos gerar dados sintéticos para Regressão Binomial.\n\n# Gerar dados sintéticos\nnp.random.seed(43)\nn_obs = 100\nx_binomial = np.random.rand(n_obs) * 5\n# Modelo verdadeiro: logit(p) = -1 + 0.8 * x\nlogit_p = -1 + 0.8 * x_binomial\np_binomial = 1 / (1 + np.exp(-logit_p)) # Sigmoid function\n# Para dados binários (0 ou 1)\ny_binomial = (np.random.rand(n_obs) < p_binomial).astype(int)\ndata_binomial = pd.DataFrame({'y': y_binomial, 'x': x_binomial})\n\n# Especificar o modelo Binomial\n# Usamos family='bernoulli' para resposta binária (0 ou 1)\n# Para proporções/N tentativas, usar family='binomial' e arg 'trials'\nmodel_binomial = bm.Model('y ~ x', data=data_binomial, family='bernoulli')\n\n# Ajustar o modelo\nidata_binomial = model_binomial.fit(\n    draws=1000,\n    tune=1000,\n    target_accept=0.95,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_binomial, hdi_prob=0.94)\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\nimport numpy as np\n\n# Gerar dados sintéticos\nnp.random.seed(43)\nn_obs = 100\nx_binomial = np.random.rand(n_obs) * 5\nlogit_p = -1 + 0.8 * x_binomial\np_binomial = 1 / (1 + np.exp(-logit_p))\ny_binomial = (np.random.rand(n_obs) < p_binomial).astype(int)\ndata_binomial = pd.DataFrame({'y': y_binomial, 'x': x_binomial})\n\n# Especificar o modelo Binomial\nmodel_binomial = bm.Model('y ~ x', data=data_binomial, family='bernoulli')\n\n# Ajustar o modelo\nidata_binomial = model_binomial.fit(\n    draws=500, # Fewer draws for speed\n    tune=500,\n    target_accept=0.9,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_binomial, hdi_prob=0.94)\n```\n\n{python}\n:::\n:::\n\nInterpretando (Binomial)\nOs coeficientes (\nβ\nβ\ns) estão na escala logit (log-odds).\n\nUm aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n está associado a um aumento de \nβ\nj\nβ \nj\n​\n \n na escala logit da probabilidade de sucesso.\n\nNa escala original (\np\np\n), o efeito é não-linear. O efeito multiplicativo se aplica às odds (\np\n/\n(\n1\n−\np\n)\np/(1−p)\n). Um aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n multiplica as odds por \nexp\n⁡\n(\nβ\nj\n)\nexp(β \nj\n​\n )\n. \nexp\n⁡\n(\nβ\nj\n)\nexp(β \nj\n​\n )\n é a Razão de Chances (Odds Ratio).\n\nRegressão Exponencial / Gamma (Para Positivo Contínuo)\nContexto: Variáveis resposta positivas e contínuas, frequentemente assimétricas (ex: tempos de vida, renda, valores monetários). A Exponencial é um caso especial da Gamma.\n\nIdeia: A resposta \ny\ny\n segue uma distribuição Exponencial ou Gamma. A média ou a taxa da distribuição é relacionada ao preditor linear. Comumente, usa-se a ligação logarítmica para garantir que o parâmetro da distribuição (taxa ou média) seja positivo.\n\nVerossimilhança e Prioris (Gamma)\nVerossimilhança (usando parametrização média-dispersão para Gamma):\n\ny\n∼\nGamma\n(\nμ\n,\nϕ\n)\nonde \nlog\n⁡\n(\nμ\n)\n=\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\ne \nϕ\n \ne\nˊ\n o par\na\nˆ\nmetro de dispers\na\n˜\no (forma/taxa)\ny∼Gamma(μ,ϕ)\nonde log(μ)=β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n \ne ϕ  \ne\nˊ\n  o par \na\nˆ\n metro de dispers \na\n˜\n o (forma/taxa)\nOu equivalentemente:\n\nμ\n=\nexp\n⁡\n(\nβ\n0\n+\nβ\n1\nx\n1\n+\n⋯\n+\nβ\np\nx\np\n)\nμ=exp(β \n0\n​\n +β \n1\n​\n x \n1\n​\n +⋯+β \np\n​\n x \np\n​\n )\n(A Exponencial é Gamma com \nϕ\n=\n1\nϕ=1\n.)\n\nPrioris:\n\nβ\nj\n∼\nN\n(\nμ\nj\n,\nσ\nj\n)\npara \nj\n=\n0\n,\n1\n,\n…\n,\np\nβ \nj\n​\n ∼N(μ \nj\n​\n ,σ \nj\n​\n )para j=0,1,…,p\nϕ\n∼\nHalfCauchy\n(\nS\nϕ\n)\n(Dispers\na\n˜\no, deve ser positiva)\nϕ∼HalfCauchy(S \nϕ\n​\n )(Dispers \na\n˜\n o, deve ser positiva)\nAjustando Gamma com Bambi\n::: columns\n\n::: {.column width=\"55%\"}\n\nVamos gerar dados sintéticos para Regressão Gamma (simulando Exponencial para simplicidade).\n\n# Gerar dados sintéticos (simulando Exponencial)\nnp.random.seed(44)\nn_obs = 100\nx_gamma = np.random.rand(n_obs) * 3\n# Modelo verdadeiro: log(media) = 0.5 - 0.3 * x\n# media = exp(log(media))\nmean_gamma = np.exp(0.5 - 0.3 * x_gamma)\n# Para Exponential, rate = 1/media. For Gamma, shape=1, rate=1/media\ny_gamma = np.random.exponential(scale=mean_gamma) # scale = 1/rate = mean\ndata_gamma = pd.DataFrame({'y': y_gamma, 'x': x_gamma})\n\n# Especificar o modelo Gamma\n# Usamos family='gamma'. Bambi usa log link por padrão para a média.\n# Nota: A parametrização da família Gamma pode variar entre softwares.\nmodel_gamma = bm.Model('y ~ x', data=data_gamma, family='gamma')\n\n# Ajustar o modelo\nidata_gamma = model_gamma.fit(\n    draws=1000,\n    tune=1000,\n    target_accept=0.95,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_gamma, hdi_prob=0.94)\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\nimport numpy as np\n\n# Gerar dados sintéticos\nnp.random.seed(44)\nn_obs = 100\nx_gamma = np.random.rand(n_obs) * 3\nmean_gamma = np.exp(0.5 - 0.3 * x_gamma)\ny_gamma = np.random.exponential(scale=mean_gamma) # scale = 1/rate = mean\ndata_gamma = pd.DataFrame({'y': y_gamma, 'x': x_gamma})\n\n# Especificar o modelo Gamma\nmodel_gamma = bm.Model('y ~ x', data=data_gamma, family='gamma')\n\n# Ajustar o modelo\nidata_gamma = model_gamma.fit(\n    draws=500, # Fewer draws for speed\n    tune=500,\n    target_accept=0.9,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_gamma, hdi_prob=0.94)\n```\n\n{python}\n:::\n:::\n\nInterpretando (Gamma)\nOs coeficientes (\nβ\nβ\ns) estão na escala logarítmica da média da resposta.\n\nUm aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n está associado a um aumento de \nβ\nj\nβ \nj\n​\n \n no log da média esperada.\n\nNa escala original (da média), o efeito é multiplicativo: um aumento de 1 unidade em \nx\nj\nx \nj\n​\n \n multiplica a média esperada por \nexp\n⁡\n(\nβ\nj\n)\nexp(β \nj\n​\n )\n.\n\nO modelo também estima um parâmetro de dispersão (\nϕ\nϕ\n), que afeta a forma da distribuição Gamma.\n\nModelos Hierárquicos (Multinível)\nRecap GLMs e Transição\nVimos como modelar diferentes tipos de dados resposta (contínuo, contagem, binário, positivo) escolhendo a Verossimilhança adequada (Normal, Poisson, Binomial, Gamma...).\n\nAgora: Como modelar estruturas de dados (dados agrupados, aninhados) onde os parâmetros podem variar entre os grupos?\n\nRetornaremos à Verossimilhança Gaussiana para focar na ideia hierárquica.\n\nA Ideia dos Modelos Hierárquicos\nContexto: Dados organizados em grupos (alunos em salas, pacientes em hospitais, medições repetidas no tempo para o mesmo indivíduo).\n\nProblema:\n\nModelos separados por grupo: Ignora similaridades, menos robusto para grupos pequenos.\n\nModelo único para todos os grupos (pooling completo): Ignora diferenças entre grupos.\n\nSolução Bayesiana (pooling parcial): Os parâmetros de cada grupo são sorteados de uma distribuição de nível superior (nível \"mestre\"). As informações são compartilhadas entre os grupos através desta distribuição mestra.\n\nModelo com Interceptos Variáveis por Grupo\nIdeia: A inclinação (\nβ\n1\nβ \n1\n​\n \n) é a mesma para todos os grupos, mas cada grupo (\nk\nk\n) tem seu próprio intercepto (\nβ\n0\n,\nk\nβ \n0,k\n​\n \n).\n\nOs interceptos de grupo (\nβ\n0\n,\nk\nβ \n0,k\n​\n \n) não são independentes, mas vêm de uma distribuição comum (geralmente Normal) com média (\nμ\n0\nμ \n0\n​\n \n) e desvio padrão (\nσ\n0\nσ \n0\n​\n \n) próprios. \nμ\n0\nμ \n0\n​\n \n e \nσ\n0\nσ \n0\n​\n \n são os hiperparâmetros do modelo hierárquico.\n\nVerossimilhança e Prioris (Interceptos Variáveis)\nVerossimilhança (Ainda Gaussiana):\n\ny\ni\n∼\nN\n(\nμ\ni\n,\nσ\n)\nonde \nμ\ni\n=\nβ\n0\n,\ngrupo\n[\ni\n]\n+\nβ\n1\nx\ni\n(Cada observa\nc\n¸\na\n˜\no \ni\n pertence a um grupo, grupo\n[\ni\n]\n)\ny \ni\n​\n ∼N(μ \ni\n​\n ,σ)\nonde μ \ni\n​\n =β \n0,grupo[i]\n​\n +β \n1\n​\n x \ni\n​\n \n(Cada observa \nc\n¸\n​\n  \na\n˜\n o i pertence a um grupo, grupo[i])\nPrioris (Hierárquica para Interceptos):\n\nβ\n0\n,\nk\n∼\nN\n(\nμ\n0\n,\nσ\n0\n)\npara cada grupo \nk\n=\n1\n,\n…\n,\nK\nβ \n0,k\n​\n ∼N(μ \n0\n​\n ,σ \n0\n​\n )para cada grupo k=1,…,K\nμ\n0\n∼\nN\n(\nμ\nμ\n0\n,\nσ\nμ\n0\n)\n(M\ne\nˊ\ndia geral dos interceptos)\nμ \n0\n​\n ∼N(μ \nμ \n0\n​\n \n​\n ,σ \nμ \n0\n​\n \n​\n )(M \ne\nˊ\n dia geral dos interceptos)\nσ\n0\n∼\nHalfCauchy\n(\nS\n0\n)\n(Desvio Padr\na\n˜\no dos interceptos entre grupos)\nσ \n0\n​\n ∼HalfCauchy(S \n0\n​\n )(Desvio Padr \na\n˜\n o dos interceptos entre grupos)\nβ\n1\n∼\nN\n(\nμ\n1\n,\nσ\n1\n)\n(Inclina\nc\n¸\na\n˜\no fixa para todos)\nβ \n1\n​\n ∼N(μ \n1\n​\n ,σ \n1\n​\n )(Inclina \nc\n¸\n​\n  \na\n˜\n o fixa para todos)\nσ\n∼\nHalfCauchy\n(\nS\n)\n(Desvio Padr\na\n˜\no residual)\nσ∼HalfCauchy(S)(Desvio Padr \na\n˜\n o residual)\nAjustando Interceptos Variáveis com Bambi\n::: columns\n\n::: {.column width=\"55%\"}\n\nUsaremos os dados penguins novamente, modelando a massa corporal (body_mass_g) pela medida do bico (bill_length_mm), mas permitindo que o intercepto varie por species.\n\n# Use os dados penguins novamente, agora com a variável 'species' como grupo\n# data = bm.load_data('penguins').dropna(...) # Já carregado e tratado antes\n\n# Especificar o modelo com interceptos variáveis por grupo\n# Sintaxe: (1 | nome_da_variavel_de_grupo)\n# 1 representa o intercepto\nmodel_varying_intercept = bm.Model(\n    'body_mass_g ~ bill_length_mm + (1|species)',\n    data=data\n)\n\n# Ajustar o modelo\nidata_varying_intercept = model_varying_intercept.fit(\n    draws=1000,\n    tune=1000,\n    target_accept=0.95,\n    progressbar=False\n)\n\n# Exibir resumo\n# Note que Bambi renomeia os parâmetros hierárquicos\naz.summary(idata_varying_intercept, hdi_prob=0.94)\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\n\n# Use the same data loaded previously\ndata = bm.load_data('penguins')\ndata = data.dropna(subset=['body_mass_g', 'bill_length_mm', 'species'])\n\n\n# Especificar o modelo com interceptos variáveis por grupo\nmodel_varying_intercept = bm.Model(\n    'body_mass_g ~ bill_length_mm + (1|species)',\n    data=data\n)\n\n# Ajustar o modelo\nidata_varying_intercept = model_varying_intercept.fit(\n    draws=500, # Fewer draws for speed\n    tune=500,\n    target_accept=0.9,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_varying_intercept, hdi_prob=0.94)\n```\n\n{python}\n:::\n:::\n\nInterpretando (Interceptos Variáveis)\nbill_length_mm: Coeficiente da inclinação fixa para todos os grupos.\n\n1|species: Representa o termo hierárquico para o intercepto que varia por species.\n\n1|species_rv: Desvio padrão (\nσ\n0\nσ \n0\n​\n \n) da distribuição dos interceptos entre as espécies. Indica a variabilidade dos interceptos.\n\nIntercept: A média (\nμ\n0\nμ \n0\n​\n \n) da distribuição dos interceptos (o intercepto \"geral\" ou médio).\n\n1|species_species[Adelie], 1|species_species[Chinstrap], 1|species_species[Gentoo]: Estes são os desvios dos interceptos de cada espécie em relação à média geral (Intercept). O intercepto real para uma espécie é Intercept + desvio_da_especie. Bambi também pode mostrar os parâmetros de grupo diretamente dependendo da versão/configuração. Arviz summary mostra os desvios.\n\nModelo com Inclinações e Interceptos Variáveis por Grupo\nIdeia: Cada grupo (\nk\nk\n) tem seu próprio intercepto (\nβ\n0\n,\nk\nβ \n0,k\n​\n \n) E sua própria inclinação (\nβ\n1\n,\nk\nβ \n1,k\n​\n \n).\n\nOs pares \n(\nβ\n0\n,\nk\n,\nβ\n1\n,\nk\n)\n(β \n0,k\n​\n ,β \n1,k\n​\n )\n para cada grupo são sorteados de uma distribuição Normal Multivariada comum. Esta distribuição tem um vetor de médias (\nμ\n0\n,\nμ\n1\nμ \n0\n​\n ,μ \n1\n​\n \n) e uma matriz de covariância (\nΣ\nΣ\n).\n\nA matriz \nΣ\nΣ\n captura a variância dos interceptos (\nσ\n0\n2\nσ \n0\n2\n​\n \n), a variância das inclinações (\nσ\n1\n2\nσ \n1\n2\n​\n \n) e a covariância/correlação entre eles (\ncov\n(\nβ\n0\n,\nk\n,\nβ\n1\n,\nk\n)\ncov(β \n0,k\n​\n ,β \n1,k\n​\n )\n).\n\nVerossimilhança e Prioris (Inclinações e Interceptos Variáveis)\nVerossimilhança (Ainda Gaussiana):\n\ny\ni\n∼\nN\n(\nμ\ni\n,\nσ\n)\nonde \nμ\ni\n=\nβ\n0\n,\ngrupo\n[\ni\n]\n+\nβ\n1\n,\ngrupo\n[\ni\n]\nx\ni\ny \ni\n​\n ∼N(μ \ni\n​\n ,σ)\nonde μ \ni\n​\n =β \n0,grupo[i]\n​\n +β \n1,grupo[i]\n​\n x \ni\n​\n \nPrioris (Hierárquica Multivariada):\n\n(\nβ\n0\n,\nk\nβ\n1\n,\nk\n)\n∼\nN\n(\n(\nμ\n0\nμ\n1\n)\n,\nΣ\n)\npara cada grupo \nk\n=\n1\n,\n…\n,\nK\n( \nβ \n0,k\n​\n \nβ \n1,k\n​\n \n​\n )∼N(( \nμ \n0\n​\n \nμ \n1\n​\n \n​\n ),Σ)para cada grupo k=1,…,K\n(\nμ\n0\nμ\n1\n)\n∼\nN\n(\nμ\ngeral\n,\nΣ\ngeral\n)\n(M\ne\nˊ\ndias gerais dos coeficientes)\n( \nμ \n0\n​\n \nμ \n1\n​\n \n​\n )∼N(μ \ngeral\n​\n ,Σ \ngeral\n​\n )(M \ne\nˊ\n dias gerais dos coeficientes)\nΣ\n∼\nPrior para Matriz de Covari\na\nˆ\nncia Positiva-Definida\nΣ∼Prior para Matriz de Covari \na\nˆ\n ncia Positiva-Definida\n(Ex: Priori LKJCorr para a matriz de correlação e HalfCauchy/HalfNormal para os desvios padrões marginais \nσ\n0\n=\nΣ\n11\n,\nσ\n1\n=\nΣ\n22\nσ \n0\n​\n = \nΣ \n11\n​\n \n​\n ,σ \n1\n​\n = \nΣ \n22\n​\n \n​\n \n)\n\nσ\n∼\nHalfCauchy\n(\nS\n)\n(Desvio Padr\na\n˜\no residual)\nσ∼HalfCauchy(S)(Desvio Padr \na\n˜\n o residual)\nAjustando Inclinações e Interceptos Variáveis com Bambi\n::: columns\n\n::: {.column width=\"55%\"}\n\nPermitimos que tanto o intercepto quanto a inclinação (bill_length_mm) variem por species.\n\n# Especificar o modelo com inclinações e interceptos variáveis por grupo\n# Sintaxe: (termo1 + termo2 + ... | variavel_de_grupo)\n# Ou (1 + termo | grupo) se o intercepto também varia\nmodel_varying_slope_int = bm.Model(\n    'body_mass_g ~ (bill_length_mm|species)', # ou '(1 + bill_length_mm | species)'\n    data=data\n)\n# Nota: '(x|g)' no Bambi é um atalho para '(1 + x | g)' se não houver intercepto global,\n# ou '(0 + x | g)' com um intercepto global. A sintaxe '(1 + x | g)' é mais explícita.\n# Usando '(bill_length_mm | species)' implicitamente inclui o intercepto variável.\n\n# Ajustar o modelo\nidata_varying_slope_int = model_varying_slope_int.fit(\n    draws=1000,\n    tune=1000,\n    target_accept=0.95,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_varying_slope_int, hdi_prob=0.94)\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\n\n# Use the same data loaded previously\ndata = bm.load_data('penguins')\ndata = data.dropna(subset=['body_mass_g', 'bill_length_mm', 'species'])\n\n\n# Especificar o modelo com inclinações e interceptos variáveis por grupo\nmodel_varying_slope_int = bm.Model(\n    'body_mass_g ~ (bill_length_mm|species)',\n    data=data\n)\n\n# Ajustar o modelo\nidata_varying_slope_int = model_varying_slope_int.fit(\n    draws=500, # Fewer draws for speed\n    tune=500,\n    target_accept=0.9,\n    progressbar=False\n)\n\n# Exibir resumo\naz.summary(idata_varying_slope_int, hdi_prob=0.94)\n```\n\n{python}\n:::\n:::\n\nInterpretando (Inclinações e Interceptos Variáveis)\n1|species (ou similar): Termo hierárquico que agora modela a variação conjunta do intercepto e da inclinação (bill_length_mm) por species.\n\n1|species_rv: Desvio padrão das distribuições dos efeitos aleatórios. Haverá um para o intercepto variável (~Intercept) e um para a inclinação variável (~bill_length_mm).\n\nIntercept: Média geral dos interceptos (\nμ\n0\nμ \n0\n​\n \n).\n\nbill_length_mm: Média geral das inclinações (\nμ\n1\nμ \n1\n​\n \n).\n\n1|species_corr: A matriz de correlação estimada entre os interceptos variáveis e as inclinações variáveis em nível de grupo. Um valor forte (positivo ou negativo) sugere que grupos com interceptos acima da média tendem a ter inclinações acima/abaixo da média, respectivamente.\n\nParâmetros específicos de grupo (ex: 1|species~Intercept[Adelie], 1|species~bill_length_mm[Adelie]): Os desvios dos interceptos e inclinações para cada grupo em relação às médias gerais.\n\nVisualizando o Modelo Hierárquico\n::: columns\n\n::: {.column width=\"55%\"}\n\nÉ muito útil visualizar as linhas de regressão para cada grupo no modelo com interceptos e inclinações variáveis.\n\nPodemos usar as amostras posteriores para calcular as linhas preditivas para cada grupo.\n\n# Obter amostras posteriores\nposterior_samples = idata_varying_slope_int.posterior\n\n# Calcular as linhas de regressão para cada espécie\n# Precisa obter os interceptos e inclinações para CADA amostra MCMC e CADA espécie\n# E aplicar a fórmula linear: y_pred = intercepto_especie + inclinacao_especie * x\n\n# Bambi tem ferramentas para predição e visualização que facilitam isso\n# plot_predictions é uma forma\nfig, ax = bm.plot_predictions(\n    idata_varying_slope_int,\n    data,\n    group='species', # Varia a cor/linha por grupo\n    predict_kws={'average': False}, # Não calcula a média, mostra linhas por grupo\n    x='bill_length_mm',\n    cl='model', # Intervalo crível do modelo (inclui incerteza dos parâmetros e residual)\n    # add_data=True # Para mostrar os pontos de dados\n    figsize=(8, 5)\n)\n# Adicionar dados brutos para contexto\nax.scatter(data['bill_length_mm'], data['body_mass_g'], c=data['species'].astype('category').cat.codes, alpha=0.3, label='Dados', s=10)\nax.legend(title='Espécie')\nax.set(title='Linhas de Regressão por Espécie (Interceptos e Inclinações Variáveis)',\n       xlabel='Comprimento do Bico (mm)',\n       ylabel='Massa Corporal (g)');\n\n# fig.show() # Em ambiente interativo\n```\n\nPython\n:::\n\n::: {.column width=\"45%\"}\n\n#| echo: false\n#| output: true\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 5\n\nimport bambi as bm\nimport pandas as pd\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n# Use the same data and idata loaded previously\ndata = bm.load_data('penguins')\ndata = data.dropna(subset=['body_mass_g', 'bill_length_mm', 'species'])\n\nmodel_varying_slope_int = bm.Model(\n    'body_mass_g ~ (bill_length_mm|species)',\n    data=data\n)\nidata_varying_slope_int = model_varying_slope_int.fit(\n    draws=300, # Fewer draws for speed/plotting\n    tune=300,\n    target_accept=0.9,\n    progressbar=False\n)\n\n\n# Plotting\nfig, ax = bm.plot_predictions(\n    idata_varying_slope_int,\n    data,\n    group='species',\n    predict_kws={'average': False},\n    x='bill_length_mm',\n    cl='model',\n    figsize=(8, 5)\n)\n\n# Add raw data points\n# Need to map species to colors consistently with Bambi's plot_predictions if possible\n# Or just use default matplotlib colors\nspecies_labels = data['species'].unique()\ncolors = plt.cm.viridis(np.linspace(0, 1, len(species_labels)))\ncolor_map = dict(zip(species_labels, colors))\n\nfor species, color in color_map.items():\n    subset = data[data['species'] == species]\n    ax.scatter(subset['bill_length_mm'], subset['body_mass_g'], color=color, alpha=0.5, label=species, s=10)\n\n# Remove legend from plot_predictions and use custom one\nax.get_legend().remove()\nax.legend(title='Espécie')\n\n\nax.set(title='Linhas de Regressão por Espécie (Interceptos e Inclinações Variáveis)',\n       xlabel='Comprimento do Bico (mm)',\n       ylabel='Massa Corporal (g)');\n\nplt.tight_layout()\nplt.show()\n```\n\n{python}\n:::\n:::\n\nConclusão\nResumo\nComeçamos com a Regressão Linear Bayesiana, conectando RL clássica com Priores e Verossimilhança.\n\nExtendemos para múltiplos preditores.\n\nExploramos Modelos Lineares Generalizados (GLMs) para diferentes tipos de dados resposta (Poisson, Binomial, Gamma), mudando a Verossimilhança e usando Funções de Ligação.\n\nIntroduzimos Modelos Hierárquicos para dados agrupados, permitindo que parâmetros (interceptos, inclinações) variem por grupo, com compartilhamento de informação via Prioris multinível.\n\nEm cada passo, usamos a sintaxe intuitiva do Bambi para especificar e ajustar os modelos.\n\nPróximos Passos\nDiagnóstico de MCMC: Verificar convergência (r_hat, ess), plotar traces (az.plot_trace). Essencial para confiar nos resultados.\n\nVerificação do Modelo: O modelo ajustado realmente captura as características dos dados? Gráficos de posterior predictivo (bm.plot_posterior_predictive).\n\nComparação de Modelos: Como escolher entre modelos concorrentes? Critérios de informação como WAIC ou LOO (usando Arviz).\n\nModelos mais complexos: Efeitos não lineares, termos de interação, outros tipos de modelos hierárquicos, etc.\n\nRecursos\nDocumentação Oficial do Bambi: https://bambi.dev/\n\nDocumentação do Arviz: https://arviz.readthedocs.io/\n\nDocumentação do PyMC: https://www.pymc.io/\n\nLivro \"Statistical Rethinking\" por Richard McElreath (ótimas intuições Bayesianas e GLMs/Hierárquicos).\n\nLivro \"Bayesian Data Analysis\" por Andrew Gelman et al. (referência mais avançada).\n\nPerguntas?\nObrigado!\n\n",
    "supporting": [
      "regressao-bayesiana-glm-hierarquico_files"
    ],
    "filters": [],
    "includes": {}
  }
}