{
  "hash": "b945b25ac550599a9fea86c4a6950481",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regressão Linear Bayesiana\"\nsubtitle: \"Escolha das priores, checagem preditivas e inferência a posteriori.\"\ndescription: \"Regressão linear bayesiana — altura em adultos.\"\nCategories: [\n        \"Inferência bayesiana\",\n        \"Regressão linear\",\n        \"Modelo normal\",\n        \"Distribuição a priori\",\n        \"Distribuição a posteriori\",\n        \"Checagem preditiva\",\n        \"PyMC\"\n        ]\nimage: \"images/regressao-linear-bayesiana.png\"\nexecute:\n    echo: true\n    eval: false\n    warning: false\n    message: false\n---\n\n\n---\n\nNa inferência bayesiana, atualizamos nossas crenças sobre os parâmetros de um modelo combinando o **conhecimento prévio** (expresso pela distribuição *a priori*) com a **informação contida nos dados observados** (expressa pela **verossimilhança**) para obter a **distribuição *a posteriori***. A inferência bayesiana fornece uma **distribuição completa de probabilidade para os parâmetros**, refletindo explicitamente a incerteza sobre seus valores.\n\nNo modelo de regressão linear bayesiano, assumimos que a variável **resposta $y$** é uma **variável aleatória** com distribuição **Normal**, cuja média depende *linearmente* de uma variável preditora $x$, ou seja, $\\mu = \\beta_0 + \\beta_1 x$, e com desvio padrão $\\sigma$.\n\n$$\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma)\n$$ {#eq-likelihood-normal-regression}\n\nOnde:\n\n- $y$ é a variável aleatória observada;\n- $x$ é a variável preditora;\n- $\\beta_0$ é o intercepto, representando o valor esperado de $y$ quando $x = 0$;\n- $\\beta_1$ é o coeficiente de regressão, representando a variação média de $y$ para cada unidade adicional de $x$;\n- $\\sigma$ é o desvio padrão dos erros aleatórios, representando a variabilidade de $y$ em torno da média.\n\nA especificação completa do modelo bayesiano requer a definição das distribuições *a priori* para os parâmetros $\\beta_0$, $\\beta_1$ e $\\sigma$. Podemos assumir, por exemplo, distribuições normais para os coeficientes da regressão e uma distribuição **Lognormal** para o desvio padrão, garantindo que $\\sigma$ assuma apenas valores positivos. As distribuições *a priori* são então:\n\n$$\n\\beta_0 \\sim \\mathcal{N}(\\mu_{\\beta_0}, \\sigma_{\\beta_0})\n$$ {#eq-prior_beta_0}\n\n$$\n\\beta_1 \\sim \\mathcal{N}(\\mu_{\\beta_1}, \\sigma_{\\beta_1})\n$$ {#eq-prior_beta_1}\n\n$$\n\\sigma \\sim \\text{Lognormal}(\\mu_{\\log \\sigma}, \\sigma_{\\log \\sigma})\n$$ {#eq-prior_sigma}\n\n::: {.callout-tip title=\"Modelo Generativo\"}\nOs componentes de **verossimilhança** ([@eq-likelihood-normal-regression]) e as distribuições **a priori** (Equações [-@eq-prior_beta_0], [-@eq-prior_beta_1] e [-@eq-prior_sigma]) definem o **modelo generativo** para a variável aleatória $y$.\n:::\n\n\n---\n\n## Atividate prática\n\n::: {.callout-important title=\"Objetivos de Aprendizagem\"}\n- Compreender os fundamentos da regressão linear sob a abordagem bayesiana.\n- Simular dados utilizando a biblioteca [SciPy](https://scipy.org/){target=_blank}.\n- Aplicar conhecimento prévio para especificar distribuições *a priori* informativas para os parâmetros do modelo.\n- Implementar um modelo de regressão linear bayesiana com [PyMC](https://www.pymc.io/){target=_blank}, realizar a checagem preditiva *a priori* e ajustá-lo a dados reais para obter as distribuições *a posteriori* dos parâmetros.\n- Interpretar e validar os resultados da inferência bayesiana.\n:::\n\nNesta atividade, aplicaremos a inferência bayesiana para modelar a relação entre duas variáveis contínuas: a **altura de indivíduos** e o **número do calçado** que utilizam. O conjunto de dados de altura (cm) e número do calçado está disponível no link: [`altura_adultos.csv`](https://github.com/FCopf/datasets/blob/main/altura_adultos.csv){target=_blank}. \n\nIntuitivamente, esperamos que haja uma relação positiva: pessoas com pés maiores tendem a ser mais altas. Para quantificar essa relação, utilizaremos um **modelo de regressão linear**. O co\n\n::: {#8d75e241 .cell execution_count=1}\n``` {.python .cell-code}\n# Configuração inicial e importação de bibliotecas\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, lognorm\nimport pandas as pd\nimport seaborn as sns\nimport arviz as az\n\n```\n:::\n\n\n---\n\n### Escolhendo as prioris\n\nDesenvolver uma boa intuição sobre os parâmetros é essencial para construir modelos coerentes com o conhecimento prévio. Esta atividade tem como finalidade **apoiar a definição informada das distribuições a priori** no modelo bayesiano, de modo que reflitam o que sabemos (ou assumimos saber) sobre os parâmetros **antes de observar os dados**. O objetivo é explorar diferentes valores para os parâmetros da regressão linear e identificar combinações que representem, de forma realista, a relação esperada entre essas duas variáveis, e que possam ser utilizadas como prioris no modelo. \n\n**1. Escolha valores** para os parâmetros $\\beta_0$, $\\beta_1$ e $\\sigma$ (@eq-likelihood-normal-regression).\n\n   * $\\beta_0$ (intercepto): representa altura esperada quando o número do calçado é 0. Embora esse valor não tenha significado físico direto, ele influencia a posição da reta ajustada.\n   * $\\beta_1$ (inclinação da reta): representa a variação média na altura para cada número a mais de calçado.\n   * $\\sigma$ (desvio padrão): representa a variação natural nas alturas entre pessoas com o mesmo número de calçado.\n\n::: {#9359a7b7 .cell execution_count=2}\n``` {.python .cell-code}\n# Parâmetros para simulação\nbeta_0 =       # ESCOLHA a altura base (quando o número do calçado é zero)\nbeta_1 =      # ESCOLHA a taxa média de aumento na altura para cada número a mais de calçado\nsigma =         # ESCOLHA a variação individual na altura (desvio padrão dos erros)\n```\n:::\n\n\n\n\n**2. Crie uma sequência de valores para $x$** abrangendo limites coerentes com número do calçado para indivíduos adultos e utilize a função `norm.rvs` da biblioteca [SciPy](https://scipy.org/){target=_blank} para gerar dados simulados de altura com base no número do calçado.\n\n::: {#5afedf9d .cell execution_count=4}\n``` {.python .cell-code}\n# Simule o número do calçado (ex.: valores inteiros de 33 a 48, com 100 repetições por número)\nx_sim = np.repeat(np.arange(33, 49), 100)\n\n# Gere as alturas simuladas com erro normal\nmu = beta_0 + beta_1 * x_sim\ny_sim = norm.rvs(loc=mu, scale=sigma, size=len(x_sim))\n```\n:::\n\n\n**3. Utilize o `matplotlib` para visualizar os dados simulados**. O gráfico de dispersão mostrará a altura em função do número do calçado. Isso ajudará a avaliar se a simulação é coerente com sua expectativa sobre essa relação.\n\n::: {#bd960aa9 .cell execution_count=5}\n``` {.python .cell-code}\n# Use o matplotlib para plotar o resultado da simulação, isto é, altura_sim em função de x_sim\nplt.figure(figsize=(9, 6))\nplt.scatter(x_sim, y_sim, color='steelblue', alpha=0.6, label=\"Alturas simuladas\")\nplt.xlabel(\"Número do calçado\")\nplt.ylabel(\"Altura (cm)\")\nplt.title(\"Relação simulada entre número do calçado e altura\")\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n:::\n\n\n**4. Ajuste os valores** de $\\beta_0$, $\\beta_1$ e $\\sigma$ e **repita a simulação** até obter uma distribuição de pontos que represente **adequadamente** sua espectativa sobre a relação entre as variáveis.\n\n---\n\n### Implementando distribuições a priori no PyMC\n\nAgora que você já explorou os efeitos dos parâmetros $\\beta_0$, $\\beta_1$ e $\\sigma$, o próximo passo é **formalizar esse conhecimento** na distribuições a priori (Equações [-@eq-prior_beta_0], [-@eq-prior_beta_1] e [-@eq-prior_sigma]). Para isso, vamos utilizar a biblioteca de programação probabilística [PyMC](https://www.pymc.io/){target=_blank}.\n\n**1. Defina as distribuições a priori**\n\nUtilize os valores escolhidos anteriormente como os **centros das distribuições a priori**, isto é, utilize `beta_0`, `beta_1` e `sigma` respectivamente para representar $\\mu_{\\beta_0}$ (@eq-prior_beta_0), $\\mu_{\\beta_1}$ (@eq-prior_beta_1) e $\\mu_{\\log \\sigma}$ (@eq-prior_sigma). A implementação em PyMC tem por objetivo facilitar a escolha de valores razoáveis para $\\sigma_{\\beta_0}$ (@eq-prior_beta_0), $\\sigma_{\\beta_1}$ (@eq-prior_beta_1) e $\\sigma_{\\log \\sigma}$ (@eq-prior_sigma) compatíveis com seu grau de incerteza sobre estes parâmetros.\n\n::: {#dd376bd6 .cell execution_count=6}\n``` {.python .cell-code}\n# Geração de valores simulados para a variável preditora (calcado)\ncalcado_sim = np.arange(33, 49)\n\n# ESCOLHA altura base (quando o número do calçado é zero)\nmu_beta_0 =  # Média\nsd_beta_0 =  # Desvio padrão\n\n# ESCOLHA a taxa média de aumento na altura para cada número a mais de calçad\nmu_beta_1 =   # Média\nsd_beta_1 =   # Desvio padrão\n\n# ESCOLHA a variação individual na altura (desvio padrão dos erros)\nmu_lsigma =   # Média\nsd_lsigma =   # Desvio padrão\n```\n:::\n\n\n\n\n::: {#f1eef27c .cell execution_count=8}\n``` {.python .cell-code}\nn_samples = 1000\nwith pm.Model() as modelo_regressao_linear:\n\n    # Prioris\n    beta_0 = pm.Normal(\"beta_0\", mu=mu_beta_0, sigma=sd_beta_0) \n    beta_1 = pm.Normal(\"beta_1\", mu=mu_beta_1, sigma=sd_beta_1)\n    sigma = pm.Lognormal(\"sigma\", mu=np.log(mu_lsigma), sigma=sd_lsigma)\n\n    # Verossimilhança\n    mu = beta_0 + beta_1 * calcado_sim\n    altura_sim = pm.Normal(\"altura_sim\", mu=mu, sigma=sigma, shape=len(calcado_sim))\n\n    # Amostragem da distribuição preditiva a priori\n    prior_predictive_samples = pm.sample_prior_predictive(samples=n_samples)\n\n```\n:::\n\n\n**2. Checagem preditiva a priori**: Extraia as distribuições a priori dos parâmetros\n\n::: {#f262c9a9 .cell execution_count=9}\n``` {.python .cell-code}\n# Extração das distribuições a priori dos parâmetros\nbeta_0_prior = prior_predictive_samples.prior[\"beta_0\"].values.flatten()\nbeta_1_prior = prior_predictive_samples.prior[\"beta_1\"].values.flatten()\nsigma_prior = prior_predictive_samples.prior[\"sigma\"].values.flatten()\n\n# Extração da distribuição preditiva de y\naltura_sim_prior = prior_predictive_samples.prior[\"altura_sim\"].values.flatten()\n\n# Repita calcado_sim para alinhar com os n_samples valores de altura_sim_prior\ncalcado_sim_rep = np.tile(calcado_sim, n_samples)\n```\n:::\n\n\n**3. Verifique os histogramas das distribuições a priori e a distribuição preditiva com os dados simulados**\n\n::: {#fig-prior-parametros .cell execution_count=10}\n``` {.python .cell-code}\n# Plot dos histogramas e do gráfico de dispersão\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\n# Histograma do beta_0\naxes[0, 0].hist(beta_0_prior, bins=30, color='lightcoral', edgecolor='black')\naxes[0, 0].set_title(\"Intercepto: β₀\")\naxes[0, 0].set_xlabel(\"β₀\")\naxes[0, 0].set_ylabel(\"Frequência\")\n\n# Histograma do beta_1\naxes[0, 1].hist(beta_1_prior, bins=30, color='cornflowerblue', edgecolor='black')\naxes[0, 1].set_title(\"Inclinação: β₁\")\naxes[0, 1].set_xlabel(\"β₁\")\naxes[0, 1].set_ylabel(\"Frequência\")\n\n# Histograma de sigma\naxes[1, 0].hist(sigma_prior, bins=30, color='mediumseagreen', edgecolor='black')\naxes[1, 0].set_title(\"Desvio padrão: σ\")\naxes[1, 0].set_xlabel(\"σ\")\naxes[1, 0].set_ylabel(\"Frequência\")\n\n# Gráfico de dispersão dos dados simulados anteriormente\naxes[1, 1].scatter(calcado_sim_rep, altura_sim_prior, color='steelblue', alpha=0.6, label=\"Alturas simuladas\")\naxes[1, 1].set_title(\"Relação a priori predita\")\naxes[1, 1].set_xlabel(\"Número do calçado\")\naxes[1, 1].set_ylabel(\"Altura (cm)\")\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**4. Ajuste os valores** dos parâmetros e **repita a implementação do modelo** até obter uma distribuição de pontos que represente **adequadamente** sua espectativa sobre a relação entre as variáveis.\n\n### Ajustando o modelo a dados reais\n\n**1. Importe os dados** [`altura_adultos.csv`](https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/altura_adultos.csv)\n\n::: {#02424923 .cell execution_count=11}\n``` {.python .cell-code}\ndf = pd.read_csv('https://raw.githubusercontent.com/FCopf/datasets/refs/heads/main/altura_adultos.csv')\ndf\n```\n:::\n\n\nOs dados contém informações sobre `altura` (cm), número do `calcado` e `ano` de adultos.\n\n**2. Faça um gráfico de dispersão** entre `altura` ($y$) e `calcado` ($x$).\n\n::: {#135e0bdb .cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='calcado', y='altura', \n                alpha=0.7,           # transparência dos pontos\n                s=60,                # tamanho dos pontos\n                color='firebrick')   # cor dos pontos\n\nplt.title('Relação entre Número do Calçado e Altura', fontsize=14, fontweight='bold')\nplt.xlabel('Número do Calçado', fontsize=12)\nplt.ylabel('Altura (cm)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**3. Implemente os dados no PyMC** para estimar as distribuições posteriores\n\n*DADOS DE ENTRADA*\n\n::: {#1b15fb53 .cell execution_count=13}\n``` {.python .cell-code}\n# ENTRE com os parâmetros das prioris\nmu_beta_0 = \nsd_beta_0 = \n\nmu_beta_1 = \nsd_beta_1 = \n\nmu_lsigma = \nsd_lsigma = \n\n# Dados observados\nX = df['calcado']\nY = df['altura']\n```\n:::\n\n\n\n\n*IMPLEMENTAÇÃO EM PYMC*\n\n::: {#b3dcc224 .cell execution_count=15}\n``` {.python .cell-code}\nwith pm.Model() as modelo_regressao_linear:\n    \n    # Priori\n    beta_0 = pm.Normal(\"beta_0\", mu=mu_beta_0, sigma=sd_beta_0)\n    beta_1 = pm.Normal(\"beta_1\", mu=mu_beta_1, sigma=sd_beta_1)\n    sigma = pm.Lognormal(\"sigma\", mu=np.log(mu_lsigma), sigma=sd_lsigma)\n\n    # Verossimilhança\n    mu = beta_0 + beta_1 * calcado_sim # Equação da reta (modelo preditivo)\n    altura_obs = pm.Normal(\"altura_obs\", mu=beta_0 + beta_1 * X, \n                           sigma=sigma, observed = Y)\n    \n    # Amostragem MCMC para estimar a posterior e da distribuição preditiva posterior\n    trace = pm.sample(draws=1000, tune=1000, chains=4, target_accept=0.95)\n    posterior_predictive_samples = pm.sample_posterior_predictive(trace)\n```\n:::\n\n\n**4. Resultados do ajuste**\n\n*4.1. Resumo dos parâmetros posteriores*\n\n::: {#370fdd97 .cell execution_count=16}\n``` {.python .cell-code}\naz.summary(trace)\n```\n:::\n\n\n<br>\n\n*4.2. Gráficos de diagnóstico*\n\n::: {#54ade349 .cell execution_count=17}\n``` {.python .cell-code}\nfig, axes = plt.subplots(3, 2, figsize=(8, 6))\n\n# Trace plots\naz.plot_trace(trace, var_names=['beta_0', 'beta_1', 'sigma'], axes=axes)\nplt.suptitle('Trace Plots - Convergência das Cadeias MCMC')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n<br>\n\n*4.3. Distribuições posteriores*\n\n::: {#37fed8a2 .cell execution_count=18}\n``` {.python .cell-code}\naz.plot_posterior(trace, var_names=['beta_0', 'beta_1', 'sigma'], \n                 hdi_prob=0.95, figsize=(8, 4))\nplt.suptitle('Distribuições Posteriores dos Parâmetros')\nplt.show()\n```\n:::\n\n\n<br>\n\n*4.4. Ajuste do modelo (dados observados vs predições)*\n\n*Predições*\n\n::: {#cce5b970 .cell execution_count=19}\n``` {.python .cell-code}\n# Intervalo de credibilidade das predições\ncalcado_range = np.linspace(X.min(), X.max(), 100)\nposterior_beta_0 = trace.posterior['beta_0'].values.flatten()\nposterior_beta_1 = trace.posterior['beta_1'].values.flatten()\n\n# Calculando intervalos de credibilidade para a linha de regressão\npredictions = []\nfor x in calcado_range:\n    pred = posterior_beta_0 + posterior_beta_1 * x\n    predictions.append(pred)\n\npredictions = np.array(predictions)\npred_mean = np.mean(predictions, axis=1)\npred_lower = np.percentile(predictions, 2.5, axis=1)\npred_upper = np.percentile(predictions, 97.5, axis=1)\n```\n:::\n\n\n<br>\n*Gráfico de valores preditos*\n\n::: {#0f2185e7 .cell execution_count=20}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\n\nplt.scatter(X, Y, alpha=0.6, label='Dados Observados', color = 'firebrick')\nplt.plot(calcado_range, pred_mean, color = 'darkgreen', label='Regressão (Média Posterior)', \n        linewidth=2, linestyle=\"--\")\nplt.fill_between(calcado_range, pred_lower, pred_upper, \n                alpha=0.2, color='darkgreen', label='IC 95% (Posterior)')\nplt.xlabel('Número do Calçado')\nplt.ylabel('Altura (cm)')\nplt.title('Ajuste do Modelo de Regressão Linear Bayesiana')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n:::\n\n\n",
    "supporting": [
      "regressao-linear-bayesiana_files"
    ],
    "filters": [],
    "includes": {}
  }
}